{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 单阶段目标检测模型YOLOv3\n",
    "\n",
    "前面介绍了单阶段目标检测算法具有代表性的是YOLO系列，目前已经发展多个版本：YOLOv、YOLOv2、YOLOv3、YOLOv4、PP-YOLO、PP-YOLOv2等。YOLOv3在之前模型的基础上通过改变模型结构、Anchor设计来权衡速度与精度，因此我们先以经典的YOLOv3模型为例讲解单阶段目标检测模型方便快速了解YOLO模型，并在下一节进行基于YOLOv3进行实践。\n",
    "\n",
    "YOLOv3算法基本思想可以分成两部分：\n",
    "\n",
    "* 按一定规则在图片上产生一系列的候选区域，然后根据这些候选区域与图片上物体真实框之间的位置关系对候选区域进行标注。跟真实框足够接近的那些候选区域会被标注为正样本，同时将真实框的位置作为正样本的位置目标。偏离真实框较大的那些候选区域则会被标注为负样本，负样本不需要预测位置或者类别。\n",
    "* 使用卷积神经网络提取图片特征并对候选区域的位置和类别进行预测。这样每个预测框就可以看成是一个样本，根据真实框相对它的位置和类别进行了标注而获得标签值，通过网络模型预测其位置和类别，将网络预测值和标签值进行比较，就可以建立起损失函数。\n",
    "\n",
    "YOLOv3算法预测过程的流程图如 **图1** 所示，预测图片经过一系列预处理(resize、normalization等)输入到YOLOv3模型，根据预先设定的Anchor和提取到的图片特征得到目标预测框，最后通过非极大值抑制(NMS)消除重叠较大的冗余预测框，得到最终预测结果。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/e92ba2985c474efa8e83b17289b0534c8ffc616cb8c3441c8d268936d90bc571\"></center>\n",
    "<center><br>图1：YOLOv3算法预测流程图 </br></center>\n",
    "<br></br>\n",
    "\n",
    "YOLOv3网络结构大致分为3个部分：Backbone、Neck、Head，如 **图2** 所示:\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/682c9a2807ed4b34b5ef575e56d4c629623c5e2d86a442b6b49d32decd1f821c\" width='800'></center>\n",
    "<center><br>图2：YOLOv3网络结构 </br></center>\n",
    "<br></br>\n",
    "\n",
    "- Bockbone：骨干网络，主要用于特征提取\n",
    "- Neck：在Backbone和Head之间提取不同阶段中特征图\n",
    "- Head：检测头，用于预测目标的类别和位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 产生候选区域\n",
    "\n",
    "如何产生候选区域，是检测模型的核心设计方案。目前大多数基于卷积神经网络的模型所采用的方式大体如下：\n",
    "\n",
    "* 按一定的规则在图片上生成一系列位置固定的锚框，将这些锚框看作是可能的候选区域。\n",
    "* 对锚框是否包含目标物体进行预测，如果包含目标物体，还需要预测所包含物体的类别，以及预测框相对于锚框位置需要调整的幅度。\n",
    "\n",
    "\n",
    "### 5.2.1.1 生成锚框\n",
    "\n",
    "将原始图片划分成$m\\times n$个区域，如下图所示，原始图片高度$H=640$, 宽度$W=480$，如果我们选择小块区域的尺寸为$32 \\times 32$，则$m$和$n$分别为：\n",
    "\n",
    "$$m = \\frac{640}{32} = 20$$\n",
    "\n",
    "$$n = \\frac{480}{32} = 15$$\n",
    "\n",
    "如 **图3** 所示，将原始图像分成了20行15列小方块区域。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/2dd1cbeb53644552a8cb38f3f834dbdda5046a489465454d93cdc88d1ce65ca5\" width = \"400\"></center>\n",
    "<center><br>图3：将图片划分成多个32x32的小方块 </br></center>\n",
    "<br></br>\n",
    "\n",
    "\n",
    "YOLOv3算法会在每个区域的中心，生成一系列锚框。为了展示方便，我们先在图中第十行第四列的小方块位置附近画出生成的锚框，如 **图4** 所示。\n",
    "\n",
    "------\n",
    "**注意：**\n",
    "\n",
    "这里为了跟程序中的编号对应，最上面的行号是第0行，最左边的列号是第0列。\n",
    "\n",
    "------\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/6dd42b9138364a379b6231ac2247d3cb449d612e17be4896986bca2703acbb29\" width = \"400\"></center>\n",
    "<center><br>图4：在第10行第4列的小方块区域生成3个锚框 </br></center>\n",
    "<br></br>\n",
    "\n",
    "**图5** 展示在每个区域附近都生成3个锚框，很多锚框堆叠在一起可能不太容易看清楚，但过程跟上面类似，只是需要以每个区域的中心点为中心，分别生成3个锚框。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/0880c3b5ec2d40edb476f4fcbadd87aa9f37059cd24d4a1a9d37c627ce5f618a\" width = \"400\"></center>\n",
    "<center><br>图5：在每个小方块区域生成3个锚框 </br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.2 生成预测框\n",
    "\n",
    "在前面已经指出，锚框的位置都是固定好的，不可能刚好跟物体边界框重合，需要在锚框的基础上进行位置的微调以生成预测框。预测框相对于锚框会有不同的中心位置和大小，采用什么方式能得到预测框呢？我们先来考虑如何生成其中心位置坐标。\n",
    "\n",
    "比如上面图中在第10行第4列的小方块区域中心生成的一个锚框，如绿色虚线框所示。以小方格的宽度为单位长度，\n",
    "\n",
    "此小方块区域左上角的位置坐标是：\n",
    "$$c_x = 4$$\n",
    "$$c_y = 10$$\n",
    "\n",
    "此锚框的区域中心坐标是：\n",
    "$$center\\_x = c_x + 0.5 = 4.5$$\n",
    "$$center\\_y = c_y + 0.5 = 10.5$$\n",
    "\n",
    "可以通过下面的方式生成预测框的中心坐标：\n",
    "$$b_x = c_x + \\sigma(t_x)$$\n",
    "$$b_y = c_y + \\sigma(t_y)$$\n",
    "\n",
    "其中$t_x$和$t_y$为实数，$\\sigma(x)$是我们之前学过的Sigmoid函数，其定义如下：\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + exp(-x)}$$\n",
    "\n",
    "由于Sigmoid的函数值在$0 \\thicksim 1$之间，因此由上面公式计算出来的预测框的中心点总是落在第十行第四列的小区域内部。\n",
    "\n",
    "当$t_x=t_y=0$时，$b_x = c_x + 0.5$，$b_y = c_y + 0.5$，预测框中心与锚框中心重合，都是小区域的中心。\n",
    "\n",
    "锚框的大小是预先设定好的，在模型中可以当作是超参数，下图中画出的锚框尺寸是\n",
    "\n",
    "$$p_h = 350$$\n",
    "$$p_w = 250$$\n",
    "\n",
    "通过下面的公式生成预测框的大小：\n",
    "\n",
    "$$b_h = p_h e^{t_h}$$\n",
    "$$b_w = p_w e^{t_w}$$\n",
    "\n",
    "如果$t_x=t_y=0, t_h=t_w=0$，则预测框跟锚框重合。\n",
    "\n",
    "如果给$t_x, t_y, t_h, t_w$随机赋值如下：\n",
    "\n",
    "$$t_x = 0.2,  t_y = 0.3, t_w = 0.1, t_h = -0.12$$\n",
    "\n",
    "则可以得到预测框的坐标是(154.98, 357.44, 276.29, 310.42)，如 **图6** 中蓝色框所示。\n",
    "\n",
    "------\n",
    "**说明：**\n",
    "这里坐标采用$xywh$的格式。\n",
    "\n",
    "-------\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f4b33522eb5a45f0804b94a5c66b76a0a2d13345d6de499399580a031b6ccc74\" width = \"400\"></center>\n",
    "<center><br>图6：生成预测框 </br></center>\n",
    "<br></br>\n",
    "\n",
    "这里我们会问：当$t_x, t_y, t_w, t_h$取值为多少的时候，预测框能够跟真实框重合？为了回答问题，只需要将上面预测框坐标中的$b_x, b_y, b_h, b_w$设置为真实框的位置，即可求解出$t$的数值。\n",
    "\n",
    "令：\n",
    "$$\\sigma(t^*_x) + c_x = gt_x$$\n",
    "$$\\sigma(t^*_y) + c_y = gt_y$$\n",
    "$$p_w e^{t^*_w} = gt_h$$\n",
    "$$p_h e^{t^*_h} = gt_w$$\n",
    "\n",
    "可以求解出：$(t^*_x, t^*_y, t^*_w, t^*_h)$\n",
    "\n",
    "如果$t$是网络预测的输出值，将$t^*$作为目标值，以他们之间的差距作为损失函数，则可以建立起一个回归问题，通过学习网络参数，使得$t$足够接近$t^*$，从而能够求解出预测框的位置坐标和大小。\n",
    "\n",
    "预测框可以看作是在锚框基础上的一个微调，每个锚框会有一个跟它对应的预测框，我们需要确定上面计算式中的$t_x, t_y, t_w, t_h$，从而计算出与锚框对应的预测框的位置和形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1.3 对候选区域进行标注\n",
    "\n",
    "每个区域可以产生3种不同形状的锚框，每个锚框都是一个可能的候选区域，对这些候选区域我们需要了解如下几件事情：\n",
    "\n",
    "- 锚框是否包含物体，这可以看成是一个二分类问题，使用标签objectness来表示。当锚框包含了物体时，objectness=1，表示预测框属于正类；当锚框不包含物体时，设置objectness=0，表示锚框属于负类。\n",
    "\n",
    "- 如果锚框包含了物体，那么它对应的预测框的中心位置和大小应该是多少，或者说上面计算式中的$t_x, t_y, t_w, t_h$应该是多少，使用location标签。\n",
    "\n",
    "- 如果锚框包含了物体，那么具体类别是什么，这里使用变量label来表示其所属类别的标签。\n",
    "\n",
    "选取任意一个锚框对它进行标注，也就是需要确定其对应的objectness, $(t_x, t_y, t_w, t_h)$和label，下面将分别讲述如何确定这三个标签的值。\n",
    "\n",
    "#### 标注锚框是否包含物体\n",
    "\n",
    "如 **图13** 所示，这里一共有3个目标，以最左边的人像为例，其真实框是$(133.96, 328.42, 186.06, 374.63)$。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/f21679e68d2b496698ed788a16d4ea2e5bc6f82b253a44ef9508b6a4fc9b6be4\" width = \"600\"></center>\n",
    "<center><br>图13：选出与真实框中心位于同一区域的锚框 </br></center>\n",
    "<br></br>\n",
    "\n",
    "真实框的中心点坐标是：\n",
    "\n",
    "$$center\\_x = 133.96$$\n",
    "\n",
    "$$center\\_y = 328.42$$\n",
    "\n",
    "$$ i = 133.96 / 32 = 4.18625$$\n",
    "\n",
    "$$ j = 328.42 / 32 = 10.263125$$\n",
    "\n",
    "它落在了第10行第4列的小方块内，如**图13**所示。此小方块区域可以生成3个不同形状的锚框，其在图上的编号和大小分别是$A_1(116, 90), A_2(156, 198), A_3(373, 326)$。\n",
    "\n",
    "用这3个不同形状的锚框跟真实框计算IoU，选出IoU最大的锚框。这里为了简化计算，只考虑锚框的形状，不考虑其跟真实框中心之间的偏移，具体计算结果如 **图14** 所示。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3008337ea66c44068042c670db54368edc56b1e43ced4b6b811bdc95b64ca3d5\" width = \"400\"></center>\n",
    "<center><br>图14：选出与真实框与锚框的IoU </br></center>\n",
    "<br></br>\n",
    "\n",
    "其中跟真实框IoU最大的是锚框$A_3$，形状是$(373, 326)$，将它所对应的预测框的objectness标签设置为1，其所包括的物体类别就是真实框里面的物体所属类别。\n",
    "\n",
    "依次可以找出其他几个真实框对应的IoU最大的锚框，然后将它们的预测框的objectness标签也都设置为1。这里一共有$20 \\times 15 \\times 3 = 900$个锚框，只有3个预测框会被标注为正。\n",
    "\n",
    "由于每个真实框只对应一个objectness标签为正的预测框，如果有些预测框跟真实框之间的IoU很大，但并不是最大的那个，那么直接将其objectness标签设置为0当作负样本，可能并不妥当。为了避免这种情况，YOLOv3算法设置了一个IoU阈值iou_threshold，当预测框的objectness不为1，但是其与某个真实框的IoU大于iou_threshold时，就将其objectness标签设置为-1，不参与损失函数的计算。\n",
    "\n",
    "所有其他的预测框，其objectness标签均设置为0，表示负类。\n",
    "\n",
    "对于objectness=1的预测框，需要进一步确定其位置和包含物体的具体分类标签，但是对于objectness=0或者-1的预测框，则不用管他们的位置和类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标注预测框的位置坐标标签\n",
    "\n",
    "当锚框objectness=1时，需要确定预测框位置相对于它微调的幅度，也就是锚框的位置标签。\n",
    "\n",
    "在前面我们已经问过这样一个问题：当$t_x, t_y, t_w, t_h$取值为多少的时候，预测框能够跟真实框重合？其做法是将预测框坐标中的$b_x, b_y, b_h, b_w$设置为真实框的坐标，即可求解出$t$的数值。\n",
    "\n",
    "令：\n",
    "$$\\sigma(t^*_x) + c_x = gt_x$$\n",
    "$$\\sigma(t^*_y) + c_y = gt_y$$\n",
    "$$p_w e^{t^*_w} = gt_w$$\n",
    "$$p_h e^{t^*_h} = gt_h$$\n",
    "\n",
    "对于$t_x^*$和$t_y^*$，由于Sigmoid的反函数不好计算，我们直接使用$\\sigma(t^*_x)$和$\\sigma(t^*_y)$作为回归的目标。\n",
    "\n",
    "$$d_x^* = \\sigma(t^*_x) = gt_x - c_x$$\n",
    "\n",
    "$$d_y^* = \\sigma(t^*_y) = gt_y - c_y$$\n",
    "\n",
    "$$t^*_w = log(\\frac{gt_w}{p_w})$$\n",
    "\n",
    "$$t^*_h = log(\\frac{gt_h}{p_h})$$\n",
    "\n",
    "如果$(t_x, t_y, t_h, t_w)$是网络预测的输出值，将$(d_x^*, d_y^*, t_w^*, t_h^*)$作为$(\\sigma(t_x), \\sigma(t_y), t_h, t_w)$的目标值，以它们之间的差距作为损失函数，则可以建立起一个回归问题，通过学习网络参数，使得$t$足够接近$t^*$，从而能够求解出预测框的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 标注锚框包含物体类别的标签\n",
    "\n",
    "对于objectness=1的锚框，需要确定其具体类别。正如上面所说，objectness标注为1的锚框，会有一个真实框跟它对应，该锚框所属物体类别，即是其所对应的真实框包含的物体类别。这里使用one-hot向量来表示类别标签label。比如一共有10个分类，而真实框里面包含的物体类别是第2类，则label为$(0,1,0,0,0,0,0,0,0,0)$\n",
    "\n",
    "对上述步骤进行总结，标注的流程如 **图15** 所示。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3b914be0c6274916bc7abe4922d4d0fb75be340172764f7096af5be0c2737c57\" width = \"700\"></center>\n",
    "<center><br>图15：标注流程示意图 </br></center>\n",
    "<br></br>\n",
    "\n",
    "通过这种方式，我们在每个小方块区域都生成了一系列的锚框作为候选区域，并且根据图片上真实物体的位置，标注出了每个候选区域对应的objectness标签、位置需要调整的幅度以及包含的物体所属的类别。位置需要调整的幅度由4个变量描述$(t_x, t_y, t_w, t_h)$，objectness标签需要用一个变量描述$obj$，描述所属类别的变量长度等于类别数C。\n",
    "\n",
    "对于每个锚框，模型需要预测输出$(t_x, t_y, t_w, t_h, P_{obj}, P_1, P_2,... , P_C)$，其中$P_{obj}$是锚框是否包含物体的概率，$P_1, P_2,... , P_C$则是锚框包含的物体属于每个类别的概率。接下来让我们一起学习如何通过卷积神经网络输出这样的预测值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.1 Banckbone(特征提取)\n",
    "\n",
    "在上一节图像分类的课程中，我们已经学习过了通过卷积神经网络提取图像特征。通过连续使用多层卷积和池化等操作，能得到语义含义更加丰富的特征图。在检测问题中，也使用卷积神经网络逐层提取图像特征，通过最终的输出特征图来表征物体位置和类别等信息。\n",
    "\n",
    "YOLOv3算法使用的骨干网络是Darknet53。Darknet53网络的具体结构如 **图3** 所示，在ImageNet图像分类任务上取得了很好的成绩。在检测任务中，将图中C0后面的平均池化、全连接层和Softmax去掉，保留从输入到C0部分的网络结构，作为检测模型的基础网络结构，也称为骨干网络。YOLOv3模型会在骨干网络的基础上，再添加检测相关的网络模块。\n",
    "\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/d5cb1e88d3f44259be1427a90ee454a57738ee8083ad40269f5485988526f30d\" width = \"400\"></center>\n",
    "<center><br>图3：Darknet53网络结构 </br></center>\n",
    "\n",
    "下面的程序是Darknet53骨干网络的实现代码，这里将上图中C0、C1、C2所表示的输出数据取出，并查看它们的形状分别是，$C0 [1, 1024, 20, 20]$，$C1 [1, 512, 40, 40]$，$C2 [1, 256, 80, 80]$。\n",
    "\n",
    "- 名词解释：特征图的步幅(stride)\n",
    "\n",
    "在提取特征的过程中通常会使用步幅大于1的卷积或者池化，导致后面的特征图尺寸越来越小，特征图的步幅等于输入图片尺寸除以特征图尺寸。例如：C0的尺寸是$20\\times20$，原图尺寸是$640\\times640$，则C0的步幅是$\\frac{640}{20}=32$。同理，C1的步幅是16，C2的步幅是8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-07T12:33:40.109453Z",
     "iopub.status.busy": "2022-09-07T12:33:40.109047Z",
     "iopub.status.idle": "2022-09-07T12:33:41.567126Z",
     "shell.execute_reply": "2022-09-07T12:33:41.566102Z",
     "shell.execute_reply.started": "2022-09-07T12:33:40.109424Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class ConvBNLayer(paddle.nn.Layer):\n",
    "    def __init__(self, ch_in, ch_out, \n",
    "                 kernel_size=3, stride=1, groups=1,\n",
    "                 padding=0, act=\"leaky\"):\n",
    "        super(ConvBNLayer, self).__init__()\n",
    "    \n",
    "        self.conv = paddle.nn.Conv2D(\n",
    "            in_channels=ch_in,\n",
    "            out_channels=ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            groups=groups,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Normal(0., 0.02)),\n",
    "            bias_attr=False)\n",
    "    \n",
    "        self.batch_norm = paddle.nn.BatchNorm2D(\n",
    "            num_features=ch_out,\n",
    "            weight_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Normal(0., 0.02),\n",
    "                regularizer=paddle.regularizer.L2Decay(0.)),\n",
    "            bias_attr=paddle.ParamAttr(\n",
    "                initializer=paddle.nn.initializer.Constant(0.0),\n",
    "                regularizer=paddle.regularizer.L2Decay(0.)))\n",
    "        self.act = act\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        out = self.conv(inputs)\n",
    "        out = self.batch_norm(out)\n",
    "        if self.act == 'leaky':\n",
    "            out = F.leaky_relu(x=out, negative_slope=0.1)\n",
    "        return out\n",
    "    \n",
    "class DownSample(paddle.nn.Layer):\n",
    "    # 下采样，图片尺寸减半，具体实现方式是使用stirde=2的卷积\n",
    "    def __init__(self,\n",
    "                 ch_in,\n",
    "                 ch_out,\n",
    "                 kernel_size=3,\n",
    "                 stride=2,\n",
    "                 padding=1):\n",
    "\n",
    "        super(DownSample, self).__init__()\n",
    "\n",
    "        self.conv_bn_layer = ConvBNLayer(\n",
    "            ch_in=ch_in,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=padding)\n",
    "        self.ch_out = ch_out\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv_bn_layer(inputs)\n",
    "        return out\n",
    "\n",
    "class BasicBlock(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    基本残差块的定义，输入x经过两层卷积，然后接第二层卷积的输出和输入x相加\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in, ch_out):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = ConvBNLayer(\n",
    "            ch_in=ch_in,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "            )\n",
    "        self.conv2 = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1\n",
    "            )\n",
    "    def forward(self, inputs):\n",
    "        conv1 = self.conv1(inputs)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        out = paddle.add(x=inputs, y=conv2)\n",
    "        return out\n",
    "\n",
    "     \n",
    "class LayerWarp(paddle.nn.Layer):\n",
    "    \"\"\"\n",
    "    添加多层残差块，组成Darknet53网络的一个层级\n",
    "    \"\"\"\n",
    "    def __init__(self, ch_in, ch_out, count, is_test=True):\n",
    "        super(LayerWarp,self).__init__()\n",
    "\n",
    "        self.basicblock0 = BasicBlock(ch_in,\n",
    "            ch_out)\n",
    "        self.res_out_list = []\n",
    "        for i in range(1, count):\n",
    "            res_out = self.add_sublayer(\"basic_block_%d\" % (i), # 使用add_sublayer添加子层\n",
    "                BasicBlock(ch_out*2,\n",
    "                    ch_out))\n",
    "            self.res_out_list.append(res_out)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        y = self.basicblock0(inputs)\n",
    "        for basic_block_i in self.res_out_list:\n",
    "            y = basic_block_i(y)\n",
    "        return y\n",
    "\n",
    "# DarkNet 每组残差块的个数，来自DarkNet的网络结构图\n",
    "DarkNet_cfg = {53: ([1, 2, 8, 8, 4])}\n",
    "\n",
    "class DarkNet53_conv_body(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(DarkNet53_conv_body, self).__init__()\n",
    "        self.stages = DarkNet_cfg[53]\n",
    "        self.stages = self.stages[0:5]\n",
    "\n",
    "        # 第一层卷积\n",
    "        self.conv0 = ConvBNLayer(\n",
    "            ch_in=3,\n",
    "            ch_out=32,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "\n",
    "        # 下采样，使用stride=2的卷积来实现\n",
    "        self.downsample0 = DownSample(\n",
    "            ch_in=32,\n",
    "            ch_out=32 * 2)\n",
    "\n",
    "        # 添加各个层级的实现\n",
    "        self.darknet53_conv_block_list = []\n",
    "        self.downsample_list = []\n",
    "        for i, stage in enumerate(self.stages):\n",
    "            conv_block = self.add_sublayer(\n",
    "                \"stage_%d\" % (i),\n",
    "                LayerWarp(32*(2**(i+1)),\n",
    "                32*(2**i),\n",
    "                stage))\n",
    "            self.darknet53_conv_block_list.append(conv_block)\n",
    "        # 两个层级之间使用DownSample将尺寸减半\n",
    "        for i in range(len(self.stages) - 1):\n",
    "            downsample = self.add_sublayer(\n",
    "                \"stage_%d_downsample\" % i,\n",
    "                DownSample(ch_in=32*(2**(i+1)),\n",
    "                    ch_out=32*(2**(i+2))))\n",
    "            self.downsample_list.append(downsample)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        out = self.conv0(inputs)\n",
    "        #print(\"conv1:\",out.numpy())\n",
    "        out = self.downsample0(out)\n",
    "        #print(\"dy:\",out.numpy())\n",
    "        blocks = []\n",
    "        for i, conv_block_i in enumerate(self.darknet53_conv_block_list): #依次将各个层级作用在输入上面\n",
    "            out = conv_block_i(out)\n",
    "            blocks.append(out)\n",
    "            if i < len(self.stages) - 1:\n",
    "                out = self.downsample_list[i](out)\n",
    "        return blocks[-1:-4:-1] # 将C0, C1, C2作为返回值\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-07T12:33:52.805555Z",
     "iopub.status.busy": "2022-09-07T12:33:52.804566Z",
     "iopub.status.idle": "2022-09-07T12:33:56.448760Z",
     "shell.execute_reply": "2022-09-07T12:33:56.447923Z",
     "shell.execute_reply.started": "2022-09-07T12:33:52.805519Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/nn/layer/norm.py:654: UserWarning: When training, we now always track global mean and variance.\n",
      "  \"When training, we now always track global mean and variance.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1024, 20, 20] [1, 512, 40, 40] [1, 256, 80, 80]\n"
     ]
    }
   ],
   "source": [
    "# 查看Darknet53网络输出特征图\n",
    "import numpy as np\n",
    "backbone = DarkNet53_conv_body()\n",
    "x = np.random.randn(1, 3, 640, 640).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "C0, C1, C2 = backbone(x)\n",
    "print(C0.shape, C1.shape, C2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这段示例代码，指定输入数据的形状是$(1, 3, 640, 640)$，则3个层级的输出特征图的形状分别是$C0 (1, 1024, 20, 20)$，$C1 (1, 512, 40, 40)$和$C2 (1, 256, 80, 80)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.2 Neck（多尺度检测）\n",
    "\n",
    "如果只在在特征图P0的基础上进行的，它的步幅stride=32。特征图的尺寸比较小，像素点数目比较少，每个像素点的感受野很大，具有非常丰富的高层级语义信息，可能比较容易检测到较大的目标。为了能够检测到尺寸较小的那些目标，需要在尺寸较大的特征图上面建立预测输出。如果我们在C2或者C1这种层级的特征图上直接产生预测输出，可能面临新的问题，它们没有经过充分的特征提取，像素点包含的语义信息不够丰富，有可能难以提取到有效的特征模式。在目标检测中，解决这一问题的方式是，将高层级的特征图尺寸放大之后跟低层级的特征图进行融合，得到的新特征图既能包含丰富的语义信息，又具有较多的像素点，能够描述更加精细的结构。\n",
    "\n",
    "具体的网络实现方式如 **图4** 所示：\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/b6d3b425644342e48bd0a50ebde90d882fd10717e0e44a53a44e98225bbb6df8\" width = \"800\"></center>\n",
    "<center><br>图4：生成多层级的输出特征图P0、P1、P2 </br></center>\n",
    "<br></br>\n",
    "\n",
    "YOLOv3在每个区域的中心位置产生3个锚框，在3个层级的特征图上产生锚框的大小分别为P2 [(10×13),(16×30),(33×23)]，P1 [(30×61),(62×45),(59× 119)]，P0[(116 × 90), (156 × 198), (373 × 326]。越往后的特征图上用到的锚框尺寸也越大，能捕捉到大尺寸目标的信息；越往前的特征图上锚框尺寸越小，能捕捉到小尺寸目标的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.3 检测头设计(计算预测框位置和类别)\n",
    "\n",
    "YOLOv3中对每个预测框计算逻辑如下：\n",
    "\n",
    "- 预测框是否包含物体。也可理解为objectness=1的概率是多少，可以用网络输出一个实数$x$，可以用$Sigmoid(x)$表示objectness为正的概率$P_{obj}$\n",
    "\n",
    "- 预测物体位置和形状。物体位置和形状$t_x, t_y, t_w, t_h$可以用网络输出4个实数来表示$t_x, t_y, t_w, t_h$\n",
    "\n",
    "- 预测物体类别。预测图像中物体的具体类别是什么，或者说其属于每个类别的概率分别是多少。总的类别数为C，需要预测物体属于每个类别的概率$(P_1, P_2, ..., P_C)$，可以用网络输出C个实数$(x_1, x_2, ..., x_C)$，对每个实数分别求Sigmoid函数，让$P_i = Sigmoid(x_i)$，则可以表示出物体属于每个类别的概率。\n",
    "\n",
    "\n",
    "对于一个预测框，网络需要输出$(5 + C)$个实数来表征它是否包含物体、位置和形状尺寸以及属于每个类别的概率。\n",
    "\n",
    "由于我们在每个小方块区域都生成了K个预测框，则所有预测框一共需要网络输出的预测值数目是：\n",
    "\n",
    "$$[K(5 + C)] \\times m \\times n $$\n",
    "\n",
    "还有更重要的一点是网络输出必须要能区分出小方块区域的位置来，不能直接将特征图连接一个输出大小为$[K(5 + C)] \\times m \\times n$的全连接层。\n",
    "\n",
    "#### 建立输出特征图与预测框之间的关联\n",
    "\n",
    "现在观察特征图，经过多次卷积核池化之后，其步幅stride=32，$640 \\times 480$大小的输入图片变成了$20\\times15$的特征图；而小方块区域的数目正好是$20\\times15$，也就是说可以让特征图上每个像素点分别跟原图上一个小方块区域对应。这也是为什么我们最开始将小方块区域的尺寸设置为32的原因，这样可以巧妙的将小方块区域跟特征图上的像素点对应起来，解决了空间位置的对应关系。\n",
    "\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/59bd2592dd9f4f4dada8333307198888e667b15969ce434eb52c0232d9608a62\" width = \"600\"></center>\n",
    "<center><br>图5：特征图C0与小方块区域形状对比 </br></center>\n",
    "<br></br>\n",
    "\n",
    "下面需要将像素点$(i,j)$与第i行第j列的小方块区域所需要的预测值关联起来，每个小方块区域产生K个预测框，每个预测框需要$(5 + C)$个实数预测值，则每个像素点相对应的要有$K(5 + C)$个实数。为了解决这一问题，对特征图进行多次卷积，并将最终的输出通道数设置为$K(5 + C)$，即可将生成的特征图与每个预测框所需要的预测值巧妙的对应起来。当然，这种对应是为了将骨干网络提取的特征对接输出层来形成Loss。实际中，这几个尺寸可以随着任务数据分布的不同而调整，只要保证特征图输出尺寸（控制卷积核和下采样）和输出层尺寸（控制小方块区域的大小）相同即可。\n",
    "\n",
    "骨干网络的输出特征图是C0，下面的程序是对C0进行多次卷积以得到跟预测框相关的特征图P0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class YoloDetectionBlock(paddle.nn.Layer):\n",
    "    # define YOLOv3 detection head\n",
    "    # 使用多层卷积和BN提取特征\n",
    "    def __init__(self,ch_in,ch_out,is_test=True):\n",
    "        super(YoloDetectionBlock, self).__init__()\n",
    "\n",
    "        assert ch_out % 2 == 0, \\\n",
    "            \"channel {} cannot be divided by 2\".format(ch_out)\n",
    "\n",
    "        self.conv0 = ConvBNLayer(\n",
    "            ch_in=ch_in,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.conv1 = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "        self.conv2 = ConvBNLayer(\n",
    "            ch_in=ch_out*2,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.conv3 = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "        self.route = ConvBNLayer(\n",
    "            ch_in=ch_out*2,\n",
    "            ch_out=ch_out,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0)\n",
    "        self.tip = ConvBNLayer(\n",
    "            ch_in=ch_out,\n",
    "            ch_out=ch_out*2,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1)\n",
    "    def forward(self, inputs):\n",
    "        out = self.conv0(inputs)\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        route = self.route(out)\n",
    "        tip = self.tip(route)\n",
    "        return route, tip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 36, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "NUM_ANCHORS = 3\n",
    "NUM_CLASSES = 7\n",
    "num_filters=NUM_ANCHORS * (NUM_CLASSES + 5)\n",
    "\n",
    "backbone = DarkNet53_conv_body()\n",
    "detection = YoloDetectionBlock(ch_in=1024, ch_out=512)\n",
    "conv2d_pred = paddle.nn.Conv2D(in_channels=1024, out_channels=num_filters, kernel_size=1)\n",
    "\n",
    "x = np.random.randn(1, 3, 640, 640).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "C0, C1, C2 = backbone(x)\n",
    "route, tip = detection(C0)\n",
    "P0 = conv2d_pred(tip)\n",
    "\n",
    "print(P0.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上面的代码所示，可以由特征图C0生成特征图P0，P0的形状是$[1, 36, 20, 20]$。每个小方块区域生成的锚框或者预测框的数量是3，物体类别数目是7，每个区域需要的预测值个数是$3 \\times (5 + 7) = 36$，正好等于P0的输出通道数。\n",
    "\n",
    "将$P0[t, 0:12, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框所需要的12个预测值对应，$P0[t, 12:24, i, j]$与输入的第t张图片上小方块区域$(i, j)$第2个预测框所需要的12个预测值对应，$P0[t, 24:36, i, j]$与输入的第t张图片上小方块区域$(i, j)$第3个预测框所需要的12个预测值对应。\n",
    "\n",
    "$P0[t, 0:4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的位置对应，$P0[t, 4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的objectness对应，$P0[t, 5:12, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的类别对应。\n",
    "\n",
    "如 **图6** 所示，通过这种方式可以巧妙的将网络输出特征图，与每个小方块区域生成的预测框对应起来了。\n",
    "<br></br>\n",
    "<center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/9ea44b2c11f74f1484ab2bdc93be4008008cdee0b8d34dcb97bc9f89af935d1c\" width = \"800\"></center>\n",
    "<center><br>图6：特征图P0与候选区域的关联 </br></center>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算预测框是否包含物体的概率\n",
    "\n",
    "根据前面的分析，$P0[t, 4, i, j]$与输入的第t张图片上小方块区域$(i, j)$第1个预测框的objectness对应，$P0[t, 4+12, i, j]$与第2个预测框的objectness对应，...，则可以使用下面的程序将objectness相关的预测取出，并使用`paddle.nn.functional.sigmoid`计算输出概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 20, 20] [1, 3, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "NUM_ANCHORS = 3\n",
    "NUM_CLASSES = 7\n",
    "num_filters=NUM_ANCHORS * (NUM_CLASSES + 5)\n",
    "\n",
    "backbone = DarkNet53_conv_body()\n",
    "detection = YoloDetectionBlock(ch_in=1024, ch_out=512)\n",
    "conv2d_pred = paddle.nn.Conv2D(in_channels=1024, out_channels=num_filters,  kernel_size=1)\n",
    "\n",
    "x = np.random.randn(1, 3, 640, 640).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "C0, C1, C2 = backbone(x)\n",
    "route, tip = detection(C0)\n",
    "P0 = conv2d_pred(tip)\n",
    "\n",
    "reshaped_p0 = paddle.reshape(P0, [-1, NUM_ANCHORS, NUM_CLASSES + 5, P0.shape[2], P0.shape[3]])\n",
    "pred_objectness = reshaped_p0[:, :, 4, :, :]\n",
    "pred_objectness_probability = F.sigmoid(pred_objectness)\n",
    "print(pred_objectness.shape, pred_objectness_probability.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的输出程序显示，预测框是否包含物体的概率`pred_objectness_probability`，其数据形状是[1, 3, 20, 20]，与我们上面提到的预测框个数一致，数据大小在0～1之间，表示预测框为正样本的概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算预测框位置坐标\n",
    "\n",
    "$P0[t, 0:4, i, j]$与输入的第$t$张图片上小方块区域$(i, j)$第1个预测框的位置对应，$P0[t, 12:16, i, j]$与第2个预测框的位置对应，依此类推，则使用下面的程序可以从$P0$中取出跟预测框位置相关的预测值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 4, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "NUM_ANCHORS = 3\n",
    "NUM_CLASSES = 7\n",
    "num_filters=NUM_ANCHORS * (NUM_CLASSES + 5)\n",
    "\n",
    "backbone = DarkNet53_conv_body()\n",
    "detection = YoloDetectionBlock(ch_in=1024, ch_out=512)\n",
    "conv2d_pred =  paddle.nn.Conv2D(in_channels=1024, out_channels=num_filters,  kernel_size=1)\n",
    "\n",
    "x = np.random.randn(1, 3, 640, 640).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "C0, C1, C2 = backbone(x)\n",
    "route, tip = detection(C0)\n",
    "P0 = conv2d_pred(tip)\n",
    "\n",
    "\n",
    "reshaped_p0 = paddle.reshape(P0, [-1, NUM_ANCHORS, NUM_CLASSES + 5, P0.shape[2], P0.shape[3]])\n",
    "pred_objectness = reshaped_p0[:, :, 4, :, :]\n",
    "pred_objectness_probability = F.sigmoid(pred_objectness)\n",
    "\n",
    "pred_location = reshaped_p0[:, :, 0:4, :, :]\n",
    "print(pred_location.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络输出值是$(t_x, t_y, t_w, t_h)$，还需要将其转化为$(x_1, y_1, x_2, y_2)$这种形式的坐标表示。使用飞桨[paddle.vision.ops.yolo_box](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/ops/yolo_box_cn.html) API可以直接计算出结果，但为了给读者更清楚的展示算法的实现过程，我们使用Numpy来实现这一过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义Sigmoid函数\n",
    "def sigmoid(x):\n",
    "    return 1./(1.0 + np.exp(-x))\n",
    "\n",
    "# 将网络特征图输出的[tx, ty, th, tw]转化成预测框的坐标[x1, y1, x2, y2]\n",
    "def get_yolo_box_xxyy(pred, anchors, num_classes, downsample):\n",
    "    \"\"\"\n",
    "    pred是网络输出特征图转化成的numpy.ndarray\n",
    "    anchors 是一个list。表示锚框的大小，\n",
    "                例如 anchors = [116, 90, 156, 198, 373, 326]，表示有三个锚框，\n",
    "                第一个锚框大小[w, h]是[116, 90]，第二个锚框大小是[156, 198]，第三个锚框大小是[373, 326]\n",
    "    \"\"\"\n",
    "    batchsize = pred.shape[0]\n",
    "    num_rows = pred.shape[-2]\n",
    "    num_cols = pred.shape[-1]\n",
    "\n",
    "    input_h = num_rows * downsample\n",
    "    input_w = num_cols * downsample\n",
    "\n",
    "    num_anchors = len(anchors) // 2\n",
    "\n",
    "    # pred的形状是[N, C, H, W]，其中C = NUM_ANCHORS * (5 + NUM_CLASSES)\n",
    "    # 对pred进行reshape\n",
    "    pred = pred.reshape([-1, num_anchors, 5+num_classes, num_rows, num_cols])\n",
    "    pred_location = pred[:, :, 0:4, :, :]\n",
    "    pred_location = np.transpose(pred_location, (0,3,4,1,2))\n",
    "    anchors_this = []\n",
    "    for ind in range(num_anchors):\n",
    "        anchors_this.append([anchors[ind*2], anchors[ind*2+1]])\n",
    "    anchors_this = np.array(anchors_this).astype('float32')\n",
    "    \n",
    "    # 最终输出数据保存在pred_box中，其形状是[N, H, W, NUM_ANCHORS, 4]，\n",
    "    # 其中最后一个维度4代表位置的4个坐标\n",
    "    pred_box = np.zeros(pred_location.shape)\n",
    "    for n in range(batchsize):\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                for k in range(num_anchors):\n",
    "                    pred_box[n, i, j, k, 0] = j\n",
    "                    pred_box[n, i, j, k, 1] = i\n",
    "                    pred_box[n, i, j, k, 2] = anchors_this[k][0]\n",
    "                    pred_box[n, i, j, k, 3] = anchors_this[k][1]\n",
    "\n",
    "    # 这里使用相对坐标，pred_box的输出元素数值在0.~1.0之间\n",
    "    pred_box[:, :, :, :, 0] = (sigmoid(pred_location[:, :, :, :, 0]) + pred_box[:, :, :, :, 0]) / num_cols\n",
    "    pred_box[:, :, :, :, 1] = (sigmoid(pred_location[:, :, :, :, 1]) + pred_box[:, :, :, :, 1]) / num_rows\n",
    "    pred_box[:, :, :, :, 2] = np.exp(pred_location[:, :, :, :, 2]) * pred_box[:, :, :, :, 2] / input_w\n",
    "    pred_box[:, :, :, :, 3] = np.exp(pred_location[:, :, :, :, 3]) * pred_box[:, :, :, :, 3] / input_h\n",
    "\n",
    "    # 将坐标从xywh转化成xyxy\n",
    "    pred_box[:, :, :, :, 0] = pred_box[:, :, :, :, 0] - pred_box[:, :, :, :, 2] / 2.\n",
    "    pred_box[:, :, :, :, 1] = pred_box[:, :, :, :, 1] - pred_box[:, :, :, :, 3] / 2.\n",
    "    pred_box[:, :, :, :, 2] = pred_box[:, :, :, :, 0] + pred_box[:, :, :, :, 2]\n",
    "    pred_box[:, :, :, :, 3] = pred_box[:, :, :, :, 1] + pred_box[:, :, :, :, 3]\n",
    "\n",
    "    pred_box = np.clip(pred_box, 0., 1.0)\n",
    "\n",
    "    return pred_box\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过调用上面定义的`get_yolo_box_xxyy`函数，可以从$P0$计算出预测框坐标来，具体程序如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 20, 3, 4)\n"
     ]
    }
   ],
   "source": [
    "NUM_ANCHORS = 3\n",
    "NUM_CLASSES = 7\n",
    "num_filters=NUM_ANCHORS * (NUM_CLASSES + 5)\n",
    "\n",
    "backbone = DarkNet53_conv_body()\n",
    "detection = YoloDetectionBlock(ch_in=1024, ch_out=512)\n",
    "conv2d_pred = paddle.nn.Conv2D(in_channels=1024, out_channels=num_filters,  kernel_size=1)\n",
    "\n",
    "x = np.random.randn(1, 3, 640, 640).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "C0, C1, C2 = backbone(x)\n",
    "route, tip = detection(C0)\n",
    "P0 = conv2d_pred(tip)\n",
    "\n",
    "reshaped_p0 = paddle.reshape(P0, [-1, NUM_ANCHORS, NUM_CLASSES + 5, P0.shape[2], P0.shape[3]])\n",
    "pred_objectness = reshaped_p0[:, :, 4, :, :]\n",
    "pred_objectness_probability = F.sigmoid(pred_objectness)\n",
    "\n",
    "pred_location = reshaped_p0[:, :, 0:4, :, :]\n",
    "\n",
    "# anchors包含了预先设定好的锚框尺寸\n",
    "anchors = [116, 90, 156, 198, 373, 326]\n",
    "# downsample是特征图P0的步幅\n",
    "pred_boxes = get_yolo_box_xxyy(P0.numpy(), anchors, num_classes=7, downsample=32) # 由输出特征图P0计算预测框位置坐标\n",
    "print(pred_boxes.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面程序计算出来的pred_boxes的形状是$[N, H, W, num\\_anchors, 4]$，坐标格式是$[x_1, y_1, x_2, y_2]$，数值在0~1之间，表示相对坐标。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算物体属于每个类别概率\n",
    "\n",
    "$P0[t, 5:12, i, j]$与输入的第$t$张图片上小方块区域$(i, j)$第1个预测框包含物体的类别对应，$P0[t, 17:24, i, j]$与第2个预测框的类别对应，依此类推，则使用下面的程序可以从$P0$中取出那些跟预测框类别相关的预测值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 7, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "NUM_ANCHORS = 3\n",
    "NUM_CLASSES = 7\n",
    "num_filters=NUM_ANCHORS * (NUM_CLASSES + 5)\n",
    "\n",
    "backbone = DarkNet53_conv_body()\n",
    "detection = YoloDetectionBlock(ch_in=1024, ch_out=512)\n",
    "conv2d_pred = paddle.nn.Conv2D(in_channels=1024, out_channels=num_filters,  kernel_size=1)\n",
    "\n",
    "x = np.random.randn(1, 3, 640, 640).astype('float32')\n",
    "x = paddle.to_tensor(x)\n",
    "C0, C1, C2 = backbone(x)\n",
    "route, tip = detection(C0)\n",
    "P0 = conv2d_pred(tip)\n",
    "\n",
    "reshaped_p0 = paddle.reshape(P0, [-1, NUM_ANCHORS, NUM_CLASSES + 5, P0.shape[2], P0.shape[3]])\n",
    "# 取出与objectness相关的预测值\n",
    "pred_objectness = reshaped_p0[:, :, 4, :, :]\n",
    "pred_objectness_probability = F.sigmoid(pred_objectness)\n",
    "# 取出与位置相关的预测值\n",
    "pred_location = reshaped_p0[:, :, 0:4, :, :]\n",
    "# 取出与类别相关的预测值\n",
    "pred_classification = reshaped_p0[:, :, 5:5+NUM_CLASSES, :, :]\n",
    "pred_classification_probability = F.sigmoid(pred_classification)\n",
    "print(pred_classification.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的程序通过$P0$计算出了预测框包含的物体所属类别的概率，`pred_classification_probability`的形状是$[1, 3, 7, 20, 20]$，数值在0~1之间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "了解了检测模型的backbone、neck、head3个部分，接下来定义YOLOv3的模型，同时添加模型预测代码`get_pred`，通过网络输出计算出预测框位置和所属类别的得分，推荐大家直接使用[paddle.vision.ops.yolo_box](https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/api/paddle/vision/ops/yolo_box_cn.html)获得P0、P1、P2三个层级的特征图对应的预测框和得分，并将他们拼接在一块，即可得到所有的预测框及其属于各个类别的得分，关键参数含义如下：\n",
    "\n",
    "> paddle.vision.ops.yolo_box(x, img_size, anchors, class_num, conf_thresh, downsample_ratio, clip_bbox=True, name=None, scale_x_y=1.0)\n",
    "\n",
    "- x，网络输出特征图，例如上面提到的P0或者P1、P2。\n",
    "- img_size，输入图片尺寸。\n",
    "- anchors，使用到的anchor的尺寸，如[10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326]\n",
    "- class_num，物体类别数。\n",
    "- conf_thresh, 置信度阈值，得分低于该阈值的预测框位置数值不用计算直接设置为0.0。\n",
    "- downsample_ratio, 特征图的下采样比例，例如P0是32，P1是16，P2是8。\n",
    "- name=None，名字，例如'yolo_box'，一般无需设置，默认值为None。\n",
    "   \n",
    "返回值包括两项，boxes和scores，其中boxes是所有预测框的坐标值，scores是所有预测框的得分。\n",
    "\n",
    "预测框得分的定义是所属类别的概率乘以其预测框是否包含目标物体的objectness概率，即\n",
    "\n",
    "$$score = P_{obj} \\cdot P_{classification}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 定义上采样模块\n",
    "class Upsample(paddle.nn.Layer):\n",
    "    def __init__(self, scale=2):\n",
    "        super(Upsample,self).__init__()\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # get dynamic upsample output shape\n",
    "        shape_nchw = paddle.shape(inputs)\n",
    "        shape_hw = paddle.slice(shape_nchw, axes=[0], starts=[2], ends=[4])\n",
    "        shape_hw.stop_gradient = True\n",
    "        in_shape = paddle.cast(shape_hw, dtype='int32')\n",
    "        out_shape = in_shape * self.scale\n",
    "        out_shape.stop_gradient = True\n",
    "\n",
    "        # reisze by actual_shape\n",
    "        out = paddle.nn.functional.interpolate(\n",
    "            x=inputs, scale_factor=self.scale, mode=\"NEAREST\")\n",
    "        return out\n",
    "\n",
    "class YOLOv3(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(YOLOv3,self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        # 提取图像特征的骨干代码\n",
    "        self.block = DarkNet53_conv_body()\n",
    "        self.block_outputs = []\n",
    "        self.yolo_blocks = []\n",
    "        self.route_blocks_2 = []\n",
    "        # 生成3个层级的特征图P0, P1, P2\n",
    "        for i in range(3):\n",
    "            # 添加从ci生成ri和ti的模块\n",
    "            yolo_block = self.add_sublayer(\n",
    "                \"yolo_detecton_block_%d\" % (i),\n",
    "                YoloDetectionBlock(\n",
    "                                   ch_in=512//(2**i)*2 if i==0 else 512//(2**i)*2 + 512//(2**i),\n",
    "                                   ch_out = 512//(2**i)))\n",
    "            self.yolo_blocks.append(yolo_block)\n",
    "\n",
    "            num_filters = 3 * (self.num_classes + 5)\n",
    "\n",
    "            # 添加从ti生成pi的模块，这是一个Conv2D操作，输出通道数为3 * (num_classes + 5)\n",
    "            block_out = self.add_sublayer(\n",
    "                \"block_out_%d\" % (i),\n",
    "                paddle.nn.Conv2D(in_channels=512//(2**i)*2,\n",
    "                       out_channels=num_filters,\n",
    "                       kernel_size=1,\n",
    "                       stride=1,\n",
    "                       padding=0,\n",
    "                       weight_attr=paddle.ParamAttr(\n",
    "                           initializer=paddle.nn.initializer.Normal(0., 0.02)),\n",
    "                       bias_attr=paddle.ParamAttr(\n",
    "                           initializer=paddle.nn.initializer.Constant(0.0),\n",
    "                           regularizer=paddle.regularizer.L2Decay(0.))))\n",
    "            self.block_outputs.append(block_out)\n",
    "            if i < 2:\n",
    "                # 对ri进行卷积\n",
    "                route = self.add_sublayer(\"route2_%d\"%i,\n",
    "                                          ConvBNLayer(ch_in=512//(2**i),\n",
    "                                                      ch_out=256//(2**i),\n",
    "                                                      kernel_size=1,\n",
    "                                                      stride=1,\n",
    "                                                      padding=0))\n",
    "                self.route_blocks_2.append(route)\n",
    "            # 将ri放大以便跟c_{i+1}保持同样的尺寸\n",
    "            self.upsample = Upsample()\n",
    "    def forward(self, inputs):\n",
    "        outputs = []\n",
    "        blocks = self.block(inputs)\n",
    "        for i, block in enumerate(blocks):\n",
    "            if i > 0:\n",
    "                # 将r_{i-1}经过卷积和上采样之后得到特征图，与这一级的ci进行拼接\n",
    "                block = paddle.concat([route, block], axis=1)\n",
    "            # 从ci生成ti和ri\n",
    "            route, tip = self.yolo_blocks[i](block)\n",
    "            # 从ti生成pi\n",
    "            block_out = self.block_outputs[i](tip)\n",
    "            # 将pi放入列表\n",
    "            outputs.append(block_out)\n",
    "\n",
    "            if i < 2:\n",
    "                # 对ri进行卷积调整通道数\n",
    "                route = self.route_blocks_2[i](route)\n",
    "                # 对ri进行放大，使其尺寸和c_{i+1}保持一致\n",
    "                route = self.upsample(route)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def get_pred(self,\n",
    "                 outputs,\n",
    "                 im_shape=None,\n",
    "                 anchors = [10, 13, 16, 30, 33, 23, 30, 61, 62, 45, 59, 119, 116, 90, 156, 198, 373, 326],\n",
    "                 anchor_masks = [[6, 7, 8], [3, 4, 5], [0, 1, 2]],\n",
    "                 valid_thresh = 0.01):\n",
    "        downsample = 32\n",
    "        total_boxes = []\n",
    "        total_scores = []\n",
    "        for i, out in enumerate(outputs):\n",
    "            anchor_mask = anchor_masks[i]\n",
    "            anchors_this_level = []\n",
    "            for m in anchor_mask:\n",
    "                anchors_this_level.append(anchors[2 * m])\n",
    "                anchors_this_level.append(anchors[2 * m + 1])\n",
    "\n",
    "            boxes, scores = paddle.vision.ops.yolo_box(\n",
    "                   x=out,\n",
    "                   img_size=im_shape,\n",
    "                   anchors=anchors_this_level,\n",
    "                   class_num=self.num_classes,\n",
    "                   conf_thresh=valid_thresh,\n",
    "                   downsample_ratio=downsample,\n",
    "                   name=\"yolo_box\" + str(i))\n",
    "            total_boxes.append(boxes)\n",
    "            total_scores.append(\n",
    "                        paddle.transpose(\n",
    "                        scores, perm=[0, 2, 1]))\n",
    "            downsample = downsample // 2\n",
    "\n",
    "        yolo_boxes = paddle.concat(total_boxes, axis=1)\n",
    "        yolo_scores = paddle.concat(total_scores, axis=2)\n",
    "        return yolo_boxes, yolo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2.4 损失函数\n",
    "\n",
    "上面从概念上将输出特征图上的像素点与预测框关联起来了，那么要对神经网络进行求解，还必须从数学上将网络输出和预测框关联起来，也就是要建立起损失函数跟网络输出之间的关系。下面讨论如何建立起YOLOv3的损失函数。\n",
    "\n",
    "对于每个预测框，YOLOv3模型会建立三种类型的损失函数：\n",
    "\n",
    "- 表征是否包含目标物体的损失函数，通过pred_objectness和label_objectness计算。\n",
    "\n",
    "        loss_obj = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_objectness, label_objectness)\n",
    "\n",
    "- 表征物体位置的损失函数，通过pred_location和label_location计算。\n",
    "\n",
    "        pred_location_x = pred_location[:, :, 0, :, :]\n",
    "        pred_location_y = pred_location[:, :, 1, :, :]\n",
    "        pred_location_w = pred_location[:, :, 2, :, :]\n",
    "        pred_location_h = pred_location[:, :, 3, :, :]\n",
    "        loss_location_x = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_location_x, label_location_x)\n",
    "        loss_location_y = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_location_y, label_location_y)\n",
    "        loss_location_w = paddle.abs(pred_location_w - label_location_w)\n",
    "        loss_location_h = paddle.abs(pred_location_h - label_location_h)\n",
    "        loss_location = loss_location_x + loss_location_y + loss_location_w + loss_location_h\n",
    "\n",
    "- 表征物体类别的损失函数，通过pred_classification和label_classification计算。\n",
    "\n",
    "        loss_obj = paddle.nn.fucntional.binary_cross_entropy_with_logits(pred_classification, label_classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
