{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\fluid\\reader.py:486: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from data_process import get_MNIST_dataloader\n",
    "\n",
    "train_loader, test_loader = get_MNIST_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [4.195677], acc is [0.109375]\n",
      "epoch: 0, batch: 200, loss is: [0.27381498], acc is [0.90625]\n",
      "epoch: 0, batch: 400, loss is: [0.24639821], acc is [0.953125]\n",
      "epoch: 0, batch: 600, loss is: [0.18681931], acc is [0.9375]\n",
      "epoch: 0, batch: 800, loss is: [0.31446987], acc is [0.890625]\n",
      "epoch: 1, batch: 0, loss is: [0.08133395], acc is [0.984375]\n",
      "epoch: 1, batch: 200, loss is: [0.09195037], acc is [0.984375]\n",
      "epoch: 1, batch: 400, loss is: [0.05425718], acc is [0.984375]\n",
      "epoch: 1, batch: 600, loss is: [0.20285143], acc is [0.9375]\n",
      "epoch: 1, batch: 800, loss is: [0.26960588], acc is [0.921875]\n",
      "epoch: 2, batch: 0, loss is: [0.15529877], acc is [0.9375]\n",
      "epoch: 2, batch: 200, loss is: [0.11891925], acc is [0.953125]\n",
      "epoch: 2, batch: 400, loss is: [0.10107299], acc is [0.953125]\n",
      "epoch: 2, batch: 600, loss is: [0.09463695], acc is [0.953125]\n",
      "epoch: 2, batch: 800, loss is: [0.04390793], acc is [1.]\n",
      "epoch: 3, batch: 0, loss is: [0.02251565], acc is [0.984375]\n",
      "epoch: 3, batch: 200, loss is: [0.0478178], acc is [0.984375]\n",
      "epoch: 3, batch: 400, loss is: [0.2833452], acc is [0.90625]\n",
      "epoch: 3, batch: 600, loss is: [0.00641091], acc is [1.]\n",
      "epoch: 3, batch: 800, loss is: [0.04405641], acc is [0.984375]\n",
      "epoch: 4, batch: 0, loss is: [0.01669668], acc is [1.]\n",
      "epoch: 4, batch: 200, loss is: [0.09681974], acc is [0.984375]\n",
      "epoch: 4, batch: 400, loss is: [0.37334952], acc is [0.96875]\n",
      "epoch: 4, batch: 600, loss is: [0.07528082], acc is [0.984375]\n",
      "epoch: 4, batch: 800, loss is: [0.05127333], acc is [0.96875]\n"
     ]
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "# 多层卷积神经网络实现\n",
    "class MNIST(paddle.nn.Layer):\n",
    "     def __init__(self):\n",
    "         super(MNIST, self).__init__()\n",
    "         \n",
    "         # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "         self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "         # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "         self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "         # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "         self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "         # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "         self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "         # 定义一层全连接层，输出维度是10\n",
    "         self.fc = Linear(in_features=980, out_features=10)\n",
    "         \n",
    "   # 定义网络前向计算过程，卷积后紧接着使用池化层，最后使用全连接层计算最终输出\n",
    "   # 卷积层激活函数使用Relu，全连接层激活函数使用softmax\n",
    "     def forward(self, inputs, label):\n",
    "         x = self.conv1(inputs)\n",
    "         x = F.relu(x)\n",
    "         x = self.max_pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.max_pool2(x)\n",
    "         x = paddle.reshape(x, [x.shape[0], 980])\n",
    "         x = self.fc(x)\n",
    "         if label is not None:\n",
    "             acc = paddle.metric.accuracy(input=x, label=label)\n",
    "             return x, acc\n",
    "         else:\n",
    "             return x\n",
    "\n",
    "#在使用GPU机器时，可以将use_gpu变量设置成True\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\n",
    "\n",
    "#仅优化算法的设置有所差别\n",
    "def train(model):\n",
    "    model = MNIST()\n",
    "    model.train()\n",
    "    \n",
    "    #四种优化算法的设置方案，可以逐一尝试效果\n",
    "    # opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Adagrad(learning_rate=0.01, parameters=model.parameters())\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 5\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            #前向计算的过程\n",
    "            predicts, acc = model(images, labels)\n",
    "            \n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                \n",
    "            #后向传播，更新参数，消除梯度的过程\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), 'mnist.pdparams')\n",
    "    \n",
    "#创建模型    \n",
    "model = MNIST()\n",
    "#启动训练过程\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## print network layer's superparams ##############\n",
      "conv1-- kernel_size:[20, 1, 5, 5], padding:2, stride:[1, 1]\n",
      "conv2-- kernel_size:[20, 20, 5, 5], padding:2, stride:[1, 1]\n",
      "fc-- weight_size:[980, 10], bias_size_[10]\n",
      "\n",
      "########## print shape of features of every layer ###############\n",
      "inputs_shape: [64, 1, 28, 28]\n",
      "outputs1_shape: [64, 20, 28, 28]\n",
      "outputs2_shape: [64, 20, 28, 28]\n",
      "outputs3_shape: [64, 20, 14, 14]\n",
      "outputs4_shape: [64, 20, 14, 14]\n",
      "outputs5_shape: [64, 20, 14, 14]\n",
      "outputs6_shape: [64, 980]\n",
      "outputs7_shape: [64, 10]\n",
      "epoch: 0, batch: 0, loss is: [4.5245805], acc is [0.125]\n",
      "epoch: 0, batch: 200, loss is: [0.26533666], acc is [0.921875]\n",
      "epoch: 0, batch: 400, loss is: [0.18510015], acc is [0.921875]\n",
      "\n",
      "########## print convolution layer's kernel ###############\n",
      "conv1 params -- kernel weights: Tensor(shape=[5, 5], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[-0.47458637, -0.20812006,  0.17526799, -0.13944180,  0.37350339],\n",
      "        [ 0.28461045, -0.07451025,  0.01306493,  0.04138475, -0.44686329],\n",
      "        [ 0.32759753,  0.05545608,  0.21907578,  0.00896626, -0.38247982],\n",
      "        [-0.32210431,  0.46711469,  0.29933617, -0.05468028, -0.24911575],\n",
      "        [ 0.37028259, -0.23154877, -0.05893278,  0.03565325, -0.23922905]])\n",
      "conv2 params -- kernel weights: Tensor(shape=[5, 5], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.16187763,  0.00537574, -0.07667238,  0.01240738,  0.00331076],\n",
      "        [ 0.01155921, -0.07673892, -0.00639480, -0.02121737, -0.02283492],\n",
      "        [ 0.07773882,  0.09286615,  0.09629327, -0.09912838, -0.01456976],\n",
      "        [ 0.00425731,  0.12886204, -0.00634887,  0.11469242,  0.07950386],\n",
      "        [ 0.06958096,  0.01699586, -0.03912218, -0.05644484, -0.07584870]])\n",
      "\n",
      "The 9th channel of conv1 layer:  Tensor(shape=[28, 28], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[-3.04358363, -2.58173108, -2.76732516, -2.76732516, -2.76732516,\n",
      "         -2.76732516, -2.76732516, -2.76732516, -2.76732516, -2.76732516,\n",
      "         -2.76732516, -2.76732516, -2.76732516, -2.76732516, -2.76732516,\n",
      "         -2.76732516, -2.76732516, -2.76732516, -2.76732516, -2.76732516,\n",
      "         -2.76732516, -2.76732516, -2.76732516, -2.76732516, -2.76732516,\n",
      "         -2.76732516, -1.39047790, -0.30851871],\n",
      "        [-3.35096669, -3.26936221, -3.46484065, -3.46484065, -3.46484065,\n",
      "         -3.46484065, -3.46484065, -3.46484065, -3.46484065, -3.46484065,\n",
      "         -3.46484065, -3.46484065, -3.46484065, -3.46484065, -3.46484065,\n",
      "         -3.46484065, -3.46484065, -3.46484065, -3.46484065, -3.46484065,\n",
      "         -3.46484065, -3.46484065, -3.46484065, -3.46484065, -3.46484065,\n",
      "         -3.46484065, -1.95870304, -0.83573925],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.62938118, -2.42843080,\n",
      "         -1.44682455, -0.51585484, -1.30472910, -2.50742316, -3.46069026,\n",
      "         -3.12711263, -2.69800305, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.04589820, -0.64428294,\n",
      "          0.07864264,  0.37853077, -1.78191400, -3.04033017, -3.49339604,\n",
      "         -2.78251910, -2.63725209, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.47161293, -1.49851215, -0.40802273, -0.41316369,\n",
      "         -1.02221012, -0.85934901, -1.42104518, -1.74325442, -2.45426774,\n",
      "         -2.04032969, -2.57173181, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.03617239, -0.71179748,  0.14857194, -0.51521927, -1.21963930,\n",
      "         -1.33257103, -0.29552832, -0.29628041, -0.72979623, -1.67441237,\n",
      "         -2.03413224, -2.61575818, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.62938118,\n",
      "         -1.14838398, -0.64216113, -0.43207696, -1.06275654, -1.30230165,\n",
      "         -1.45827270, -1.32632971, -1.69775808, -1.94137657, -2.21176124,\n",
      "         -2.62331033, -2.66536164, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.30463815,\n",
      "         -0.27371177,  0.37019259,  0.41139343, -1.41961098, -2.01333499,\n",
      "         -2.25824594, -2.49002194, -3.57657599, -3.30595708, -2.80749440,\n",
      "         -2.83111644, -2.66209054, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.34539843, -0.84081829,\n",
      "          0.58607888,  0.24471864, -1.55945814, -3.19339919, -1.66546929,\n",
      "         -1.35459161, -2.35225749, -3.07242322, -3.24457335, -2.94541669,\n",
      "         -2.70779109, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.31168771, -0.43165964,\n",
      "          1.34020090, -0.03457874, -2.71654868, -3.06361961, -0.76949656,\n",
      "         -0.10721250, -1.43787718, -2.93212867, -3.59660125, -2.85647750,\n",
      "         -2.65028882, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.43481660,  0.13358307,\n",
      "          1.99622726,  0.69634658, -1.85580575, -1.13479388, -0.16544503,\n",
      "          0.19738014, -1.27293980, -3.02269244, -3.09405828, -2.31853032,\n",
      "         -2.63994861, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.28613210,  0.47662237,\n",
      "          2.20601678,  1.82277989,  0.44069579,  0.42078504,  1.31435609,\n",
      "          0.56758440, -1.47835565, -2.70569968, -2.15954566, -2.08864903,\n",
      "         -2.64358926, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.60257888, -1.12913907,\n",
      "         -0.20895268,  1.58077180,  1.87709677,  2.47686958,  1.78123689,\n",
      "          0.11575401, -1.96693814, -1.95870054, -1.59421158, -2.47885776,\n",
      "         -2.64945865, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.58684039, -1.67240262,\n",
      "         -1.15112233,  0.43030706,  2.58367014,  4.10529804,  2.13298178,\n",
      "         -0.92204034, -2.73346210, -2.17221522, -2.02708316, -2.76593947,\n",
      "         -2.65037990, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.62397647, -2.23282218,\n",
      "         -2.45168424, -0.72879553,  2.16154313,  3.30321813,  0.85708278,\n",
      "         -1.72490489, -2.44364762, -1.89217556, -2.61441875, -2.73412061,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.66035247, -2.75773406,\n",
      "         -3.35785556, -1.56589913,  0.65137237,  0.79097766, -1.13048756,\n",
      "         -3.07277417, -2.30162883, -2.02851558, -2.77133536, -2.65910220,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.68794131,\n",
      "         -2.80488086, -0.63581300,  0.33912709, -0.32211545, -2.56282258,\n",
      "         -2.91571641, -1.97036040, -2.56539702, -2.74216914, -2.65037990,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -1.72826791,  0.58848894,  0.79012573, -1.19642365, -3.45400643,\n",
      "         -2.41321158, -2.07888627, -2.78883672, -2.66744590, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.62307048,\n",
      "         -0.91712612,  0.81780982,  0.24364021, -2.30846119, -3.18566728,\n",
      "         -2.02533269, -2.56141162, -2.75405908, -2.64969087, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -1.62842441,\n",
      "          0.30017802,  0.90359408, -1.09069026, -3.49137855, -2.37600255,\n",
      "         -1.99444270, -2.72886109, -2.66691232, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -1.38706017,\n",
      "          0.21782188,  0.23912767, -2.04022121, -2.93887019, -1.79056251,\n",
      "         -2.42433238, -2.73168969, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -1.03921413,\n",
      "          0.00625362, -0.80272013, -2.92725587, -2.21265292, -1.91483200,\n",
      "         -2.74923444, -2.68335843, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -1.74440670,\n",
      "         -1.15780473, -2.10368323, -2.56449771, -1.88243973, -2.54693580,\n",
      "         -2.77514291, -2.65520167, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.06749558,\n",
      "         -2.35972095, -2.95537448, -2.59447217, -2.40819621, -2.81137919,\n",
      "         -2.68275619, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-2.73420596, -2.54066253, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.59515595,\n",
      "         -3.15651107, -3.15270257, -2.55760503, -2.79882693, -2.74464822,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -2.64831328, -2.64831328, -2.64831328, -2.64831328,\n",
      "         -2.64831328, -1.20613241, -0.45847571],\n",
      "        [-1.13515949, -1.26461697, -1.60691285, -1.60691285, -1.60691285,\n",
      "         -1.60691285, -1.60691285, -1.60691285, -1.60691285, -1.67613649,\n",
      "         -2.02717304, -1.88144779, -1.76704931, -1.72655678, -1.62620068,\n",
      "         -1.60691285, -1.60691285, -1.60691285, -1.60691285, -1.60691285,\n",
      "         -1.60691285, -1.60691285, -1.60691285, -1.60691285, -1.60691285,\n",
      "         -1.60691285, -0.96935022, -0.71589535],\n",
      "        [-0.96136284, -1.18832791, -1.30457819, -1.30457819, -1.30457819,\n",
      "         -1.30457819, -1.30457819, -1.30457819, -1.30457819, -1.30457819,\n",
      "         -1.30457819, -1.30457819, -1.30457819, -1.30457819, -1.30457819,\n",
      "         -1.30457819, -1.30457819, -1.30457819, -1.30457819, -1.30457819,\n",
      "         -1.30457819, -1.30457819, -1.30457819, -1.30457819, -1.30457819,\n",
      "         -1.30457819, -0.79008496, -0.58008093]])\n",
      "The 4th channel of conv2 layer:  Tensor(shape=[14, 14], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[-0.90181738, -0.75016981,  0.10001307,  0.07890276,  0.01343453,\n",
      "         -0.25076312, -0.41541630, -0.49969286,  0.14734446, -0.01036311,\n",
      "          0.24305622,  0.35522500,  0.31740510,  0.94699144],\n",
      "        [-1.00472784, -0.52995872,  0.56388533,  0.46074194,  0.02917182,\n",
      "          0.32347083,  0.63334835, -0.48007348,  0.64077652,  0.18421069,\n",
      "         -0.23188074,  1.06329930,  1.37804437,  1.66914892],\n",
      "        [-1.82131040, -1.56588089, -0.92122364, -0.94385070, -1.54002118,\n",
      "         -0.69384503, -0.55821913, -0.99778605, -1.29546070, -1.41022599,\n",
      "         -1.11235702, -0.03003554,  0.33615583,  0.90461284],\n",
      "        [-1.81780863, -1.69900537, -1.15133512, -1.10769069, -1.58672774,\n",
      "         -0.50264704, -0.65320861, -2.30567026, -1.54634774, -1.25241518,\n",
      "         -0.36166999,  0.93151587,  0.70235729,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -1.10378194, -0.94051439, -2.06894231,\n",
      "          0.69123060, -1.14818096, -2.35424566,  1.06039786,  0.78725803,\n",
      "          1.35677516,  0.54246390,  0.21498163,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -1.14738429, -0.27760640, -1.53796637,\n",
      "         -0.92675757, -1.07852900, -0.64706701,  0.47142267, -0.36649695,\n",
      "          0.30456683, -0.45782560,  0.16035168,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -0.88428217, -0.71563554, -1.26694465,\n",
      "         -0.42896888, -0.29674166,  0.00534251, -2.62920189, -2.58758211,\n",
      "         -0.67185175, -0.37827003,  0.15139179,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -0.88460791, -0.22733179, -0.26749587,\n",
      "         -0.48738185,  0.62291521,  0.63687634, -3.26186967, -0.69445980,\n",
      "          0.31888261,  0.19568972,  0.24700917,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -0.97451484, -0.31254739,  0.25170207,\n",
      "         -0.86150873,  1.43751669,  1.80661857, -0.40837714,  1.02025688,\n",
      "          0.20979993, -0.48493302,  0.24819688,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -1.15580034, -1.22190118, -0.69259983,\n",
      "         -1.43829632,  1.73459220,  1.36463177,  1.77607131,  1.01538467,\n",
      "         -0.92667270, -0.63811404,  0.24721567,  0.98436844],\n",
      "        [-1.81780863, -1.69900537, -1.14976561, -0.41288683, -1.08310258,\n",
      "         -0.21959531,  0.57070637, -0.30763811,  1.21946180, -0.37145227,\n",
      "         -1.39473438, -0.28033963,  0.24764653,  0.98436844],\n",
      "        [-1.89996648, -1.95177078, -1.06389463, -0.39469761, -1.25709391,\n",
      "          0.01017561,  0.33483693,  0.65974241,  1.23331058, -1.07652628,\n",
      "         -1.37503874, -0.27937835,  0.27035555,  1.08653116],\n",
      "        [-1.69967246, -1.45226300, -0.52012378, -0.12681578, -1.06768417,\n",
      "          0.30344671,  0.81999099,  1.29788697,  0.74913019, -0.83187890,\n",
      "         -0.57652801, -0.11573012,  0.46992823,  0.95415556],\n",
      "        [-0.33179313, -0.40616277, -0.32325876, -0.21955724,  0.08439887,\n",
      "          0.38371664,  0.96648902,  1.24659157, -0.00516087, -0.47231624,\n",
      "          0.03421411,  0.34647670,  0.42901579,  0.35057986]])\n",
      "The output of last layer: Tensor(shape=[10], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [-3.97194481,  1.01415849, -0.80303991, -0.57175630,  2.13174748,\n",
      "         2.15554380,  1.29982781, -0.32710242,  2.84795761,  2.51247287]) \n",
      "\n",
      "epoch: 0, batch: 600, loss is: [0.17167719], acc is [0.953125]\n",
      "epoch: 0, batch: 800, loss is: [0.22542135], acc is [0.921875]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# 定义模型结构\n",
    "class MNIST(paddle.nn.Layer):\n",
    "     def __init__(self):\n",
    "         super(MNIST, self).__init__()\n",
    "         \n",
    "         # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "         self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "         # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "         self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "         # 定义卷积层，输出特征通道out_channels设置为20，卷积核的大小kernel_size为5，卷积步长stride=1，padding=2\n",
    "         self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "         # 定义池化层，池化核的大小kernel_size为2，池化步长为2\n",
    "         self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "         # 定义一层全连接层，输出维度是10\n",
    "         self.fc = Linear(in_features=980, out_features=10)\n",
    "     \n",
    "     #加入对每一层输入和输出的尺寸和数据内容的打印，根据check参数决策是否打印每层的参数和输出尺寸\n",
    "     # 卷积层激活函数使用Relu，全连接层激活函数使用softmax\n",
    "     def forward(self, inputs, label=None, check_shape=False, check_content=False):\n",
    "         # 给不同层的输出不同命名，方便调试\n",
    "         outputs1 = self.conv1(inputs)\n",
    "         outputs2 = F.relu(outputs1)\n",
    "         outputs3 = self.max_pool1(outputs2)\n",
    "         outputs4 = self.conv2(outputs3)\n",
    "         outputs5 = F.relu(outputs4)\n",
    "         outputs6 = self.max_pool2(outputs5)\n",
    "         outputs6 = paddle.reshape(outputs6, [outputs6.shape[0], -1])\n",
    "         outputs7 = self.fc(outputs6)\n",
    "         \n",
    "         # 选择是否打印神经网络每层的参数尺寸和输出尺寸，验证网络结构是否设置正确\n",
    "         if check_shape:\n",
    "             # 打印每层网络设置的超参数-卷积核尺寸，卷积步长，卷积padding，池化核尺寸\n",
    "             print(\"\\n########## print network layer's superparams ##############\")\n",
    "             print(\"conv1-- kernel_size:{}, padding:{}, stride:{}\".format(self.conv1.weight.shape, self.conv1._padding, self.conv1._stride))\n",
    "             print(\"conv2-- kernel_size:{}, padding:{}, stride:{}\".format(self.conv2.weight.shape, self.conv2._padding, self.conv2._stride))\n",
    "             #print(\"max_pool1-- kernel_size:{}, padding:{}, stride:{}\".format(self.max_pool1.pool_size, self.max_pool1.pool_stride, self.max_pool1._stride))\n",
    "             #print(\"max_pool2-- kernel_size:{}, padding:{}, stride:{}\".format(self.max_pool2.weight.shape, self.max_pool2._padding, self.max_pool2._stride))\n",
    "             print(\"fc-- weight_size:{}, bias_size_{}\".format(self.fc.weight.shape, self.fc.bias.shape))\n",
    "             \n",
    "             # 打印每层的输出尺寸\n",
    "             print(\"\\n########## print shape of features of every layer ###############\")\n",
    "             print(\"inputs_shape: {}\".format(inputs.shape))\n",
    "             print(\"outputs1_shape: {}\".format(outputs1.shape))\n",
    "             print(\"outputs2_shape: {}\".format(outputs2.shape))\n",
    "             print(\"outputs3_shape: {}\".format(outputs3.shape))\n",
    "             print(\"outputs4_shape: {}\".format(outputs4.shape))\n",
    "             print(\"outputs5_shape: {}\".format(outputs5.shape))\n",
    "             print(\"outputs6_shape: {}\".format(outputs6.shape))\n",
    "             print(\"outputs7_shape: {}\".format(outputs7.shape))\n",
    "             # print(\"outputs8_shape: {}\".format(outputs8.shape))\n",
    "             \n",
    "         # 选择是否打印训练过程中的参数和输出内容，可用于训练过程中的调试\n",
    "         if check_content:\n",
    "            # 打印卷积层的参数-卷积核权重，权重参数较多，此处只打印部分参数\n",
    "             print(\"\\n########## print convolution layer's kernel ###############\")\n",
    "             print(\"conv1 params -- kernel weights:\", self.conv1.weight[0][0])\n",
    "             print(\"conv2 params -- kernel weights:\", self.conv2.weight[0][0])\n",
    "\n",
    "             # 创建随机数，随机打印某一个通道的输出值\n",
    "             idx1 = np.random.randint(0, outputs1.shape[1])\n",
    "             idx2 = np.random.randint(0, outputs4.shape[1])\n",
    "             # 打印卷积-池化后的结果，仅打印batch中第一个图像对应的特征\n",
    "             print(\"\\nThe {}th channel of conv1 layer: \".format(idx1), outputs1[0][idx1])\n",
    "             print(\"The {}th channel of conv2 layer: \".format(idx2), outputs4[0][idx2])\n",
    "             print(\"The output of last layer:\", outputs7[0], '\\n')\n",
    "            \n",
    "        # 如果label不是None，则计算分类精度并返回\n",
    "         if label is not None:\n",
    "             acc = paddle.metric.accuracy(input=F.softmax(outputs7), label=label)\n",
    "             return outputs7, acc\n",
    "         else:\n",
    "             return outputs7\n",
    "\n",
    "#在使用GPU机器时，可以将use_gpu变量设置成True\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')    \n",
    "\n",
    "def train(model):\n",
    "    model = MNIST()\n",
    "    model.train()\n",
    "    \n",
    "    #四种优化算法的设置方案，可以逐一尝试效果\n",
    "    opt = paddle.optimizer.SGD(learning_rate=0.01, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Momentum(learning_rate=0.01, momentum=0.9, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Adagrad(learning_rate=0.01, parameters=model.parameters())\n",
    "    # opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 1\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据，变得更加简洁\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            if batch_id == 0 and epoch_id==0:\n",
    "                # 打印模型参数和每层输出的尺寸\n",
    "                predicts, acc = model(images, labels, check_shape=True, check_content=False)\n",
    "            elif batch_id==401:\n",
    "                # 打印模型参数和每层输出的值\n",
    "                predicts, acc = model(images, labels, check_shape=False, check_content=True)\n",
    "            else:\n",
    "                predicts, acc = model(images, labels)\n",
    "            \n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 200 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "            \n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), 'mnist_test.pdparams')\n",
    "    \n",
    "#创建模型    \n",
    "model = MNIST()\n",
    "#启动训练过程\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluation ……\n",
      "loss=0.07501348815350438, acc=0.9764132165605095\n"
     ]
    }
   ],
   "source": [
    "def evaluation(model):\n",
    "    print('start evaluation ……')\n",
    "    # 定义预测过程\n",
    "    params_file_path = 'mnist.pdparams'\n",
    "    # 加载模型参数\n",
    "    param_dict = paddle.load(params_file_path)\n",
    "    model.load_dict(param_dict)\n",
    "\n",
    "    model.eval()\n",
    "    eval_loader = test_loader\n",
    "\n",
    "    acc_set = []\n",
    "    avg_loss_set = []\n",
    "    for batch_id, data in enumerate(eval_loader()):\n",
    "        images, labels = data\n",
    "        images = paddle.to_tensor(images)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        predicts, acc = model(images, labels)\n",
    "        loss = F.cross_entropy(input=predicts, label=labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        acc_set.append(float(acc.numpy()))\n",
    "        avg_loss_set.append(float(avg_loss.numpy()))\n",
    "    \n",
    "    # 计算平均\n",
    "    acc_val_mean = np.array(acc_set).mean()\n",
    "    avg_loss_val_mean = np.array(avg_loss_set).mean()\n",
    "\n",
    "    \n",
    "    print('loss={}, acc={}'.format(avg_loss_val_mean, acc_val_mean))\n",
    "\n",
    "model = MNIST()\n",
    "evaluation(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.7237353], acc is [0.109375]\n",
      "epoch: 0, batch: 100, loss is: [0.32819325], acc is [0.890625]\n",
      "epoch: 0, batch: 200, loss is: [0.2034941], acc is [0.921875]\n",
      "epoch: 0, batch: 300, loss is: [0.20030469], acc is [0.921875]\n",
      "epoch: 0, batch: 400, loss is: [0.18654649], acc is [0.9375]\n",
      "epoch: 0, batch: 500, loss is: [0.14485016], acc is [0.96875]\n",
      "epoch: 0, batch: 600, loss is: [0.14352942], acc is [0.9375]\n",
      "epoch: 0, batch: 700, loss is: [0.06066392], acc is [0.96875]\n",
      "epoch: 0, batch: 800, loss is: [0.0945859], acc is [0.96875]\n",
      "epoch: 0, batch: 900, loss is: [0.04628385], acc is [0.984375]\n",
      "epoch: 1, batch: 0, loss is: [0.0375597], acc is [0.984375]\n",
      "epoch: 1, batch: 100, loss is: [0.04833531], acc is [0.984375]\n",
      "epoch: 1, batch: 200, loss is: [0.0723602], acc is [0.96875]\n",
      "epoch: 1, batch: 300, loss is: [0.04284273], acc is [0.984375]\n",
      "epoch: 1, batch: 400, loss is: [0.03631476], acc is [0.984375]\n",
      "epoch: 1, batch: 500, loss is: [0.02013882], acc is [1.]\n",
      "epoch: 1, batch: 600, loss is: [0.03154227], acc is [1.]\n",
      "epoch: 1, batch: 700, loss is: [0.04343209], acc is [0.984375]\n",
      "epoch: 1, batch: 800, loss is: [0.02080373], acc is [0.984375]\n",
      "epoch: 1, batch: 900, loss is: [0.1556414], acc is [0.953125]\n",
      "epoch: 2, batch: 0, loss is: [0.04779391], acc is [0.984375]\n",
      "epoch: 2, batch: 100, loss is: [0.03126743], acc is [0.984375]\n",
      "epoch: 2, batch: 200, loss is: [0.01065678], acc is [1.]\n",
      "epoch: 2, batch: 300, loss is: [0.03928299], acc is [0.984375]\n",
      "epoch: 2, batch: 400, loss is: [0.09337523], acc is [0.96875]\n",
      "epoch: 2, batch: 500, loss is: [0.03532862], acc is [0.984375]\n",
      "epoch: 2, batch: 600, loss is: [0.0988458], acc is [0.96875]\n",
      "epoch: 2, batch: 700, loss is: [0.04610633], acc is [0.984375]\n",
      "epoch: 2, batch: 800, loss is: [0.0607108], acc is [0.984375]\n",
      "epoch: 2, batch: 900, loss is: [0.02509657], acc is [0.984375]\n",
      "epoch: 3, batch: 0, loss is: [0.02589535], acc is [1.]\n",
      "epoch: 3, batch: 100, loss is: [0.03234244], acc is [0.984375]\n",
      "epoch: 3, batch: 200, loss is: [0.11656073], acc is [0.96875]\n",
      "epoch: 3, batch: 300, loss is: [0.00309862], acc is [1.]\n",
      "epoch: 3, batch: 400, loss is: [0.04738146], acc is [0.984375]\n",
      "epoch: 3, batch: 500, loss is: [0.13256183], acc is [0.96875]\n",
      "epoch: 3, batch: 600, loss is: [0.48063913], acc is [0.984375]\n",
      "epoch: 3, batch: 700, loss is: [0.11340696], acc is [0.953125]\n",
      "epoch: 3, batch: 800, loss is: [0.00710717], acc is [1.]\n",
      "epoch: 3, batch: 900, loss is: [0.13379182], acc is [0.953125]\n",
      "epoch: 4, batch: 0, loss is: [0.05047082], acc is [0.984375]\n",
      "epoch: 4, batch: 100, loss is: [0.00668794], acc is [1.]\n",
      "epoch: 4, batch: 200, loss is: [0.00341121], acc is [1.]\n",
      "epoch: 4, batch: 300, loss is: [0.00057056], acc is [1.]\n",
      "epoch: 4, batch: 400, loss is: [0.05356241], acc is [0.96875]\n",
      "epoch: 4, batch: 500, loss is: [0.00649338], acc is [1.]\n",
      "epoch: 4, batch: 600, loss is: [0.01935278], acc is [0.984375]\n",
      "epoch: 4, batch: 700, loss is: [0.00955623], acc is [1.]\n",
      "epoch: 4, batch: 800, loss is: [0.02238107], acc is [0.984375]\n",
      "epoch: 4, batch: 900, loss is: [0.00528345], acc is [1.]\n",
      "epoch: 5, batch: 0, loss is: [0.03478754], acc is [0.984375]\n",
      "epoch: 5, batch: 100, loss is: [0.05357917], acc is [0.96875]\n",
      "epoch: 5, batch: 200, loss is: [0.00552247], acc is [1.]\n",
      "epoch: 5, batch: 300, loss is: [0.00837943], acc is [1.]\n",
      "epoch: 5, batch: 400, loss is: [0.00476244], acc is [1.]\n",
      "epoch: 5, batch: 500, loss is: [0.00168343], acc is [1.]\n",
      "epoch: 5, batch: 600, loss is: [0.0013497], acc is [1.]\n",
      "epoch: 5, batch: 700, loss is: [0.00206247], acc is [1.]\n",
      "epoch: 5, batch: 800, loss is: [0.05096286], acc is [0.984375]\n",
      "epoch: 5, batch: 900, loss is: [0.0231536], acc is [0.984375]\n",
      "epoch: 6, batch: 0, loss is: [0.00360693], acc is [1.]\n",
      "epoch: 6, batch: 100, loss is: [0.00393834], acc is [1.]\n",
      "epoch: 6, batch: 200, loss is: [0.01146927], acc is [1.]\n",
      "epoch: 6, batch: 300, loss is: [0.0082303], acc is [1.]\n",
      "epoch: 6, batch: 400, loss is: [0.00808164], acc is [1.]\n",
      "epoch: 6, batch: 500, loss is: [0.052568], acc is [0.984375]\n",
      "epoch: 6, batch: 600, loss is: [0.03400715], acc is [0.984375]\n",
      "epoch: 6, batch: 700, loss is: [0.01252626], acc is [1.]\n",
      "epoch: 6, batch: 800, loss is: [0.03494349], acc is [0.984375]\n",
      "epoch: 6, batch: 900, loss is: [0.02082938], acc is [0.984375]\n",
      "epoch: 7, batch: 0, loss is: [0.00650427], acc is [1.]\n",
      "epoch: 7, batch: 100, loss is: [0.00291281], acc is [1.]\n",
      "epoch: 7, batch: 200, loss is: [0.00254895], acc is [1.]\n",
      "epoch: 7, batch: 300, loss is: [0.00463171], acc is [1.]\n",
      "epoch: 7, batch: 400, loss is: [0.06646475], acc is [0.96875]\n",
      "epoch: 7, batch: 500, loss is: [0.0051891], acc is [1.]\n",
      "epoch: 7, batch: 600, loss is: [0.00347033], acc is [1.]\n",
      "epoch: 7, batch: 700, loss is: [0.00277202], acc is [1.]\n",
      "epoch: 7, batch: 800, loss is: [0.0012123], acc is [1.]\n",
      "epoch: 7, batch: 900, loss is: [0.00688713], acc is [1.]\n",
      "epoch: 8, batch: 0, loss is: [0.00406706], acc is [1.]\n",
      "epoch: 8, batch: 100, loss is: [0.00334674], acc is [1.]\n",
      "epoch: 8, batch: 200, loss is: [0.3034104], acc is [0.984375]\n",
      "epoch: 8, batch: 300, loss is: [0.00033282], acc is [1.]\n",
      "epoch: 8, batch: 400, loss is: [0.01188678], acc is [1.]\n",
      "epoch: 8, batch: 500, loss is: [0.02562316], acc is [0.984375]\n",
      "epoch: 8, batch: 600, loss is: [0.01762437], acc is [0.984375]\n",
      "epoch: 8, batch: 700, loss is: [0.00038742], acc is [1.]\n",
      "epoch: 8, batch: 800, loss is: [0.00207564], acc is [1.]\n",
      "epoch: 8, batch: 900, loss is: [0.00258538], acc is [1.]\n",
      "epoch: 9, batch: 0, loss is: [0.00040599], acc is [1.]\n",
      "epoch: 9, batch: 100, loss is: [5.1682087e-05], acc is [1.]\n",
      "epoch: 9, batch: 200, loss is: [0.00205752], acc is [1.]\n",
      "epoch: 9, batch: 300, loss is: [0.00021422], acc is [1.]\n",
      "epoch: 9, batch: 400, loss is: [0.00833133], acc is [1.]\n",
      "epoch: 9, batch: 500, loss is: [0.0002267], acc is [1.]\n",
      "epoch: 9, batch: 600, loss is: [0.00156926], acc is [1.]\n",
      "epoch: 9, batch: 700, loss is: [0.00383577], acc is [1.]\n",
      "epoch: 9, batch: 800, loss is: [0.0018361], acc is [1.]\n",
      "epoch: 9, batch: 900, loss is: [0.07397968], acc is [0.96875]\n"
     ]
    }
   ],
   "source": [
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir='./log')\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    \n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "    \n",
    "    EPOCH_NUM = 10\n",
    "    iter = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            #准备数据，变得更加简洁\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "            \n",
    "            #前向计算的过程，同时拿到模型输出值和分类准确率\n",
    "            predicts, avg_acc = model(images, labels)\n",
    "            #计算损失，取一个批次样本损失的平均值\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "            #每训练了100批次的数据，打印下当前Loss的情况\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), avg_acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc', step = iter, value = avg_acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss', step = iter, value = avg_loss.numpy())\n",
    "                iter = iter + 100\n",
    "\n",
    "            #后向传播，更新参数的过程\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "    #保存模型参数\n",
    "    paddle.save(model.state_dict(), 'mnist.pdparams')\n",
    "    \n",
    "model = MNIST()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualdl\n",
    "import visualdl.server.app \n",
    "visualdl.server.app.run('./log',\n",
    "                        host=\"127.0.0.1\",\n",
    "                        port=8080,\n",
    "                        cache_timeout=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
