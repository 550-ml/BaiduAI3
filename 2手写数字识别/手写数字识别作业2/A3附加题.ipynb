{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 附加题中，我们选择使用cifar10\n",
    "import paddle\n",
    "from paddle.vision.transforms import Normalize, Compose, Transpose\n",
    "from paddle.vision.datasets import Cifar10\n",
    "def get_cifar10_dataloader():\n",
    "    # cifar10是一个三通道的图片\n",
    "    transforms = Compose([Normalize(mean=[127.5, 127.5, 127.5], std= [127.5, 127.5, 127.5], data_format='HWC'), Transpose() ])\n",
    "    # 数据集\n",
    "    train_datasets = Cifar10(mode='train', transform=transforms)\n",
    "    test_datasets = Cifar10(mode='test', transform=transforms)\n",
    "    # 数据读取器\n",
    "    train_loader = paddle.io.DataLoader(train_datasets, batch_size=64, shuffle = True, num_workers=1, drop_last=True)\n",
    "    test_loader = paddle.io.DataLoader(test_datasets, batch_size=64, shuffle = False, num_workers=1, drop_last=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\io\\reader.py:433: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_cifar10_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17988"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualdl\n",
    "import visualdl.server.app\n",
    "visualdl.server.app.run('./run/cifar_log',\n",
    "                        host = \"127.0.0.1\",\n",
    "                        port=8081,\n",
    "                        cache_timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 首先只来一层卷积\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_ONE_CONV(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_ONE_CONV, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=5120, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/one_conv\")\n",
    "def train_Cifar_ONE(model):\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 10\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        total_test_accuracy = 0  # 用于累积测试准确率\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            total_test_accuracy += acc.numpy()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "        \n",
    "        total_test_accuracy /= len(test_loader())\n",
    "        logwriter.add_scalar(tag='test/total_acc', step=epoch_id, value=total_test_accuracy)\n",
    "            \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.591126], acc is [0.0625]\n",
      "epoch: 0, batch: 150, loss is: [1.3860409], acc is [0.5]\n",
      "epoch: 0, batch: 300, loss is: [1.6586318], acc is [0.421875]\n",
      "epoch: 0, batch: 450, loss is: [1.3182759], acc is [0.546875]\n",
      "epoch: 0, batch: 600, loss is: [1.3328769], acc is [0.5]\n",
      "epoch: 0, batch: 750, loss is: [1.2842821], acc is [0.5]\n",
      "epoch: 1, batch: 0, loss is: [1.5553687], acc is [0.4375]\n",
      "epoch: 1, batch: 150, loss is: [1.4089432], acc is [0.46875]\n",
      "epoch: 1, batch: 300, loss is: [1.7016492], acc is [0.40625]\n",
      "epoch: 1, batch: 450, loss is: [1.7483552], acc is [0.4375]\n",
      "epoch: 1, batch: 600, loss is: [1.9843371], acc is [0.484375]\n",
      "epoch: 1, batch: 750, loss is: [1.5778127], acc is [0.59375]\n",
      "epoch: 2, batch: 0, loss is: [0.99006325], acc is [0.671875]\n",
      "epoch: 2, batch: 150, loss is: [1.0828569], acc is [0.671875]\n",
      "epoch: 2, batch: 300, loss is: [1.5346546], acc is [0.515625]\n",
      "epoch: 2, batch: 450, loss is: [1.424325], acc is [0.546875]\n",
      "epoch: 2, batch: 600, loss is: [1.148083], acc is [0.484375]\n",
      "epoch: 2, batch: 750, loss is: [1.606745], acc is [0.546875]\n",
      "epoch: 3, batch: 0, loss is: [1.0138097], acc is [0.71875]\n",
      "epoch: 3, batch: 150, loss is: [1.4130068], acc is [0.59375]\n",
      "epoch: 3, batch: 300, loss is: [1.8123772], acc is [0.421875]\n",
      "epoch: 3, batch: 450, loss is: [1.8230156], acc is [0.4375]\n",
      "epoch: 3, batch: 600, loss is: [1.5236015], acc is [0.46875]\n",
      "epoch: 3, batch: 750, loss is: [1.2659686], acc is [0.625]\n",
      "epoch: 4, batch: 0, loss is: [1.6685276], acc is [0.46875]\n",
      "epoch: 4, batch: 150, loss is: [1.6474288], acc is [0.484375]\n",
      "epoch: 4, batch: 300, loss is: [1.3585172], acc is [0.5625]\n",
      "epoch: 4, batch: 450, loss is: [1.5800259], acc is [0.4375]\n",
      "epoch: 4, batch: 600, loss is: [1.9334084], acc is [0.484375]\n",
      "epoch: 4, batch: 750, loss is: [1.9059424], acc is [0.5625]\n",
      "epoch: 5, batch: 0, loss is: [1.2359545], acc is [0.609375]\n",
      "epoch: 5, batch: 150, loss is: [1.5085492], acc is [0.59375]\n",
      "epoch: 5, batch: 300, loss is: [1.3632245], acc is [0.578125]\n",
      "epoch: 5, batch: 450, loss is: [1.3946587], acc is [0.53125]\n",
      "epoch: 5, batch: 600, loss is: [1.3113455], acc is [0.484375]\n",
      "epoch: 5, batch: 750, loss is: [1.3868012], acc is [0.5625]\n",
      "epoch: 6, batch: 0, loss is: [1.2346315], acc is [0.578125]\n",
      "epoch: 6, batch: 150, loss is: [1.3036747], acc is [0.625]\n",
      "epoch: 6, batch: 300, loss is: [1.3903413], acc is [0.515625]\n",
      "epoch: 6, batch: 450, loss is: [1.1993575], acc is [0.609375]\n",
      "epoch: 6, batch: 600, loss is: [1.2789807], acc is [0.625]\n",
      "epoch: 6, batch: 750, loss is: [1.6110198], acc is [0.515625]\n",
      "epoch: 7, batch: 0, loss is: [1.2400544], acc is [0.625]\n",
      "epoch: 7, batch: 150, loss is: [1.0377488], acc is [0.6875]\n",
      "epoch: 7, batch: 300, loss is: [1.6593614], acc is [0.5]\n",
      "epoch: 7, batch: 450, loss is: [1.1471016], acc is [0.59375]\n",
      "epoch: 7, batch: 600, loss is: [1.2781821], acc is [0.5625]\n",
      "epoch: 7, batch: 750, loss is: [1.1238511], acc is [0.5]\n",
      "epoch: 8, batch: 0, loss is: [1.2169787], acc is [0.625]\n",
      "epoch: 8, batch: 150, loss is: [0.9222016], acc is [0.6875]\n",
      "epoch: 8, batch: 300, loss is: [1.1557806], acc is [0.546875]\n",
      "epoch: 8, batch: 450, loss is: [1.1885433], acc is [0.578125]\n",
      "epoch: 8, batch: 600, loss is: [1.4046565], acc is [0.5625]\n",
      "epoch: 8, batch: 750, loss is: [1.1105142], acc is [0.59375]\n",
      "epoch: 9, batch: 0, loss is: [1.2819945], acc is [0.53125]\n",
      "epoch: 9, batch: 150, loss is: [1.4741534], acc is [0.484375]\n",
      "epoch: 9, batch: 300, loss is: [1.3179073], acc is [0.546875]\n",
      "epoch: 9, batch: 450, loss is: [1.2120701], acc is [0.5625]\n",
      "epoch: 9, batch: 600, loss is: [1.4861335], acc is [0.453125]\n",
      "epoch: 9, batch: 750, loss is: [1.3195872], acc is [0.53125]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_one = Cifar_ONE_CONV()\n",
    "train_Cifar_ONE(model_cifar_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_TWO_CONV(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=1536, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv\")\n",
    "def train_Cifar_TWO(model):\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 10  \n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        total_test_accuracy=0\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            total_test_accuracy += acc.numpy()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "        \n",
    "        total_test_accuracy /= len(test_loader())\n",
    "        logwriter.add_scalar(tag='test/total_acc', step=epoch_id, value=total_test_accuracy)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.779589], acc is [0.140625]\n",
      "epoch: 0, batch: 150, loss is: [2.3006978], acc is [0.09375]\n",
      "epoch: 0, batch: 300, loss is: [2.3085442], acc is [0.0625]\n",
      "epoch: 0, batch: 450, loss is: [2.2996986], acc is [0.140625]\n",
      "epoch: 0, batch: 600, loss is: [2.3036814], acc is [0.078125]\n",
      "epoch: 0, batch: 750, loss is: [2.3033], acc is [0.046875]\n",
      "epoch: 1, batch: 0, loss is: [2.3024077], acc is [0.1875]\n",
      "epoch: 1, batch: 150, loss is: [2.2998648], acc is [0.125]\n",
      "epoch: 1, batch: 300, loss is: [2.3046794], acc is [0.140625]\n",
      "epoch: 1, batch: 450, loss is: [2.3101408], acc is [0.140625]\n",
      "epoch: 1, batch: 600, loss is: [2.3043513], acc is [0.09375]\n",
      "epoch: 1, batch: 750, loss is: [2.2676044], acc is [0.09375]\n",
      "epoch: 2, batch: 0, loss is: [2.157767], acc is [0.1875]\n",
      "epoch: 2, batch: 150, loss is: [2.1264591], acc is [0.234375]\n",
      "epoch: 2, batch: 300, loss is: [1.9482131], acc is [0.296875]\n",
      "epoch: 2, batch: 450, loss is: [1.9311458], acc is [0.328125]\n",
      "epoch: 2, batch: 600, loss is: [1.4913139], acc is [0.515625]\n",
      "epoch: 2, batch: 750, loss is: [1.627842], acc is [0.4375]\n",
      "epoch: 3, batch: 0, loss is: [1.7887897], acc is [0.359375]\n",
      "epoch: 3, batch: 150, loss is: [1.8625277], acc is [0.34375]\n",
      "epoch: 3, batch: 300, loss is: [1.3586428], acc is [0.46875]\n",
      "epoch: 3, batch: 450, loss is: [1.835011], acc is [0.375]\n",
      "epoch: 3, batch: 600, loss is: [1.4880346], acc is [0.5]\n",
      "epoch: 3, batch: 750, loss is: [1.3995185], acc is [0.546875]\n",
      "epoch: 4, batch: 0, loss is: [1.510181], acc is [0.4375]\n",
      "epoch: 4, batch: 150, loss is: [1.6927247], acc is [0.453125]\n",
      "epoch: 4, batch: 300, loss is: [1.5058777], acc is [0.5625]\n",
      "epoch: 4, batch: 450, loss is: [1.4460961], acc is [0.53125]\n",
      "epoch: 4, batch: 600, loss is: [1.346287], acc is [0.515625]\n",
      "epoch: 4, batch: 750, loss is: [1.4622848], acc is [0.40625]\n",
      "epoch: 5, batch: 0, loss is: [1.5157661], acc is [0.421875]\n",
      "epoch: 5, batch: 150, loss is: [1.4023224], acc is [0.4375]\n",
      "epoch: 5, batch: 300, loss is: [1.5250664], acc is [0.5]\n",
      "epoch: 5, batch: 450, loss is: [1.4378669], acc is [0.375]\n",
      "epoch: 5, batch: 600, loss is: [1.1814308], acc is [0.640625]\n",
      "epoch: 5, batch: 750, loss is: [1.6573485], acc is [0.40625]\n",
      "epoch: 6, batch: 0, loss is: [1.4492824], acc is [0.515625]\n",
      "epoch: 6, batch: 150, loss is: [1.3089219], acc is [0.5625]\n",
      "epoch: 6, batch: 300, loss is: [1.1262121], acc is [0.59375]\n",
      "epoch: 6, batch: 450, loss is: [1.1387523], acc is [0.5625]\n",
      "epoch: 6, batch: 600, loss is: [1.0888435], acc is [0.671875]\n",
      "epoch: 6, batch: 750, loss is: [1.1552951], acc is [0.609375]\n",
      "epoch: 7, batch: 0, loss is: [1.1688895], acc is [0.53125]\n",
      "epoch: 7, batch: 150, loss is: [1.6807525], acc is [0.375]\n",
      "epoch: 7, batch: 300, loss is: [1.3691639], acc is [0.484375]\n",
      "epoch: 7, batch: 450, loss is: [1.1892445], acc is [0.578125]\n",
      "epoch: 7, batch: 600, loss is: [1.4766235], acc is [0.546875]\n",
      "epoch: 7, batch: 750, loss is: [1.2595106], acc is [0.515625]\n",
      "epoch: 8, batch: 0, loss is: [1.236192], acc is [0.5625]\n",
      "epoch: 8, batch: 150, loss is: [1.3065782], acc is [0.5]\n",
      "epoch: 8, batch: 300, loss is: [1.5203745], acc is [0.484375]\n",
      "epoch: 8, batch: 450, loss is: [1.3215716], acc is [0.515625]\n",
      "epoch: 8, batch: 600, loss is: [1.2104776], acc is [0.65625]\n",
      "epoch: 8, batch: 750, loss is: [1.2595866], acc is [0.515625]\n",
      "epoch: 9, batch: 0, loss is: [1.243851], acc is [0.578125]\n",
      "epoch: 9, batch: 150, loss is: [1.3125374], acc is [0.546875]\n",
      "epoch: 9, batch: 300, loss is: [1.5990217], acc is [0.34375]\n",
      "epoch: 9, batch: 450, loss is: [1.1948636], acc is [0.578125]\n",
      "epoch: 9, batch: 600, loss is: [1.2145499], acc is [0.515625]\n",
      "epoch: 9, batch: 750, loss is: [1.3916966], acc is [0.484375]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_two = Cifar_TWO_CONV()\n",
    "train_Cifar_TWO(model_cifar_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 首先只来一层卷积\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_THree_CONV2(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_THree_CONV2, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 再定义一层卷积层\n",
    "        self.conv3 = Conv2D(in_channels=20, out_channels=32,kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool3 = MaxPool2D(kernel_size=2,stride=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=2048, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/three_conv\")\n",
    "def train_Cifar_Three(model):\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 10\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        total_test_accuracy=0\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            total_test_accuracy += acc.numpy()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "        \n",
    "        total_test_accuracy /= len(test_loader())\n",
    "        logwriter.add_scalar(tag='test/total_acc', step=epoch_id, value=total_test_accuracy)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.321579], acc is [0.109375]\n",
      "epoch: 0, batch: 150, loss is: [1.4878459], acc is [0.5]\n",
      "epoch: 0, batch: 300, loss is: [1.502988], acc is [0.375]\n",
      "epoch: 0, batch: 450, loss is: [1.3144822], acc is [0.53125]\n",
      "epoch: 0, batch: 600, loss is: [1.5574405], acc is [0.46875]\n",
      "epoch: 0, batch: 750, loss is: [1.1929733], acc is [0.609375]\n",
      "epoch: 1, batch: 0, loss is: [1.4715104], acc is [0.5]\n",
      "epoch: 1, batch: 150, loss is: [1.2889777], acc is [0.515625]\n",
      "epoch: 1, batch: 300, loss is: [1.548099], acc is [0.484375]\n",
      "epoch: 1, batch: 450, loss is: [1.4900177], acc is [0.46875]\n",
      "epoch: 1, batch: 600, loss is: [1.1400313], acc is [0.59375]\n",
      "epoch: 1, batch: 750, loss is: [1.6508226], acc is [0.40625]\n",
      "epoch: 2, batch: 0, loss is: [1.4976201], acc is [0.4375]\n",
      "epoch: 2, batch: 150, loss is: [1.3817183], acc is [0.5]\n",
      "epoch: 2, batch: 300, loss is: [1.3742287], acc is [0.453125]\n",
      "epoch: 2, batch: 450, loss is: [1.5464451], acc is [0.5625]\n",
      "epoch: 2, batch: 600, loss is: [1.3242497], acc is [0.546875]\n",
      "epoch: 2, batch: 750, loss is: [1.7140722], acc is [0.390625]\n",
      "epoch: 3, batch: 0, loss is: [1.3617277], acc is [0.5]\n",
      "epoch: 3, batch: 150, loss is: [1.1825875], acc is [0.625]\n",
      "epoch: 3, batch: 300, loss is: [1.0954431], acc is [0.59375]\n",
      "epoch: 3, batch: 450, loss is: [1.3056998], acc is [0.515625]\n",
      "epoch: 3, batch: 600, loss is: [1.2168849], acc is [0.515625]\n",
      "epoch: 3, batch: 750, loss is: [1.5687301], acc is [0.5]\n",
      "epoch: 4, batch: 0, loss is: [1.3435276], acc is [0.46875]\n",
      "epoch: 4, batch: 150, loss is: [1.514787], acc is [0.453125]\n",
      "epoch: 4, batch: 300, loss is: [1.1981106], acc is [0.53125]\n",
      "epoch: 4, batch: 450, loss is: [1.4283596], acc is [0.40625]\n",
      "epoch: 4, batch: 600, loss is: [1.4344621], acc is [0.453125]\n",
      "epoch: 4, batch: 750, loss is: [1.4503305], acc is [0.5]\n",
      "epoch: 5, batch: 0, loss is: [1.2471395], acc is [0.546875]\n",
      "epoch: 5, batch: 150, loss is: [1.2824821], acc is [0.546875]\n",
      "epoch: 5, batch: 300, loss is: [1.3028983], acc is [0.53125]\n",
      "epoch: 5, batch: 450, loss is: [1.136545], acc is [0.578125]\n",
      "epoch: 5, batch: 600, loss is: [1.5311017], acc is [0.5]\n",
      "epoch: 5, batch: 750, loss is: [1.1234449], acc is [0.640625]\n",
      "epoch: 6, batch: 0, loss is: [1.0845089], acc is [0.546875]\n",
      "epoch: 6, batch: 150, loss is: [1.3056052], acc is [0.5]\n",
      "epoch: 6, batch: 300, loss is: [1.1410203], acc is [0.59375]\n",
      "epoch: 6, batch: 450, loss is: [1.2836692], acc is [0.453125]\n",
      "epoch: 6, batch: 600, loss is: [1.3455675], acc is [0.5]\n",
      "epoch: 6, batch: 750, loss is: [1.185667], acc is [0.578125]\n",
      "epoch: 7, batch: 0, loss is: [1.2938337], acc is [0.546875]\n",
      "epoch: 7, batch: 150, loss is: [1.1417136], acc is [0.671875]\n",
      "epoch: 7, batch: 300, loss is: [1.4817624], acc is [0.484375]\n",
      "epoch: 7, batch: 450, loss is: [1.2696455], acc is [0.53125]\n",
      "epoch: 7, batch: 600, loss is: [1.2401958], acc is [0.53125]\n",
      "epoch: 7, batch: 750, loss is: [1.0696707], acc is [0.640625]\n",
      "epoch: 8, batch: 0, loss is: [1.3144238], acc is [0.515625]\n",
      "epoch: 8, batch: 150, loss is: [1.1561859], acc is [0.5625]\n",
      "epoch: 8, batch: 300, loss is: [1.3402545], acc is [0.40625]\n",
      "epoch: 8, batch: 450, loss is: [1.2388376], acc is [0.5625]\n",
      "epoch: 8, batch: 600, loss is: [1.1344459], acc is [0.546875]\n",
      "epoch: 8, batch: 750, loss is: [1.5274503], acc is [0.453125]\n",
      "epoch: 9, batch: 0, loss is: [1.3183353], acc is [0.484375]\n",
      "epoch: 9, batch: 150, loss is: [1.4342389], acc is [0.453125]\n",
      "epoch: 9, batch: 300, loss is: [1.3831974], acc is [0.5]\n",
      "epoch: 9, batch: 450, loss is: [1.06954], acc is [0.609375]\n",
      "epoch: 9, batch: 600, loss is: [1.3379653], acc is [0.515625]\n",
      "epoch: 9, batch: 750, loss is: [1.2770432], acc is [0.5625]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_two2 = Cifar_THree_CONV2()\n",
    "train_Cifar_Three(model_cifar_two2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 使用dropout\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_TWO_CONV2_drop(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_drop, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 再定义一层卷积层\n",
    "        self.conv3 = Conv2D(in_channels=20, out_channels=32,kernel_size=5, stride=1, padding=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=2048, out_features=3072)\n",
    "        self.fc2 = Linear(in_features=3072,out_features=1024)\n",
    "        self.fc3 = Linear(in_features=1024, out_features=10)\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        x = F.dropout(x, p = 0.007,mode=mode)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p = 0.005, mode=mode)\n",
    "        x = self.fc3(x)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/three_conv_drop\")\n",
    "def train_Cifar_TWO2_drop(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 15\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        total_test_accuracy=0\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            total_test_accuracy += acc.numpy()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "        \n",
    "        total_test_accuracy /= len(test_loader())\n",
    "        logwriter.add_scalar(tag='test/total_acc', step=epoch_id, value=total_test_accuracy)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.6706896], acc is [0.109375]\n",
      "epoch: 0, batch: 150, loss is: [2.399619], acc is [0.078125]\n",
      "epoch: 0, batch: 300, loss is: [2.36034], acc is [0.09375]\n",
      "epoch: 0, batch: 450, loss is: [2.3338962], acc is [0.078125]\n",
      "epoch: 0, batch: 600, loss is: [2.2996335], acc is [0.078125]\n",
      "epoch: 0, batch: 750, loss is: [2.305578], acc is [0.109375]\n",
      "epoch: 1, batch: 0, loss is: [2.2970946], acc is [0.078125]\n",
      "epoch: 1, batch: 150, loss is: [2.4451375], acc is [0.078125]\n",
      "epoch: 1, batch: 300, loss is: [2.3126059], acc is [0.140625]\n",
      "epoch: 1, batch: 450, loss is: [2.3647337], acc is [0.109375]\n",
      "epoch: 1, batch: 600, loss is: [2.357091], acc is [0.0625]\n",
      "epoch: 1, batch: 750, loss is: [2.3048577], acc is [0.140625]\n",
      "epoch: 2, batch: 0, loss is: [2.3547986], acc is [0.109375]\n",
      "epoch: 2, batch: 150, loss is: [2.3652806], acc is [0.171875]\n",
      "epoch: 2, batch: 300, loss is: [2.314753], acc is [0.09375]\n",
      "epoch: 2, batch: 450, loss is: [3262.0986], acc is [0.0625]\n",
      "epoch: 2, batch: 600, loss is: [124.69141], acc is [0.125]\n",
      "epoch: 2, batch: 750, loss is: [9.03786], acc is [0.109375]\n",
      "epoch: 3, batch: 0, loss is: [6.040266], acc is [0.046875]\n",
      "epoch: 3, batch: 150, loss is: [3.3069105], acc is [0.109375]\n",
      "epoch: 3, batch: 300, loss is: [2.8060143], acc is [0.078125]\n",
      "epoch: 3, batch: 450, loss is: [3.1495113], acc is [0.078125]\n",
      "epoch: 3, batch: 600, loss is: [2.784768], acc is [0.140625]\n",
      "epoch: 3, batch: 750, loss is: [3.2344987], acc is [0.078125]\n",
      "epoch: 4, batch: 0, loss is: [2.5814912], acc is [0.078125]\n",
      "epoch: 4, batch: 150, loss is: [3.0429707], acc is [0.109375]\n",
      "epoch: 4, batch: 300, loss is: [2.4582634], acc is [0.125]\n",
      "epoch: 4, batch: 450, loss is: [2.8124309], acc is [0.09375]\n",
      "epoch: 4, batch: 600, loss is: [2.8372855], acc is [0.109375]\n",
      "epoch: 4, batch: 750, loss is: [2.4068203], acc is [0.125]\n",
      "epoch: 5, batch: 0, loss is: [2.5114655], acc is [0.078125]\n",
      "epoch: 5, batch: 150, loss is: [2.6027288], acc is [0.09375]\n",
      "epoch: 5, batch: 300, loss is: [2.4634964], acc is [0.09375]\n",
      "epoch: 5, batch: 450, loss is: [2.4954064], acc is [0.125]\n",
      "epoch: 5, batch: 600, loss is: [2.506998], acc is [0.09375]\n",
      "epoch: 5, batch: 750, loss is: [2.5097299], acc is [0.09375]\n",
      "epoch: 6, batch: 0, loss is: [2.488224], acc is [0.078125]\n",
      "epoch: 6, batch: 150, loss is: [2.6705217], acc is [0.046875]\n",
      "epoch: 6, batch: 300, loss is: [2.5263047], acc is [0.109375]\n",
      "epoch: 6, batch: 450, loss is: [2.326428], acc is [0.078125]\n",
      "epoch: 6, batch: 600, loss is: [2.3902807], acc is [0.09375]\n",
      "epoch: 6, batch: 750, loss is: [2.6190624], acc is [0.078125]\n",
      "epoch: 7, batch: 0, loss is: [2.5274181], acc is [0.078125]\n",
      "epoch: 7, batch: 150, loss is: [2.3627427], acc is [0.125]\n",
      "epoch: 7, batch: 300, loss is: [2.3128529], acc is [0.09375]\n",
      "epoch: 7, batch: 450, loss is: [2.3991964], acc is [0.09375]\n",
      "epoch: 7, batch: 600, loss is: [2.427661], acc is [0.046875]\n",
      "epoch: 7, batch: 750, loss is: [2.4758325], acc is [0.125]\n",
      "epoch: 8, batch: 0, loss is: [2.4065773], acc is [0.078125]\n",
      "epoch: 8, batch: 150, loss is: [2.480135], acc is [0.03125]\n",
      "epoch: 8, batch: 300, loss is: [2.542946], acc is [0.125]\n",
      "epoch: 8, batch: 450, loss is: [312.0035], acc is [0.125]\n",
      "epoch: 8, batch: 600, loss is: [15.79865], acc is [0.109375]\n",
      "epoch: 8, batch: 750, loss is: [2.9010766], acc is [0.125]\n",
      "epoch: 9, batch: 0, loss is: [2.6257987], acc is [0.0625]\n",
      "epoch: 9, batch: 150, loss is: [2.5948806], acc is [0.09375]\n",
      "epoch: 9, batch: 300, loss is: [2.6378317], acc is [0.15625]\n",
      "epoch: 9, batch: 450, loss is: [2.442668], acc is [0.09375]\n",
      "epoch: 9, batch: 600, loss is: [2.3803368], acc is [0.125]\n",
      "epoch: 9, batch: 750, loss is: [2.639248], acc is [0.125]\n",
      "epoch: 10, batch: 0, loss is: [2.4013457], acc is [0.0625]\n",
      "epoch: 10, batch: 150, loss is: [2.3720326], acc is [0.109375]\n",
      "epoch: 10, batch: 300, loss is: [2.4448504], acc is [0.078125]\n",
      "epoch: 10, batch: 450, loss is: [2.6425276], acc is [0.09375]\n",
      "epoch: 10, batch: 600, loss is: [2.4331944], acc is [0.109375]\n",
      "epoch: 10, batch: 750, loss is: [2.7158537], acc is [0.078125]\n",
      "epoch: 11, batch: 0, loss is: [2.3922305], acc is [0.09375]\n",
      "epoch: 11, batch: 150, loss is: [2.6998842], acc is [0.046875]\n",
      "epoch: 11, batch: 300, loss is: [2.3379135], acc is [0.078125]\n",
      "epoch: 11, batch: 450, loss is: [2.4852505], acc is [0.140625]\n",
      "epoch: 11, batch: 600, loss is: [2.5036254], acc is [0.125]\n",
      "epoch: 11, batch: 750, loss is: [2.5233254], acc is [0.09375]\n",
      "epoch: 12, batch: 0, loss is: [2.4112446], acc is [0.109375]\n",
      "epoch: 12, batch: 150, loss is: [2.3869455], acc is [0.0625]\n",
      "epoch: 12, batch: 300, loss is: [2.6763182], acc is [0.078125]\n",
      "epoch: 12, batch: 450, loss is: [2.435577], acc is [0.109375]\n",
      "epoch: 12, batch: 600, loss is: [2.458991], acc is [0.09375]\n",
      "epoch: 12, batch: 750, loss is: [2.5210338], acc is [0.078125]\n",
      "epoch: 13, batch: 0, loss is: [2.4836097], acc is [0.078125]\n",
      "epoch: 13, batch: 150, loss is: [2.359448], acc is [0.109375]\n",
      "epoch: 13, batch: 300, loss is: [2.3525019], acc is [0.109375]\n",
      "epoch: 13, batch: 450, loss is: [2.3919404], acc is [0.0625]\n",
      "epoch: 13, batch: 600, loss is: [2.4309292], acc is [0.046875]\n",
      "epoch: 13, batch: 750, loss is: [2.4025578], acc is [0.09375]\n",
      "epoch: 14, batch: 0, loss is: [2.3268116], acc is [0.125]\n",
      "epoch: 14, batch: 150, loss is: [2.3957162], acc is [0.078125]\n",
      "epoch: 14, batch: 300, loss is: [2.3391356], acc is [0.0625]\n",
      "epoch: 14, batch: 450, loss is: [438.85764], acc is [0.09375]\n",
      "epoch: 14, batch: 600, loss is: [400.5636], acc is [0.078125]\n",
      "epoch: 14, batch: 750, loss is: [12.720661], acc is [0.109375]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_drop = Cifar_TWO_CONV2_drop()\n",
    "train_Cifar_TWO2_drop(model_cifar_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, BatchNorm2D\n",
    "\n",
    "class Cifar_TWO_CONV2_norm(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_norm, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义批归一化层\n",
    "        self.bn1 = BatchNorm2D(12)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=2)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = BatchNorm2D(24)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第三个卷积层\n",
    "        self.conv3 = Conv2D(in_channels=24, out_channels= 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = BatchNorm2D(32)\n",
    "        self.max_pool3 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是32*8*8,定义一个全连接\n",
    "        self.fc = Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = F.dropout(x, p = 0.01,mode=mode)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.nn import BatchNorm2D\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_new_batnorm\")\n",
    "def train_Cifar_TWO2_norm(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 10\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        total_test_accuracy=0\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            total_test_accuracy += acc.numpy()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "        \n",
    "        total_test_accuracy /= len(test_loader())\n",
    "        logwriter.add_scalar(tag='test/total_acc', step=epoch_id, value=total_test_accuracy)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:777: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.8470454], acc is [0.0625]\n",
      "epoch: 0, batch: 150, loss is: [1.3579888], acc is [0.46875]\n",
      "epoch: 0, batch: 300, loss is: [1.173275], acc is [0.546875]\n",
      "epoch: 0, batch: 450, loss is: [1.1697747], acc is [0.625]\n",
      "epoch: 0, batch: 600, loss is: [1.308835], acc is [0.5625]\n",
      "epoch: 0, batch: 750, loss is: [1.1410599], acc is [0.53125]\n",
      "epoch: 1, batch: 0, loss is: [1.1541077], acc is [0.59375]\n",
      "epoch: 1, batch: 150, loss is: [1.0717705], acc is [0.640625]\n",
      "epoch: 1, batch: 300, loss is: [1.142252], acc is [0.640625]\n",
      "epoch: 1, batch: 450, loss is: [1.0128442], acc is [0.609375]\n",
      "epoch: 1, batch: 600, loss is: [0.9101277], acc is [0.765625]\n",
      "epoch: 1, batch: 750, loss is: [0.94764787], acc is [0.671875]\n",
      "epoch: 2, batch: 0, loss is: [0.8985741], acc is [0.71875]\n",
      "epoch: 2, batch: 150, loss is: [0.921988], acc is [0.65625]\n",
      "epoch: 2, batch: 300, loss is: [0.7929771], acc is [0.75]\n",
      "epoch: 2, batch: 450, loss is: [0.8885379], acc is [0.703125]\n",
      "epoch: 2, batch: 600, loss is: [0.81782955], acc is [0.640625]\n",
      "epoch: 2, batch: 750, loss is: [1.0406723], acc is [0.65625]\n",
      "epoch: 3, batch: 0, loss is: [0.7228193], acc is [0.734375]\n",
      "epoch: 3, batch: 150, loss is: [0.8830439], acc is [0.671875]\n",
      "epoch: 3, batch: 300, loss is: [0.88856685], acc is [0.671875]\n",
      "epoch: 3, batch: 450, loss is: [0.86885315], acc is [0.703125]\n",
      "epoch: 3, batch: 600, loss is: [0.70155287], acc is [0.6875]\n",
      "epoch: 3, batch: 750, loss is: [1.019598], acc is [0.65625]\n",
      "epoch: 4, batch: 0, loss is: [0.837131], acc is [0.703125]\n",
      "epoch: 4, batch: 150, loss is: [0.88915443], acc is [0.703125]\n",
      "epoch: 4, batch: 300, loss is: [0.9020985], acc is [0.6875]\n",
      "epoch: 4, batch: 450, loss is: [0.9204312], acc is [0.703125]\n",
      "epoch: 4, batch: 600, loss is: [0.6318008], acc is [0.71875]\n",
      "epoch: 4, batch: 750, loss is: [0.6928241], acc is [0.765625]\n",
      "epoch: 5, batch: 0, loss is: [0.8696827], acc is [0.71875]\n",
      "epoch: 5, batch: 150, loss is: [0.6464192], acc is [0.828125]\n",
      "epoch: 5, batch: 300, loss is: [0.78671885], acc is [0.6875]\n",
      "epoch: 5, batch: 450, loss is: [1.1720673], acc is [0.578125]\n",
      "epoch: 5, batch: 600, loss is: [0.74215853], acc is [0.75]\n",
      "epoch: 5, batch: 750, loss is: [0.62222326], acc is [0.734375]\n",
      "epoch: 6, batch: 0, loss is: [0.74691796], acc is [0.765625]\n",
      "epoch: 6, batch: 150, loss is: [0.77905405], acc is [0.6875]\n",
      "epoch: 6, batch: 300, loss is: [0.68758166], acc is [0.765625]\n",
      "epoch: 6, batch: 450, loss is: [0.61767024], acc is [0.8125]\n",
      "epoch: 6, batch: 600, loss is: [0.5981493], acc is [0.828125]\n",
      "epoch: 6, batch: 750, loss is: [0.8084145], acc is [0.625]\n",
      "epoch: 7, batch: 0, loss is: [0.81322026], acc is [0.671875]\n",
      "epoch: 7, batch: 150, loss is: [0.699082], acc is [0.734375]\n",
      "epoch: 7, batch: 300, loss is: [0.63933176], acc is [0.75]\n",
      "epoch: 7, batch: 450, loss is: [0.44707596], acc is [0.875]\n",
      "epoch: 7, batch: 600, loss is: [0.6910343], acc is [0.75]\n",
      "epoch: 7, batch: 750, loss is: [0.791425], acc is [0.671875]\n",
      "epoch: 8, batch: 0, loss is: [0.6208346], acc is [0.78125]\n",
      "epoch: 8, batch: 150, loss is: [0.8771972], acc is [0.703125]\n",
      "epoch: 8, batch: 300, loss is: [0.66968787], acc is [0.8125]\n",
      "epoch: 8, batch: 450, loss is: [0.55419946], acc is [0.78125]\n",
      "epoch: 8, batch: 600, loss is: [0.89915216], acc is [0.6875]\n",
      "epoch: 8, batch: 750, loss is: [0.8564254], acc is [0.75]\n",
      "epoch: 9, batch: 0, loss is: [0.90070355], acc is [0.65625]\n",
      "epoch: 9, batch: 150, loss is: [0.8335182], acc is [0.703125]\n",
      "epoch: 9, batch: 300, loss is: [0.6121764], acc is [0.796875]\n",
      "epoch: 9, batch: 450, loss is: [0.6967522], acc is [0.75]\n",
      "epoch: 9, batch: 600, loss is: [0.68076444], acc is [0.75]\n",
      "epoch: 9, batch: 750, loss is: [0.9312615], acc is [0.609375]\n"
     ]
    }
   ],
   "source": [
    "model = Cifar_TWO_CONV2_norm()\n",
    "train_Cifar_TWO2_norm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 使用dropout\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, BatchNorm2D\n",
    "\n",
    "class Cifar_TWO_CONV2_norm_end(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_norm_end, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义批归一化层\n",
    "        self.bn1 = BatchNorm2D(12)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=2)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = BatchNorm2D(24)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第三个卷积层\n",
    "        self.conv3 = Conv2D(in_channels=24, out_channels= 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = BatchNorm2D(32)\n",
    "        self.max_pool3 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是32*8*8,定义一个全连接\n",
    "        self.fc = Linear(in_features=512, out_features=218)\n",
    "        self.fc2 = Linear(in_features=218,out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.dropout(x, p = 0.01,mode=mode)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.nn import BatchNorm2D\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_end\")\n",
    "def train_Cifar_TWO2_end(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 20\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        total_test_accuracy=0\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            total_test_accuracy += acc.numpy()\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "        \n",
    "        total_test_accuracy /= len(test_loader())\n",
    "        logwriter.add_scalar(tag='test/total_acc', step=epoch_id, value=total_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [0.5429214], acc is [0.84375]\n",
      "epoch: 0, batch: 150, loss is: [0.45616698], acc is [0.828125]\n",
      "epoch: 0, batch: 300, loss is: [0.7704087], acc is [0.71875]\n",
      "epoch: 0, batch: 450, loss is: [0.9242352], acc is [0.703125]\n",
      "epoch: 0, batch: 600, loss is: [0.55163693], acc is [0.765625]\n",
      "epoch: 0, batch: 750, loss is: [0.69744873], acc is [0.703125]\n",
      "epoch: 1, batch: 0, loss is: [0.5685594], acc is [0.796875]\n",
      "epoch: 1, batch: 150, loss is: [0.64746255], acc is [0.78125]\n",
      "epoch: 1, batch: 300, loss is: [0.9464253], acc is [0.6875]\n",
      "epoch: 1, batch: 450, loss is: [0.5504892], acc is [0.78125]\n",
      "epoch: 1, batch: 600, loss is: [0.7924937], acc is [0.734375]\n",
      "epoch: 1, batch: 750, loss is: [0.75680757], acc is [0.78125]\n",
      "epoch: 2, batch: 0, loss is: [0.6888693], acc is [0.671875]\n",
      "epoch: 2, batch: 150, loss is: [0.7400059], acc is [0.6875]\n",
      "epoch: 2, batch: 300, loss is: [0.803023], acc is [0.6875]\n",
      "epoch: 2, batch: 450, loss is: [0.74006987], acc is [0.75]\n",
      "epoch: 2, batch: 600, loss is: [0.5326379], acc is [0.84375]\n",
      "epoch: 2, batch: 750, loss is: [0.63674253], acc is [0.734375]\n",
      "epoch: 3, batch: 0, loss is: [0.6393819], acc is [0.78125]\n",
      "epoch: 3, batch: 150, loss is: [0.6319531], acc is [0.71875]\n",
      "epoch: 3, batch: 300, loss is: [0.84251], acc is [0.71875]\n",
      "epoch: 3, batch: 450, loss is: [0.78508383], acc is [0.78125]\n",
      "epoch: 3, batch: 600, loss is: [0.8116807], acc is [0.734375]\n",
      "epoch: 3, batch: 750, loss is: [0.52192664], acc is [0.796875]\n",
      "epoch: 4, batch: 0, loss is: [0.5832294], acc is [0.75]\n",
      "epoch: 4, batch: 150, loss is: [0.64732856], acc is [0.734375]\n",
      "epoch: 4, batch: 300, loss is: [0.8304092], acc is [0.71875]\n",
      "epoch: 4, batch: 450, loss is: [0.4450635], acc is [0.8125]\n",
      "epoch: 4, batch: 600, loss is: [0.8645177], acc is [0.75]\n",
      "epoch: 4, batch: 750, loss is: [0.5719505], acc is [0.796875]\n",
      "epoch: 5, batch: 0, loss is: [0.7987533], acc is [0.65625]\n",
      "epoch: 5, batch: 150, loss is: [0.84120405], acc is [0.6875]\n",
      "epoch: 5, batch: 300, loss is: [0.54397154], acc is [0.734375]\n",
      "epoch: 5, batch: 450, loss is: [0.6858704], acc is [0.8125]\n",
      "epoch: 5, batch: 600, loss is: [0.57354605], acc is [0.8125]\n",
      "epoch: 5, batch: 750, loss is: [0.5635679], acc is [0.78125]\n",
      "epoch: 6, batch: 0, loss is: [0.4371037], acc is [0.84375]\n",
      "epoch: 6, batch: 150, loss is: [0.55419713], acc is [0.8125]\n",
      "epoch: 6, batch: 300, loss is: [0.60511106], acc is [0.75]\n",
      "epoch: 6, batch: 450, loss is: [0.9512773], acc is [0.609375]\n",
      "epoch: 6, batch: 600, loss is: [0.5976158], acc is [0.765625]\n",
      "epoch: 6, batch: 750, loss is: [0.80296654], acc is [0.734375]\n",
      "epoch: 7, batch: 0, loss is: [0.9614102], acc is [0.6875]\n",
      "epoch: 7, batch: 150, loss is: [0.5315316], acc is [0.828125]\n",
      "epoch: 7, batch: 300, loss is: [0.7840159], acc is [0.734375]\n",
      "epoch: 7, batch: 450, loss is: [0.54422563], acc is [0.8125]\n",
      "epoch: 7, batch: 600, loss is: [0.61790997], acc is [0.796875]\n",
      "epoch: 7, batch: 750, loss is: [0.57588017], acc is [0.8125]\n",
      "epoch: 8, batch: 0, loss is: [0.46550724], acc is [0.84375]\n",
      "epoch: 8, batch: 150, loss is: [0.64474964], acc is [0.78125]\n",
      "epoch: 8, batch: 300, loss is: [0.6677854], acc is [0.78125]\n",
      "epoch: 8, batch: 450, loss is: [0.5763403], acc is [0.765625]\n",
      "epoch: 8, batch: 600, loss is: [0.41659433], acc is [0.890625]\n",
      "epoch: 8, batch: 750, loss is: [0.7177656], acc is [0.78125]\n",
      "epoch: 9, batch: 0, loss is: [0.5504833], acc is [0.796875]\n",
      "epoch: 9, batch: 150, loss is: [0.7347334], acc is [0.71875]\n",
      "epoch: 9, batch: 300, loss is: [0.42019114], acc is [0.828125]\n",
      "epoch: 9, batch: 450, loss is: [0.6585457], acc is [0.796875]\n",
      "epoch: 9, batch: 600, loss is: [0.74176806], acc is [0.6875]\n",
      "epoch: 9, batch: 750, loss is: [0.7307803], acc is [0.703125]\n",
      "epoch: 10, batch: 0, loss is: [0.7983074], acc is [0.71875]\n",
      "epoch: 10, batch: 150, loss is: [0.60410875], acc is [0.765625]\n",
      "epoch: 10, batch: 300, loss is: [0.47088727], acc is [0.765625]\n",
      "epoch: 10, batch: 450, loss is: [0.7108173], acc is [0.75]\n",
      "epoch: 10, batch: 600, loss is: [0.6727022], acc is [0.703125]\n",
      "epoch: 10, batch: 750, loss is: [0.40978098], acc is [0.828125]\n",
      "epoch: 11, batch: 0, loss is: [0.7185315], acc is [0.78125]\n",
      "epoch: 11, batch: 150, loss is: [0.59401333], acc is [0.796875]\n",
      "epoch: 11, batch: 300, loss is: [0.80747724], acc is [0.6875]\n",
      "epoch: 11, batch: 450, loss is: [0.49805662], acc is [0.859375]\n",
      "epoch: 11, batch: 600, loss is: [0.5800369], acc is [0.78125]\n",
      "epoch: 11, batch: 750, loss is: [0.6393892], acc is [0.78125]\n",
      "epoch: 12, batch: 0, loss is: [0.5887245], acc is [0.78125]\n",
      "epoch: 12, batch: 150, loss is: [0.6739542], acc is [0.734375]\n",
      "epoch: 12, batch: 300, loss is: [0.6759993], acc is [0.78125]\n",
      "epoch: 12, batch: 450, loss is: [0.7972063], acc is [0.71875]\n",
      "epoch: 12, batch: 600, loss is: [0.6021354], acc is [0.8125]\n",
      "epoch: 12, batch: 750, loss is: [0.64771557], acc is [0.8125]\n",
      "epoch: 13, batch: 0, loss is: [0.39476526], acc is [0.84375]\n",
      "epoch: 13, batch: 150, loss is: [0.43850493], acc is [0.859375]\n",
      "epoch: 13, batch: 300, loss is: [0.5038288], acc is [0.828125]\n",
      "epoch: 13, batch: 450, loss is: [0.589115], acc is [0.765625]\n",
      "epoch: 13, batch: 600, loss is: [0.5222877], acc is [0.828125]\n",
      "epoch: 13, batch: 750, loss is: [0.59217584], acc is [0.75]\n",
      "epoch: 14, batch: 0, loss is: [0.36388433], acc is [0.875]\n",
      "epoch: 14, batch: 150, loss is: [0.5681962], acc is [0.765625]\n",
      "epoch: 14, batch: 300, loss is: [0.46232355], acc is [0.84375]\n",
      "epoch: 14, batch: 450, loss is: [0.45915428], acc is [0.796875]\n",
      "epoch: 14, batch: 600, loss is: [0.6533746], acc is [0.765625]\n",
      "epoch: 14, batch: 750, loss is: [0.6383393], acc is [0.78125]\n",
      "epoch: 15, batch: 0, loss is: [0.62939394], acc is [0.84375]\n",
      "epoch: 15, batch: 150, loss is: [0.30308017], acc is [0.890625]\n",
      "epoch: 15, batch: 300, loss is: [0.48189116], acc is [0.84375]\n",
      "epoch: 15, batch: 450, loss is: [0.59882975], acc is [0.8125]\n",
      "epoch: 15, batch: 600, loss is: [0.556226], acc is [0.828125]\n",
      "epoch: 15, batch: 750, loss is: [0.45711273], acc is [0.875]\n",
      "epoch: 16, batch: 0, loss is: [0.25041172], acc is [0.921875]\n",
      "epoch: 16, batch: 150, loss is: [0.57883227], acc is [0.734375]\n",
      "epoch: 16, batch: 300, loss is: [0.9210988], acc is [0.703125]\n",
      "epoch: 16, batch: 450, loss is: [0.6000582], acc is [0.8125]\n",
      "epoch: 16, batch: 600, loss is: [0.301566], acc is [0.875]\n",
      "epoch: 16, batch: 750, loss is: [0.6285813], acc is [0.828125]\n",
      "epoch: 17, batch: 0, loss is: [0.5047483], acc is [0.8125]\n",
      "epoch: 17, batch: 150, loss is: [0.45354235], acc is [0.84375]\n",
      "epoch: 17, batch: 300, loss is: [0.5264151], acc is [0.828125]\n",
      "epoch: 17, batch: 450, loss is: [0.7025925], acc is [0.796875]\n",
      "epoch: 17, batch: 600, loss is: [0.44134113], acc is [0.828125]\n",
      "epoch: 17, batch: 750, loss is: [0.82216394], acc is [0.78125]\n",
      "epoch: 18, batch: 0, loss is: [0.35666215], acc is [0.875]\n",
      "epoch: 18, batch: 150, loss is: [0.5260601], acc is [0.828125]\n",
      "epoch: 18, batch: 300, loss is: [0.6215975], acc is [0.75]\n",
      "epoch: 18, batch: 450, loss is: [0.4153179], acc is [0.890625]\n",
      "epoch: 18, batch: 600, loss is: [0.61799854], acc is [0.75]\n",
      "epoch: 18, batch: 750, loss is: [0.5927186], acc is [0.796875]\n",
      "epoch: 19, batch: 0, loss is: [0.5581173], acc is [0.78125]\n",
      "epoch: 19, batch: 150, loss is: [0.43756706], acc is [0.859375]\n",
      "epoch: 19, batch: 300, loss is: [0.4516405], acc is [0.8125]\n",
      "epoch: 19, batch: 450, loss is: [0.6958507], acc is [0.796875]\n",
      "epoch: 19, batch: 600, loss is: [0.7640016], acc is [0.78125]\n",
      "epoch: 19, batch: 750, loss is: [0.5071075], acc is [0.796875]\n"
     ]
    }
   ],
   "source": [
    "mdoel = Cifar_TWO_CONV2_norm_end()\n",
    "train_Cifar_TWO2_end(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 把以上的都综合在一起\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, BatchNorm2D\n",
    "\n",
    "class Cifar_TWO_CONV2_lenet_3(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_lenet_3, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义批归一化层\n",
    "        self.bn1 = BatchNorm2D(32)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = BatchNorm2D(64)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 再增加一层卷积\n",
    "        self.conv3 = Conv2D(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = BatchNorm2D(64)\n",
    "\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc1 = Linear(in_features=4096, out_features=10)\n",
    "        self.fc3 = Linear(in_features=1024, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = paddle.to_tensor(x)\n",
    "        x = self.fc1(x)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.nn import BatchNorm2D\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_new_lenet7\")\n",
    "def train_Cifar_TWO2_lenet(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 15\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.005, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%200 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=200\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 200 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=200\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.5494406], acc is [0.1328125]\n",
      "epoch: 0, batch: 200, loss is: [1.546255], acc is [0.4609375]\n",
      "epoch: 1, batch: 0, loss is: [1.4955535], acc is [0.5]\n",
      "epoch: 1, batch: 200, loss is: [1.3238297], acc is [0.6015625]\n",
      "epoch: 2, batch: 0, loss is: [1.2607361], acc is [0.59375]\n",
      "epoch: 2, batch: 200, loss is: [1.2540855], acc is [0.6328125]\n",
      "epoch: 3, batch: 0, loss is: [1.102382], acc is [0.640625]\n",
      "epoch: 3, batch: 200, loss is: [1.1954417], acc is [0.6640625]\n",
      "epoch: 4, batch: 0, loss is: [1.124525], acc is [0.640625]\n",
      "epoch: 4, batch: 200, loss is: [1.3655242], acc is [0.578125]\n",
      "epoch: 5, batch: 0, loss is: [1.1353847], acc is [0.625]\n",
      "epoch: 5, batch: 200, loss is: [1.190679], acc is [0.625]\n",
      "epoch: 6, batch: 0, loss is: [1.0680808], acc is [0.609375]\n",
      "epoch: 6, batch: 200, loss is: [1.2858982], acc is [0.6328125]\n",
      "epoch: 7, batch: 0, loss is: [1.3763033], acc is [0.578125]\n",
      "epoch: 7, batch: 200, loss is: [1.1359574], acc is [0.640625]\n",
      "epoch: 8, batch: 0, loss is: [1.1208253], acc is [0.6796875]\n",
      "epoch: 8, batch: 200, loss is: [1.131259], acc is [0.6328125]\n",
      "epoch: 9, batch: 0, loss is: [1.0670694], acc is [0.6328125]\n",
      "epoch: 9, batch: 200, loss is: [1.5183477], acc is [0.6015625]\n",
      "epoch: 10, batch: 0, loss is: [1.2377145], acc is [0.625]\n",
      "epoch: 10, batch: 200, loss is: [1.1134982], acc is [0.625]\n",
      "epoch: 11, batch: 0, loss is: [1.2755997], acc is [0.640625]\n",
      "epoch: 11, batch: 200, loss is: [1.081486], acc is [0.59375]\n",
      "epoch: 12, batch: 0, loss is: [1.1003363], acc is [0.65625]\n",
      "epoch: 12, batch: 200, loss is: [1.0382322], acc is [0.6953125]\n",
      "epoch: 13, batch: 0, loss is: [0.87438166], acc is [0.7421875]\n",
      "epoch: 13, batch: 200, loss is: [1.3153304], acc is [0.65625]\n",
      "epoch: 14, batch: 0, loss is: [1.0955257], acc is [0.6640625]\n",
      "epoch: 14, batch: 200, loss is: [1.2748395], acc is [0.609375]\n"
     ]
    }
   ],
   "source": [
    "model = Cifar_TWO_CONV2_lenet_3()\n",
    "train_Cifar_TWO2_lenet(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
