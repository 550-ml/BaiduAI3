{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 附加题中，我们选择使用cifar10\n",
    "import paddle\n",
    "from paddle.vision.transforms import Normalize, Compose, Transpose\n",
    "from paddle.vision.datasets import Cifar10\n",
    "def get_cifar10_dataloader():\n",
    "    # cifar10是一个三通道的图片\n",
    "    transforms = Compose([Normalize(mean=[127.5, 127.5, 127.5], std= [127.5, 127.5, 127.5], data_format='HWC'), Transpose() ])\n",
    "    # 数据集\n",
    "    train_datasets = Cifar10(mode='train', transform=transforms)\n",
    "    test_datasets = Cifar10(mode='test', transform=transforms)\n",
    "    # 数据读取器\n",
    "    train_loader = paddle.io.DataLoader(train_datasets, batch_size=64, shuffle = True, num_workers=1, drop_last=True)\n",
    "    test_loader = paddle.io.DataLoader(test_datasets, batch_size=64, shuffle = False, num_workers=1, drop_last=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\io\\reader.py:433: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_cifar10_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualdl\n",
    "import visualdl.server.app\n",
    "visualdl.server.app.run('./run/cifar_log',\n",
    "                        host = \"127.0.0.1\",\n",
    "                        port=8081,\n",
    "                        cache_timeout=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 首先只来一层卷积\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_ONE_CONV(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_ONE_CONV, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=5120, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/one_conv\")\n",
    "def train_Cifar_ONE(model):\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 10\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.7267888], acc is [0.078125]\n",
      "epoch: 0, batch: 150, loss is: [1.4831171], acc is [0.5]\n",
      "epoch: 0, batch: 300, loss is: [1.4324566], acc is [0.578125]\n",
      "epoch: 0, batch: 450, loss is: [1.242635], acc is [0.5625]\n",
      "epoch: 0, batch: 600, loss is: [1.7445328], acc is [0.40625]\n",
      "epoch: 0, batch: 750, loss is: [1.2230357], acc is [0.578125]\n",
      "epoch: 1, batch: 0, loss is: [1.6908634], acc is [0.375]\n",
      "epoch: 1, batch: 150, loss is: [1.2385381], acc is [0.515625]\n",
      "epoch: 1, batch: 300, loss is: [1.397466], acc is [0.53125]\n",
      "epoch: 1, batch: 450, loss is: [1.1587546], acc is [0.59375]\n",
      "epoch: 1, batch: 600, loss is: [1.4375345], acc is [0.5625]\n",
      "epoch: 1, batch: 750, loss is: [1.5021424], acc is [0.484375]\n",
      "epoch: 2, batch: 0, loss is: [1.4640067], acc is [0.46875]\n",
      "epoch: 2, batch: 150, loss is: [1.1618476], acc is [0.546875]\n",
      "epoch: 2, batch: 300, loss is: [1.7091419], acc is [0.5]\n",
      "epoch: 2, batch: 450, loss is: [1.4363967], acc is [0.5625]\n",
      "epoch: 2, batch: 600, loss is: [1.3610146], acc is [0.453125]\n",
      "epoch: 2, batch: 750, loss is: [1.3631508], acc is [0.46875]\n",
      "epoch: 3, batch: 0, loss is: [1.0590277], acc is [0.578125]\n",
      "epoch: 3, batch: 150, loss is: [1.8994558], acc is [0.4375]\n",
      "epoch: 3, batch: 300, loss is: [1.21914], acc is [0.59375]\n",
      "epoch: 3, batch: 450, loss is: [1.3880887], acc is [0.625]\n",
      "epoch: 3, batch: 600, loss is: [1.9030048], acc is [0.46875]\n",
      "epoch: 3, batch: 750, loss is: [1.1170063], acc is [0.59375]\n",
      "epoch: 4, batch: 0, loss is: [1.071088], acc is [0.59375]\n",
      "epoch: 4, batch: 150, loss is: [1.3571866], acc is [0.5]\n",
      "epoch: 4, batch: 300, loss is: [1.2212732], acc is [0.640625]\n",
      "epoch: 4, batch: 450, loss is: [1.3536048], acc is [0.546875]\n",
      "epoch: 4, batch: 600, loss is: [1.3422122], acc is [0.484375]\n",
      "epoch: 4, batch: 750, loss is: [1.4403875], acc is [0.46875]\n",
      "epoch: 5, batch: 0, loss is: [1.0716748], acc is [0.625]\n",
      "epoch: 5, batch: 150, loss is: [1.4907054], acc is [0.625]\n",
      "epoch: 5, batch: 300, loss is: [1.2761679], acc is [0.5625]\n",
      "epoch: 5, batch: 450, loss is: [1.2981064], acc is [0.546875]\n",
      "epoch: 5, batch: 600, loss is: [1.0420829], acc is [0.625]\n",
      "epoch: 5, batch: 750, loss is: [1.4246666], acc is [0.5625]\n",
      "epoch: 6, batch: 0, loss is: [1.044416], acc is [0.65625]\n",
      "epoch: 6, batch: 150, loss is: [1.0883276], acc is [0.578125]\n",
      "epoch: 6, batch: 300, loss is: [1.1813593], acc is [0.640625]\n",
      "epoch: 6, batch: 450, loss is: [1.3881786], acc is [0.453125]\n",
      "epoch: 6, batch: 600, loss is: [1.409737], acc is [0.515625]\n",
      "epoch: 6, batch: 750, loss is: [1.4053221], acc is [0.546875]\n",
      "epoch: 7, batch: 0, loss is: [1.2693915], acc is [0.578125]\n",
      "epoch: 7, batch: 150, loss is: [1.2026765], acc is [0.59375]\n",
      "epoch: 7, batch: 300, loss is: [1.3125395], acc is [0.578125]\n",
      "epoch: 7, batch: 450, loss is: [1.0503062], acc is [0.625]\n",
      "epoch: 7, batch: 600, loss is: [1.5925627], acc is [0.53125]\n",
      "epoch: 7, batch: 750, loss is: [1.0771902], acc is [0.625]\n",
      "epoch: 8, batch: 0, loss is: [1.7784601], acc is [0.484375]\n",
      "epoch: 8, batch: 150, loss is: [1.3004372], acc is [0.53125]\n",
      "epoch: 8, batch: 300, loss is: [1.4361904], acc is [0.515625]\n",
      "epoch: 8, batch: 450, loss is: [1.1382616], acc is [0.515625]\n",
      "epoch: 8, batch: 600, loss is: [1.4737141], acc is [0.5]\n",
      "epoch: 8, batch: 750, loss is: [1.4446039], acc is [0.484375]\n",
      "epoch: 9, batch: 0, loss is: [1.0394944], acc is [0.578125]\n",
      "epoch: 9, batch: 150, loss is: [1.5794687], acc is [0.53125]\n",
      "epoch: 9, batch: 300, loss is: [1.1243314], acc is [0.640625]\n",
      "epoch: 9, batch: 450, loss is: [1.2474351], acc is [0.578125]\n",
      "epoch: 9, batch: 600, loss is: [1.2937679], acc is [0.59375]\n",
      "epoch: 9, batch: 750, loss is: [1.3798568], acc is [0.484375]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_one = Cifar_ONE_CONV()\n",
    "train_Cifar_ONE(model_cifar_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_TWO_CONV(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=1536, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_2\")\n",
    "def train_Cifar_TWO(model):\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 10  \n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.0159125], acc is [0.09375]\n",
      "epoch: 0, batch: 150, loss is: [1.7808365], acc is [0.40625]\n",
      "epoch: 0, batch: 300, loss is: [1.6551902], acc is [0.46875]\n",
      "epoch: 0, batch: 450, loss is: [1.2367995], acc is [0.578125]\n",
      "epoch: 0, batch: 600, loss is: [1.5560794], acc is [0.4375]\n",
      "epoch: 0, batch: 750, loss is: [1.3427678], acc is [0.46875]\n",
      "epoch: 1, batch: 0, loss is: [1.4758031], acc is [0.53125]\n",
      "epoch: 1, batch: 150, loss is: [1.3802402], acc is [0.546875]\n",
      "epoch: 1, batch: 300, loss is: [1.1563023], acc is [0.5625]\n",
      "epoch: 1, batch: 450, loss is: [1.570127], acc is [0.421875]\n",
      "epoch: 1, batch: 600, loss is: [1.2995212], acc is [0.5625]\n",
      "epoch: 1, batch: 750, loss is: [1.0789251], acc is [0.546875]\n",
      "epoch: 2, batch: 0, loss is: [1.2229929], acc is [0.53125]\n",
      "epoch: 2, batch: 150, loss is: [1.3716687], acc is [0.484375]\n",
      "epoch: 2, batch: 300, loss is: [1.4375595], acc is [0.46875]\n",
      "epoch: 2, batch: 450, loss is: [1.2701385], acc is [0.578125]\n",
      "epoch: 2, batch: 600, loss is: [1.4556639], acc is [0.546875]\n",
      "epoch: 2, batch: 750, loss is: [1.3753521], acc is [0.5]\n",
      "epoch: 3, batch: 0, loss is: [1.5462358], acc is [0.484375]\n",
      "epoch: 3, batch: 150, loss is: [1.3424199], acc is [0.421875]\n",
      "epoch: 3, batch: 300, loss is: [1.206362], acc is [0.640625]\n",
      "epoch: 3, batch: 450, loss is: [1.3698838], acc is [0.625]\n",
      "epoch: 3, batch: 600, loss is: [1.3966583], acc is [0.484375]\n",
      "epoch: 3, batch: 750, loss is: [1.1886804], acc is [0.578125]\n",
      "epoch: 4, batch: 0, loss is: [1.227988], acc is [0.59375]\n",
      "epoch: 4, batch: 150, loss is: [1.0888252], acc is [0.609375]\n",
      "epoch: 4, batch: 300, loss is: [0.9866679], acc is [0.65625]\n",
      "epoch: 4, batch: 450, loss is: [1.2184172], acc is [0.546875]\n",
      "epoch: 4, batch: 600, loss is: [1.2020252], acc is [0.6875]\n",
      "epoch: 4, batch: 750, loss is: [1.724721], acc is [0.390625]\n",
      "epoch: 5, batch: 0, loss is: [1.2928183], acc is [0.484375]\n",
      "epoch: 5, batch: 150, loss is: [1.2530541], acc is [0.5625]\n",
      "epoch: 5, batch: 300, loss is: [1.3859334], acc is [0.546875]\n",
      "epoch: 5, batch: 450, loss is: [1.2304976], acc is [0.5]\n",
      "epoch: 5, batch: 600, loss is: [1.1031549], acc is [0.578125]\n",
      "epoch: 5, batch: 750, loss is: [1.2669685], acc is [0.546875]\n",
      "epoch: 6, batch: 0, loss is: [0.9908668], acc is [0.640625]\n",
      "epoch: 6, batch: 150, loss is: [1.2352778], acc is [0.515625]\n",
      "epoch: 6, batch: 300, loss is: [1.2263054], acc is [0.546875]\n",
      "epoch: 6, batch: 450, loss is: [1.0996599], acc is [0.640625]\n",
      "epoch: 6, batch: 600, loss is: [1.3501421], acc is [0.625]\n",
      "epoch: 6, batch: 750, loss is: [1.3088723], acc is [0.59375]\n",
      "epoch: 7, batch: 0, loss is: [1.1618865], acc is [0.59375]\n",
      "epoch: 7, batch: 150, loss is: [1.0050976], acc is [0.640625]\n",
      "epoch: 7, batch: 300, loss is: [1.0481428], acc is [0.65625]\n",
      "epoch: 7, batch: 450, loss is: [1.1763464], acc is [0.546875]\n",
      "epoch: 7, batch: 600, loss is: [1.221248], acc is [0.59375]\n",
      "epoch: 7, batch: 750, loss is: [0.9697289], acc is [0.625]\n",
      "epoch: 8, batch: 0, loss is: [1.189497], acc is [0.59375]\n",
      "epoch: 8, batch: 150, loss is: [1.0832844], acc is [0.609375]\n",
      "epoch: 8, batch: 300, loss is: [1.2816997], acc is [0.484375]\n",
      "epoch: 8, batch: 450, loss is: [1.1755645], acc is [0.625]\n",
      "epoch: 8, batch: 600, loss is: [1.3058686], acc is [0.59375]\n",
      "epoch: 8, batch: 750, loss is: [1.2658193], acc is [0.546875]\n",
      "epoch: 9, batch: 0, loss is: [0.9838942], acc is [0.640625]\n",
      "epoch: 9, batch: 150, loss is: [1.4187185], acc is [0.515625]\n",
      "epoch: 9, batch: 300, loss is: [1.3206072], acc is [0.578125]\n",
      "epoch: 9, batch: 450, loss is: [1.4735252], acc is [0.484375]\n",
      "epoch: 9, batch: 600, loss is: [1.1328534], acc is [0.65625]\n",
      "epoch: 9, batch: 750, loss is: [0.94947946], acc is [0.640625]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_two = Cifar_TWO_CONV()\n",
    "train_Cifar_TWO(model_cifar_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 首先只来一层卷积\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_THree_CONV2(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_THree_CONV2, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 再定义一层卷积层\n",
    "        self.conv3 = Conv2D(in_channels=20, out_channels=32,kernel_size=3, stride=1, padding=1)\n",
    "        self.max_pool3 = MaxPool2D(kernel_size=2,stride=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=2048, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/three_conv2\")\n",
    "def train_Cifar_Three(model):\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 15\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.5732203], acc is [0.09375]\n",
      "epoch: 0, batch: 150, loss is: [1.7769077], acc is [0.3125]\n",
      "epoch: 0, batch: 300, loss is: [1.5719324], acc is [0.484375]\n",
      "epoch: 0, batch: 450, loss is: [1.6376365], acc is [0.375]\n",
      "epoch: 0, batch: 600, loss is: [1.1803364], acc is [0.625]\n",
      "epoch: 0, batch: 750, loss is: [1.8372405], acc is [0.28125]\n",
      "epoch: 1, batch: 0, loss is: [1.2950017], acc is [0.5625]\n",
      "epoch: 1, batch: 150, loss is: [1.9494592], acc is [0.375]\n",
      "epoch: 1, batch: 300, loss is: [1.370954], acc is [0.46875]\n",
      "epoch: 1, batch: 450, loss is: [1.6023482], acc is [0.390625]\n",
      "epoch: 1, batch: 600, loss is: [1.2391276], acc is [0.515625]\n",
      "epoch: 1, batch: 750, loss is: [1.3884648], acc is [0.46875]\n",
      "epoch: 2, batch: 0, loss is: [1.0867006], acc is [0.65625]\n",
      "epoch: 2, batch: 150, loss is: [1.3110088], acc is [0.484375]\n",
      "epoch: 2, batch: 300, loss is: [1.4036814], acc is [0.453125]\n",
      "epoch: 2, batch: 450, loss is: [1.3103821], acc is [0.5625]\n",
      "epoch: 2, batch: 600, loss is: [1.3687835], acc is [0.515625]\n",
      "epoch: 2, batch: 750, loss is: [1.2564485], acc is [0.578125]\n",
      "epoch: 3, batch: 0, loss is: [1.3084644], acc is [0.46875]\n",
      "epoch: 3, batch: 150, loss is: [1.2374976], acc is [0.53125]\n",
      "epoch: 3, batch: 300, loss is: [1.0269719], acc is [0.65625]\n",
      "epoch: 3, batch: 450, loss is: [1.2779326], acc is [0.515625]\n",
      "epoch: 3, batch: 600, loss is: [1.4948363], acc is [0.390625]\n",
      "epoch: 3, batch: 750, loss is: [1.0554504], acc is [0.59375]\n",
      "epoch: 4, batch: 0, loss is: [1.2915913], acc is [0.484375]\n",
      "epoch: 4, batch: 150, loss is: [1.4663212], acc is [0.40625]\n",
      "epoch: 4, batch: 300, loss is: [1.2921598], acc is [0.5625]\n",
      "epoch: 4, batch: 450, loss is: [1.2874507], acc is [0.5625]\n",
      "epoch: 4, batch: 600, loss is: [1.4550188], acc is [0.5]\n",
      "epoch: 4, batch: 750, loss is: [1.5103197], acc is [0.484375]\n",
      "epoch: 5, batch: 0, loss is: [1.4895467], acc is [0.5]\n",
      "epoch: 5, batch: 150, loss is: [1.4169714], acc is [0.53125]\n",
      "epoch: 5, batch: 300, loss is: [1.4252985], acc is [0.5]\n",
      "epoch: 5, batch: 450, loss is: [1.3537394], acc is [0.4375]\n",
      "epoch: 5, batch: 600, loss is: [1.4256706], acc is [0.421875]\n",
      "epoch: 5, batch: 750, loss is: [1.3236563], acc is [0.546875]\n",
      "epoch: 6, batch: 0, loss is: [1.2400315], acc is [0.578125]\n",
      "epoch: 6, batch: 150, loss is: [1.1908218], acc is [0.625]\n",
      "epoch: 6, batch: 300, loss is: [1.2653244], acc is [0.484375]\n",
      "epoch: 6, batch: 450, loss is: [1.6718965], acc is [0.421875]\n",
      "epoch: 6, batch: 600, loss is: [1.4470469], acc is [0.578125]\n",
      "epoch: 6, batch: 750, loss is: [1.5374203], acc is [0.5625]\n",
      "epoch: 7, batch: 0, loss is: [1.0469208], acc is [0.640625]\n",
      "epoch: 7, batch: 150, loss is: [1.5314312], acc is [0.53125]\n",
      "epoch: 7, batch: 300, loss is: [1.5894868], acc is [0.40625]\n",
      "epoch: 7, batch: 450, loss is: [1.0123651], acc is [0.578125]\n",
      "epoch: 7, batch: 600, loss is: [1.1400825], acc is [0.578125]\n",
      "epoch: 7, batch: 750, loss is: [1.3500441], acc is [0.53125]\n",
      "epoch: 8, batch: 0, loss is: [1.1154442], acc is [0.625]\n",
      "epoch: 8, batch: 150, loss is: [1.119705], acc is [0.53125]\n",
      "epoch: 8, batch: 300, loss is: [1.2182605], acc is [0.625]\n",
      "epoch: 8, batch: 450, loss is: [1.0973092], acc is [0.65625]\n",
      "epoch: 8, batch: 600, loss is: [1.1197655], acc is [0.5625]\n",
      "epoch: 8, batch: 750, loss is: [1.3650596], acc is [0.4375]\n",
      "epoch: 9, batch: 0, loss is: [1.3802841], acc is [0.484375]\n",
      "epoch: 9, batch: 150, loss is: [1.4882722], acc is [0.484375]\n",
      "epoch: 9, batch: 300, loss is: [1.4364927], acc is [0.484375]\n",
      "epoch: 9, batch: 450, loss is: [1.252791], acc is [0.5625]\n",
      "epoch: 9, batch: 600, loss is: [1.3417995], acc is [0.546875]\n",
      "epoch: 9, batch: 750, loss is: [1.1250821], acc is [0.625]\n",
      "epoch: 10, batch: 0, loss is: [1.3190696], acc is [0.5]\n",
      "epoch: 10, batch: 150, loss is: [1.1611507], acc is [0.65625]\n",
      "epoch: 10, batch: 300, loss is: [1.2813839], acc is [0.546875]\n",
      "epoch: 10, batch: 450, loss is: [1.0421779], acc is [0.578125]\n",
      "epoch: 10, batch: 600, loss is: [1.2004187], acc is [0.578125]\n",
      "epoch: 10, batch: 750, loss is: [1.1068611], acc is [0.5625]\n",
      "epoch: 11, batch: 0, loss is: [1.129653], acc is [0.5625]\n",
      "epoch: 11, batch: 150, loss is: [1.3639834], acc is [0.515625]\n",
      "epoch: 11, batch: 300, loss is: [1.1902723], acc is [0.578125]\n",
      "epoch: 11, batch: 450, loss is: [1.0648303], acc is [0.609375]\n",
      "epoch: 11, batch: 600, loss is: [1.2609497], acc is [0.578125]\n",
      "epoch: 11, batch: 750, loss is: [1.2477832], acc is [0.578125]\n",
      "epoch: 12, batch: 0, loss is: [1.3162937], acc is [0.53125]\n",
      "epoch: 12, batch: 150, loss is: [1.3695922], acc is [0.484375]\n",
      "epoch: 12, batch: 300, loss is: [1.1688597], acc is [0.5625]\n",
      "epoch: 12, batch: 450, loss is: [1.2865896], acc is [0.59375]\n",
      "epoch: 12, batch: 600, loss is: [1.2282152], acc is [0.5]\n",
      "epoch: 12, batch: 750, loss is: [1.3511999], acc is [0.5625]\n",
      "epoch: 13, batch: 0, loss is: [1.2941154], acc is [0.609375]\n",
      "epoch: 13, batch: 150, loss is: [1.3330595], acc is [0.5]\n",
      "epoch: 13, batch: 300, loss is: [1.3213494], acc is [0.546875]\n",
      "epoch: 13, batch: 450, loss is: [1.3060417], acc is [0.609375]\n",
      "epoch: 13, batch: 600, loss is: [1.5618814], acc is [0.4375]\n",
      "epoch: 13, batch: 750, loss is: [1.1241992], acc is [0.5625]\n",
      "epoch: 14, batch: 0, loss is: [0.96968126], acc is [0.640625]\n",
      "epoch: 14, batch: 150, loss is: [1.1370115], acc is [0.546875]\n",
      "epoch: 14, batch: 300, loss is: [1.0029807], acc is [0.671875]\n",
      "epoch: 14, batch: 450, loss is: [1.4266236], acc is [0.453125]\n",
      "epoch: 14, batch: 600, loss is: [1.3388371], acc is [0.515625]\n",
      "epoch: 14, batch: 750, loss is: [1.5527712], acc is [0.46875]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_two2 = Cifar_THree_CONV2()\n",
    "train_Cifar_Three(model_cifar_two2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 使用dropout\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "class Cifar_TWO_CONV2_drop(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_drop, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 再定义一层卷积层\n",
    "        self.conv3 = Conv2D(in_channels=20, out_channels=32,kernel_size=5, stride=1, padding=2)\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc = Linear(in_features=2048, out_features=3072)\n",
    "        self.fc2 = Linear(in_features=3072,out_features=1024)\n",
    "        self.fc3 = Linear(in_features=1024, out_features=10)\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        x = F.dropout(x, p = 0.007,mode=mode)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p = 0.005, mode=mode)\n",
    "        x = self.fc3(x)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/three_conv_drop\")\n",
    "def train_Cifar_TWO2_drop(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 15\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels,mode='downscale_in_infer')\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.6741738], acc is [0.03125]\n",
      "epoch: 0, batch: 150, loss is: [2.6778302], acc is [0.09375]\n",
      "epoch: 0, batch: 300, loss is: [2.422026], acc is [0.09375]\n",
      "epoch: 0, batch: 450, loss is: [2.3471777], acc is [0.078125]\n",
      "epoch: 0, batch: 600, loss is: [2.3338192], acc is [0.0625]\n",
      "epoch: 0, batch: 750, loss is: [2.3259423], acc is [0.140625]\n",
      "epoch: 1, batch: 0, loss is: [2.3339372], acc is [0.09375]\n",
      "epoch: 1, batch: 150, loss is: [2.3465264], acc is [0.125]\n",
      "epoch: 1, batch: 300, loss is: [2.3495932], acc is [0.125]\n",
      "epoch: 1, batch: 450, loss is: [2.2595706], acc is [0.1875]\n",
      "epoch: 1, batch: 600, loss is: [2.3430743], acc is [0.109375]\n",
      "epoch: 1, batch: 750, loss is: [2.339295], acc is [0.09375]\n",
      "epoch: 2, batch: 0, loss is: [2.298511], acc is [0.125]\n",
      "epoch: 2, batch: 150, loss is: [2.3004704], acc is [0.140625]\n",
      "epoch: 2, batch: 300, loss is: [4908.909], acc is [0.125]\n",
      "epoch: 2, batch: 450, loss is: [101.91498], acc is [0.078125]\n",
      "epoch: 2, batch: 600, loss is: [4.698474], acc is [0.03125]\n",
      "epoch: 2, batch: 750, loss is: [2.6009758], acc is [0.046875]\n",
      "epoch: 3, batch: 0, loss is: [2.5634532], acc is [0.171875]\n",
      "epoch: 3, batch: 150, loss is: [2.8048935], acc is [0.046875]\n",
      "epoch: 3, batch: 300, loss is: [3.292232], acc is [0.109375]\n",
      "epoch: 3, batch: 450, loss is: [2.6680384], acc is [0.1875]\n",
      "epoch: 3, batch: 600, loss is: [2.61864], acc is [0.09375]\n",
      "epoch: 3, batch: 750, loss is: [2.738772], acc is [0.015625]\n",
      "epoch: 4, batch: 0, loss is: [2.367083], acc is [0.125]\n",
      "epoch: 4, batch: 150, loss is: [2.4707816], acc is [0.09375]\n",
      "epoch: 4, batch: 300, loss is: [2.3458292], acc is [0.140625]\n",
      "epoch: 4, batch: 450, loss is: [2.4788456], acc is [0.109375]\n",
      "epoch: 4, batch: 600, loss is: [2.5485017], acc is [0.109375]\n",
      "epoch: 4, batch: 750, loss is: [2.4576423], acc is [0.078125]\n",
      "epoch: 5, batch: 0, loss is: [2.3609521], acc is [0.046875]\n",
      "epoch: 5, batch: 150, loss is: [2.4015346], acc is [0.109375]\n",
      "epoch: 5, batch: 300, loss is: [2.5674553], acc is [0.0625]\n",
      "epoch: 5, batch: 450, loss is: [2.4055924], acc is [0.078125]\n",
      "epoch: 5, batch: 600, loss is: [2.3083491], acc is [0.0625]\n",
      "epoch: 5, batch: 750, loss is: [2.510928], acc is [0.0625]\n",
      "epoch: 6, batch: 0, loss is: [2.304487], acc is [0.046875]\n",
      "epoch: 6, batch: 150, loss is: [2.3468785], acc is [0.125]\n",
      "epoch: 6, batch: 300, loss is: [2.41815], acc is [0.125]\n",
      "epoch: 6, batch: 450, loss is: [2.3337767], acc is [0.109375]\n",
      "epoch: 6, batch: 600, loss is: [2.4526725], acc is [0.046875]\n",
      "epoch: 6, batch: 750, loss is: [2.5199957], acc is [0.09375]\n",
      "epoch: 7, batch: 0, loss is: [2.3378186], acc is [0.125]\n",
      "epoch: 7, batch: 150, loss is: [2.3300033], acc is [0.09375]\n",
      "epoch: 7, batch: 300, loss is: [2.3344293], acc is [0.09375]\n",
      "epoch: 7, batch: 450, loss is: [2.4027789], acc is [0.078125]\n",
      "epoch: 7, batch: 600, loss is: [2.3440218], acc is [0.125]\n",
      "epoch: 7, batch: 750, loss is: [2.4022243], acc is [0.125]\n",
      "epoch: 8, batch: 0, loss is: [2.3417306], acc is [0.109375]\n",
      "epoch: 8, batch: 150, loss is: [2.314919], acc is [0.125]\n",
      "epoch: 8, batch: 300, loss is: [2.3191948], acc is [0.109375]\n",
      "epoch: 8, batch: 450, loss is: [2.3022428], acc is [0.078125]\n",
      "epoch: 8, batch: 600, loss is: [2.3400824], acc is [0.078125]\n",
      "epoch: 8, batch: 750, loss is: [2.316184], acc is [0.078125]\n",
      "epoch: 9, batch: 0, loss is: [2.3032594], acc is [0.109375]\n",
      "epoch: 9, batch: 150, loss is: [2.427879], acc is [0.109375]\n",
      "epoch: 9, batch: 300, loss is: [438.5227], acc is [0.15625]\n",
      "epoch: 9, batch: 450, loss is: [54.815166], acc is [0.078125]\n",
      "epoch: 9, batch: 600, loss is: [10.676128], acc is [0.03125]\n",
      "epoch: 9, batch: 750, loss is: [3.026719], acc is [0.109375]\n",
      "epoch: 10, batch: 0, loss is: [2.7255983], acc is [0.203125]\n",
      "epoch: 10, batch: 150, loss is: [4.2268877], acc is [0.125]\n",
      "epoch: 10, batch: 300, loss is: [3.9270878], acc is [0.125]\n",
      "epoch: 10, batch: 450, loss is: [2.9774601], acc is [0.125]\n",
      "epoch: 10, batch: 600, loss is: [3.1713066], acc is [0.140625]\n",
      "epoch: 10, batch: 750, loss is: [3.0826044], acc is [0.078125]\n",
      "epoch: 11, batch: 0, loss is: [2.4896877], acc is [0.03125]\n",
      "epoch: 11, batch: 150, loss is: [3.666436], acc is [0.078125]\n",
      "epoch: 11, batch: 300, loss is: [3.0172293], acc is [0.0625]\n",
      "epoch: 11, batch: 450, loss is: [2.6280797], acc is [0.0625]\n",
      "epoch: 11, batch: 600, loss is: [2.293543], acc is [0.09375]\n",
      "epoch: 11, batch: 750, loss is: [2.4638395], acc is [0.09375]\n",
      "epoch: 12, batch: 0, loss is: [2.9189053], acc is [0.109375]\n",
      "epoch: 12, batch: 150, loss is: [2.4501665], acc is [0.046875]\n",
      "epoch: 12, batch: 300, loss is: [3.1463935], acc is [0.125]\n",
      "epoch: 12, batch: 450, loss is: [2.5857694], acc is [0.125]\n",
      "epoch: 12, batch: 600, loss is: [2.409114], acc is [0.109375]\n",
      "epoch: 12, batch: 750, loss is: [2.420825], acc is [0.03125]\n",
      "epoch: 13, batch: 0, loss is: [2.8581562], acc is [0.078125]\n",
      "epoch: 13, batch: 150, loss is: [2.5002327], acc is [0.171875]\n",
      "epoch: 13, batch: 300, loss is: [2.4941852], acc is [0.109375]\n",
      "epoch: 13, batch: 450, loss is: [2.3554559], acc is [0.125]\n",
      "epoch: 13, batch: 600, loss is: [2.421176], acc is [0.078125]\n",
      "epoch: 13, batch: 750, loss is: [2.3688736], acc is [0.1875]\n",
      "epoch: 14, batch: 0, loss is: [2.3989573], acc is [0.125]\n",
      "epoch: 14, batch: 150, loss is: [2.4605236], acc is [0.125]\n",
      "epoch: 14, batch: 300, loss is: [2.295824], acc is [0.09375]\n",
      "epoch: 14, batch: 450, loss is: [2.4984424], acc is [0.109375]\n",
      "epoch: 14, batch: 600, loss is: [2.4523313], acc is [0.109375]\n",
      "epoch: 14, batch: 750, loss is: [2.469772], acc is [0.0625]\n"
     ]
    }
   ],
   "source": [
    "model_cifar_drop = Cifar_TWO_CONV2_drop()\n",
    "train_Cifar_TWO2_drop(model_cifar_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 使用dropout\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, BatchNorm2D\n",
    "\n",
    "class Cifar_TWO_CONV2_norm(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_norm, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义批归一化层\n",
    "        self.bn1 = BatchNorm2D(12)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=2)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = BatchNorm2D(24)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第三个卷积层\n",
    "        self.conv3 = Conv2D(in_channels=24, out_channels= 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = BatchNorm2D(32)\n",
    "        self.max_pool3 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是32*8*8,定义一个全连接\n",
    "        self.fc = Linear(in_features=512, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = F.dropout(x, p = 0.01,mode=mode)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.nn import BatchNorm2D\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_new_batnorm1\")\n",
    "def train_Cifar_TWO2_norm(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 15\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.970825], acc is [0.234375]\n",
      "epoch: 0, batch: 150, loss is: [1.5816582], acc is [0.359375]\n",
      "epoch: 0, batch: 300, loss is: [1.4609181], acc is [0.4375]\n",
      "epoch: 0, batch: 450, loss is: [1.1803728], acc is [0.59375]\n",
      "epoch: 0, batch: 600, loss is: [0.94525087], acc is [0.65625]\n",
      "epoch: 0, batch: 750, loss is: [1.3759532], acc is [0.578125]\n",
      "epoch: 1, batch: 0, loss is: [1.1294346], acc is [0.640625]\n",
      "epoch: 1, batch: 150, loss is: [1.0835528], acc is [0.609375]\n",
      "epoch: 1, batch: 300, loss is: [0.7557789], acc is [0.75]\n",
      "epoch: 1, batch: 450, loss is: [1.0806919], acc is [0.59375]\n",
      "epoch: 1, batch: 600, loss is: [1.0914581], acc is [0.6875]\n",
      "epoch: 1, batch: 750, loss is: [0.90149915], acc is [0.71875]\n",
      "epoch: 2, batch: 0, loss is: [1.011965], acc is [0.671875]\n",
      "epoch: 2, batch: 150, loss is: [1.1985189], acc is [0.640625]\n",
      "epoch: 2, batch: 300, loss is: [0.9783133], acc is [0.6875]\n",
      "epoch: 2, batch: 450, loss is: [1.025254], acc is [0.671875]\n",
      "epoch: 2, batch: 600, loss is: [0.93987715], acc is [0.65625]\n",
      "epoch: 2, batch: 750, loss is: [0.90811604], acc is [0.71875]\n",
      "epoch: 3, batch: 0, loss is: [0.8984541], acc is [0.6875]\n",
      "epoch: 3, batch: 150, loss is: [0.8487873], acc is [0.703125]\n",
      "epoch: 3, batch: 300, loss is: [0.9356247], acc is [0.640625]\n",
      "epoch: 3, batch: 450, loss is: [0.81221414], acc is [0.75]\n",
      "epoch: 3, batch: 600, loss is: [1.0533799], acc is [0.6875]\n",
      "epoch: 3, batch: 750, loss is: [0.7623317], acc is [0.8125]\n",
      "epoch: 4, batch: 0, loss is: [0.9394107], acc is [0.671875]\n",
      "epoch: 4, batch: 150, loss is: [0.90263695], acc is [0.625]\n",
      "epoch: 4, batch: 300, loss is: [0.7747778], acc is [0.6875]\n",
      "epoch: 4, batch: 450, loss is: [0.7365738], acc is [0.75]\n",
      "epoch: 4, batch: 600, loss is: [0.7457768], acc is [0.78125]\n",
      "epoch: 4, batch: 750, loss is: [0.92329735], acc is [0.640625]\n",
      "epoch: 5, batch: 0, loss is: [0.75420237], acc is [0.6875]\n",
      "epoch: 5, batch: 150, loss is: [0.71448576], acc is [0.734375]\n",
      "epoch: 5, batch: 300, loss is: [0.70835173], acc is [0.6875]\n",
      "epoch: 5, batch: 450, loss is: [0.8711908], acc is [0.734375]\n",
      "epoch: 5, batch: 600, loss is: [0.80260503], acc is [0.671875]\n",
      "epoch: 5, batch: 750, loss is: [0.9413054], acc is [0.71875]\n",
      "epoch: 6, batch: 0, loss is: [0.6390817], acc is [0.78125]\n",
      "epoch: 6, batch: 150, loss is: [0.7538954], acc is [0.75]\n",
      "epoch: 6, batch: 300, loss is: [0.73058593], acc is [0.796875]\n",
      "epoch: 6, batch: 450, loss is: [0.77887344], acc is [0.703125]\n",
      "epoch: 6, batch: 600, loss is: [0.84759676], acc is [0.671875]\n",
      "epoch: 6, batch: 750, loss is: [0.8546134], acc is [0.671875]\n",
      "epoch: 7, batch: 0, loss is: [0.5458887], acc is [0.8125]\n",
      "epoch: 7, batch: 150, loss is: [0.7548553], acc is [0.734375]\n",
      "epoch: 7, batch: 300, loss is: [0.6777015], acc is [0.78125]\n",
      "epoch: 7, batch: 450, loss is: [0.6712435], acc is [0.75]\n",
      "epoch: 7, batch: 600, loss is: [0.71804285], acc is [0.6875]\n",
      "epoch: 7, batch: 750, loss is: [0.9582903], acc is [0.65625]\n",
      "epoch: 8, batch: 0, loss is: [0.5260777], acc is [0.828125]\n",
      "epoch: 8, batch: 150, loss is: [0.74066174], acc is [0.71875]\n",
      "epoch: 8, batch: 300, loss is: [0.5956972], acc is [0.765625]\n",
      "epoch: 8, batch: 450, loss is: [0.78016007], acc is [0.71875]\n",
      "epoch: 8, batch: 600, loss is: [0.8665546], acc is [0.703125]\n",
      "epoch: 8, batch: 750, loss is: [0.57188183], acc is [0.828125]\n",
      "epoch: 9, batch: 0, loss is: [0.6077843], acc is [0.75]\n",
      "epoch: 9, batch: 150, loss is: [0.65063417], acc is [0.796875]\n",
      "epoch: 9, batch: 300, loss is: [0.5043288], acc is [0.859375]\n",
      "epoch: 9, batch: 450, loss is: [0.537404], acc is [0.78125]\n",
      "epoch: 9, batch: 600, loss is: [0.89861196], acc is [0.78125]\n",
      "epoch: 9, batch: 750, loss is: [0.6316432], acc is [0.765625]\n",
      "epoch: 10, batch: 0, loss is: [0.46902388], acc is [0.8125]\n",
      "epoch: 10, batch: 150, loss is: [0.4813259], acc is [0.8125]\n",
      "epoch: 10, batch: 300, loss is: [0.6693717], acc is [0.703125]\n",
      "epoch: 10, batch: 450, loss is: [0.5935971], acc is [0.734375]\n",
      "epoch: 10, batch: 600, loss is: [0.8056344], acc is [0.65625]\n",
      "epoch: 10, batch: 750, loss is: [0.48368323], acc is [0.875]\n",
      "epoch: 11, batch: 0, loss is: [0.7565255], acc is [0.734375]\n",
      "epoch: 11, batch: 150, loss is: [0.49497658], acc is [0.796875]\n",
      "epoch: 11, batch: 300, loss is: [0.6956666], acc is [0.703125]\n",
      "epoch: 11, batch: 450, loss is: [0.5048349], acc is [0.8125]\n",
      "epoch: 11, batch: 600, loss is: [0.79430664], acc is [0.75]\n",
      "epoch: 11, batch: 750, loss is: [0.7342331], acc is [0.765625]\n",
      "epoch: 12, batch: 0, loss is: [0.73620117], acc is [0.765625]\n",
      "epoch: 12, batch: 150, loss is: [0.47885382], acc is [0.875]\n",
      "epoch: 12, batch: 300, loss is: [0.5001105], acc is [0.84375]\n",
      "epoch: 12, batch: 450, loss is: [0.39125985], acc is [0.875]\n",
      "epoch: 12, batch: 600, loss is: [0.68546295], acc is [0.703125]\n",
      "epoch: 12, batch: 750, loss is: [0.57635826], acc is [0.78125]\n",
      "epoch: 13, batch: 0, loss is: [0.620057], acc is [0.796875]\n",
      "epoch: 13, batch: 150, loss is: [0.61436844], acc is [0.765625]\n",
      "epoch: 13, batch: 300, loss is: [0.55291545], acc is [0.8125]\n",
      "epoch: 13, batch: 450, loss is: [0.77187663], acc is [0.6875]\n",
      "epoch: 13, batch: 600, loss is: [0.54882336], acc is [0.765625]\n",
      "epoch: 13, batch: 750, loss is: [0.65070665], acc is [0.828125]\n",
      "epoch: 14, batch: 0, loss is: [0.65286815], acc is [0.796875]\n",
      "epoch: 14, batch: 150, loss is: [0.70906854], acc is [0.75]\n",
      "epoch: 14, batch: 300, loss is: [0.44498402], acc is [0.84375]\n",
      "epoch: 14, batch: 450, loss is: [0.7401669], acc is [0.765625]\n",
      "epoch: 14, batch: 600, loss is: [0.547703], acc is [0.828125]\n",
      "epoch: 14, batch: 750, loss is: [0.778971], acc is [0.765625]\n"
     ]
    }
   ],
   "source": [
    "model = Cifar_TWO_CONV2_norm()\n",
    "train_Cifar_TWO2_norm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 使用dropout\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, BatchNorm2D\n",
    "\n",
    "class Cifar_TWO_CONV2_norm_end(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_norm_end, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        # 定义批归一化层\n",
    "        self.bn1 = BatchNorm2D(12)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=2)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = BatchNorm2D(24)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第三个卷积层\n",
    "        self.conv3 = Conv2D(in_channels=24, out_channels= 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = BatchNorm2D(32)\n",
    "        self.max_pool3 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 这样输出的就是32*8*8,定义一个全连接\n",
    "        self.fc = Linear(in_features=512, out_features=218)\n",
    "        self.fc2 = Linear(in_features=218,out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = self.fc(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.dropout(x, p = 0.01,mode=mode)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.nn import BatchNorm2D\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_end\")\n",
    "def train_Cifar_TWO2_end(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 20\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.01, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%150 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=150\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 100 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=100\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:777: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [0.64566314], acc is [0.75]\n",
      "epoch: 0, batch: 150, loss is: [0.48755217], acc is [0.859375]\n",
      "epoch: 0, batch: 300, loss is: [0.5207561], acc is [0.8125]\n",
      "epoch: 0, batch: 450, loss is: [0.8076332], acc is [0.75]\n",
      "epoch: 0, batch: 600, loss is: [0.7619858], acc is [0.703125]\n",
      "epoch: 0, batch: 750, loss is: [0.65657187], acc is [0.71875]\n",
      "epoch: 1, batch: 0, loss is: [0.44764915], acc is [0.859375]\n",
      "epoch: 1, batch: 150, loss is: [0.5023011], acc is [0.828125]\n",
      "epoch: 1, batch: 300, loss is: [0.5162792], acc is [0.765625]\n",
      "epoch: 1, batch: 450, loss is: [0.49148616], acc is [0.859375]\n",
      "epoch: 1, batch: 600, loss is: [0.584095], acc is [0.828125]\n",
      "epoch: 1, batch: 750, loss is: [0.54790777], acc is [0.8125]\n",
      "epoch: 2, batch: 0, loss is: [0.70908505], acc is [0.703125]\n",
      "epoch: 2, batch: 150, loss is: [0.43550918], acc is [0.859375]\n",
      "epoch: 2, batch: 300, loss is: [0.74243844], acc is [0.796875]\n",
      "epoch: 2, batch: 450, loss is: [0.52732307], acc is [0.765625]\n",
      "epoch: 2, batch: 600, loss is: [0.65305674], acc is [0.765625]\n",
      "epoch: 2, batch: 750, loss is: [0.6142598], acc is [0.765625]\n",
      "epoch: 3, batch: 0, loss is: [0.5691579], acc is [0.765625]\n",
      "epoch: 3, batch: 150, loss is: [0.590785], acc is [0.796875]\n",
      "epoch: 3, batch: 300, loss is: [0.8244101], acc is [0.75]\n",
      "epoch: 3, batch: 450, loss is: [0.5136351], acc is [0.859375]\n",
      "epoch: 3, batch: 600, loss is: [0.52938354], acc is [0.828125]\n",
      "epoch: 3, batch: 750, loss is: [0.47643006], acc is [0.828125]\n",
      "epoch: 4, batch: 0, loss is: [0.8858811], acc is [0.71875]\n",
      "epoch: 4, batch: 150, loss is: [0.4494521], acc is [0.875]\n",
      "epoch: 4, batch: 300, loss is: [0.39320332], acc is [0.859375]\n",
      "epoch: 4, batch: 450, loss is: [0.67221636], acc is [0.78125]\n",
      "epoch: 4, batch: 600, loss is: [0.60763556], acc is [0.734375]\n",
      "epoch: 4, batch: 750, loss is: [0.5153452], acc is [0.796875]\n",
      "epoch: 5, batch: 0, loss is: [0.38465372], acc is [0.828125]\n",
      "epoch: 5, batch: 150, loss is: [0.47948852], acc is [0.765625]\n",
      "epoch: 5, batch: 300, loss is: [0.5345385], acc is [0.78125]\n",
      "epoch: 5, batch: 450, loss is: [0.4111966], acc is [0.859375]\n",
      "epoch: 5, batch: 600, loss is: [0.49003303], acc is [0.828125]\n",
      "epoch: 5, batch: 750, loss is: [0.7593671], acc is [0.6875]\n",
      "epoch: 6, batch: 0, loss is: [0.5891522], acc is [0.78125]\n",
      "epoch: 6, batch: 150, loss is: [0.33403575], acc is [0.84375]\n",
      "epoch: 6, batch: 300, loss is: [0.69928503], acc is [0.734375]\n",
      "epoch: 6, batch: 450, loss is: [0.6099534], acc is [0.75]\n",
      "epoch: 6, batch: 600, loss is: [0.57304823], acc is [0.734375]\n",
      "epoch: 6, batch: 750, loss is: [0.63418776], acc is [0.765625]\n",
      "epoch: 7, batch: 0, loss is: [0.5807794], acc is [0.78125]\n",
      "epoch: 7, batch: 150, loss is: [0.53037465], acc is [0.765625]\n",
      "epoch: 7, batch: 300, loss is: [0.7635553], acc is [0.75]\n",
      "epoch: 7, batch: 450, loss is: [0.54081553], acc is [0.796875]\n",
      "epoch: 7, batch: 600, loss is: [0.6048659], acc is [0.828125]\n",
      "epoch: 7, batch: 750, loss is: [0.41279688], acc is [0.8125]\n",
      "epoch: 8, batch: 0, loss is: [0.3273821], acc is [0.90625]\n",
      "epoch: 8, batch: 150, loss is: [0.50258505], acc is [0.828125]\n",
      "epoch: 8, batch: 300, loss is: [0.43308967], acc is [0.84375]\n",
      "epoch: 8, batch: 450, loss is: [0.5987557], acc is [0.78125]\n",
      "epoch: 8, batch: 600, loss is: [0.6627099], acc is [0.6875]\n",
      "epoch: 8, batch: 750, loss is: [0.71276116], acc is [0.734375]\n",
      "epoch: 9, batch: 0, loss is: [0.4935615], acc is [0.875]\n",
      "epoch: 9, batch: 150, loss is: [0.465438], acc is [0.875]\n",
      "epoch: 9, batch: 300, loss is: [0.47898394], acc is [0.828125]\n",
      "epoch: 9, batch: 450, loss is: [0.5739523], acc is [0.75]\n",
      "epoch: 9, batch: 600, loss is: [0.49254963], acc is [0.828125]\n",
      "epoch: 9, batch: 750, loss is: [0.58694077], acc is [0.796875]\n",
      "epoch: 10, batch: 0, loss is: [0.41594002], acc is [0.875]\n",
      "epoch: 10, batch: 150, loss is: [0.5108686], acc is [0.828125]\n",
      "epoch: 10, batch: 300, loss is: [0.68995833], acc is [0.78125]\n",
      "epoch: 10, batch: 450, loss is: [0.7079825], acc is [0.734375]\n",
      "epoch: 10, batch: 600, loss is: [0.6928464], acc is [0.796875]\n",
      "epoch: 10, batch: 750, loss is: [0.34316838], acc is [0.859375]\n",
      "epoch: 11, batch: 0, loss is: [0.5090332], acc is [0.84375]\n",
      "epoch: 11, batch: 150, loss is: [0.4645617], acc is [0.84375]\n",
      "epoch: 11, batch: 300, loss is: [0.5292378], acc is [0.796875]\n",
      "epoch: 11, batch: 450, loss is: [0.5054441], acc is [0.765625]\n",
      "epoch: 11, batch: 600, loss is: [0.6094003], acc is [0.765625]\n",
      "epoch: 11, batch: 750, loss is: [0.56718487], acc is [0.765625]\n",
      "epoch: 12, batch: 0, loss is: [0.5109044], acc is [0.859375]\n",
      "epoch: 12, batch: 150, loss is: [0.47459385], acc is [0.796875]\n",
      "epoch: 12, batch: 300, loss is: [0.41105035], acc is [0.84375]\n",
      "epoch: 12, batch: 450, loss is: [0.7731143], acc is [0.765625]\n",
      "epoch: 12, batch: 600, loss is: [0.42119122], acc is [0.875]\n",
      "epoch: 12, batch: 750, loss is: [0.45863953], acc is [0.8125]\n",
      "epoch: 13, batch: 0, loss is: [0.43497393], acc is [0.828125]\n",
      "epoch: 13, batch: 150, loss is: [0.3391599], acc is [0.859375]\n",
      "epoch: 13, batch: 300, loss is: [0.40379548], acc is [0.8125]\n",
      "epoch: 13, batch: 450, loss is: [0.23298526], acc is [0.921875]\n",
      "epoch: 13, batch: 600, loss is: [0.43299043], acc is [0.859375]\n",
      "epoch: 13, batch: 750, loss is: [0.7880639], acc is [0.75]\n",
      "epoch: 14, batch: 0, loss is: [0.42331052], acc is [0.84375]\n",
      "epoch: 14, batch: 150, loss is: [0.48910207], acc is [0.796875]\n",
      "epoch: 14, batch: 300, loss is: [0.47421712], acc is [0.84375]\n",
      "epoch: 14, batch: 450, loss is: [0.5095426], acc is [0.828125]\n",
      "epoch: 14, batch: 600, loss is: [0.39328483], acc is [0.84375]\n",
      "epoch: 14, batch: 750, loss is: [0.53179306], acc is [0.78125]\n",
      "epoch: 15, batch: 0, loss is: [0.4499734], acc is [0.84375]\n",
      "epoch: 15, batch: 150, loss is: [0.4844087], acc is [0.859375]\n",
      "epoch: 15, batch: 300, loss is: [0.2903475], acc is [0.890625]\n",
      "epoch: 15, batch: 450, loss is: [0.5130213], acc is [0.765625]\n",
      "epoch: 15, batch: 600, loss is: [0.63654006], acc is [0.796875]\n",
      "epoch: 15, batch: 750, loss is: [0.677179], acc is [0.75]\n",
      "epoch: 16, batch: 0, loss is: [0.42867744], acc is [0.84375]\n",
      "epoch: 16, batch: 150, loss is: [0.49378732], acc is [0.8125]\n",
      "epoch: 16, batch: 300, loss is: [0.34566712], acc is [0.890625]\n",
      "epoch: 16, batch: 450, loss is: [0.6216757], acc is [0.828125]\n",
      "epoch: 16, batch: 600, loss is: [0.4235772], acc is [0.828125]\n",
      "epoch: 16, batch: 750, loss is: [0.36732224], acc is [0.84375]\n",
      "epoch: 17, batch: 0, loss is: [0.5590066], acc is [0.75]\n",
      "epoch: 17, batch: 150, loss is: [0.5384346], acc is [0.796875]\n",
      "epoch: 17, batch: 300, loss is: [0.5655963], acc is [0.796875]\n",
      "epoch: 17, batch: 450, loss is: [0.5589092], acc is [0.828125]\n",
      "epoch: 17, batch: 600, loss is: [0.40881312], acc is [0.828125]\n",
      "epoch: 17, batch: 750, loss is: [0.4117507], acc is [0.828125]\n",
      "epoch: 18, batch: 0, loss is: [0.6530666], acc is [0.78125]\n",
      "epoch: 18, batch: 150, loss is: [0.42460626], acc is [0.828125]\n",
      "epoch: 18, batch: 300, loss is: [0.5127356], acc is [0.8125]\n",
      "epoch: 18, batch: 450, loss is: [0.5018427], acc is [0.84375]\n",
      "epoch: 18, batch: 600, loss is: [0.52790403], acc is [0.78125]\n",
      "epoch: 18, batch: 750, loss is: [0.49006706], acc is [0.796875]\n",
      "epoch: 19, batch: 0, loss is: [0.5047963], acc is [0.828125]\n",
      "epoch: 19, batch: 150, loss is: [0.33666486], acc is [0.890625]\n",
      "epoch: 19, batch: 300, loss is: [0.62680286], acc is [0.796875]\n",
      "epoch: 19, batch: 450, loss is: [0.68372357], acc is [0.78125]\n",
      "epoch: 19, batch: 600, loss is: [0.68350947], acc is [0.75]\n",
      "epoch: 19, batch: 750, loss is: [0.6191951], acc is [0.8125]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "mdoel = Cifar_TWO_CONV2_norm_end()\n",
    "train_Cifar_TWO2_end(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "# 把以上的都综合在一起\n",
    "\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear, BatchNorm2D\n",
    "\n",
    "class Cifar_TWO_CONV2_lenet_3(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(Cifar_TWO_CONV2_lenet_3, self).__init__()\n",
    "\n",
    "        # 定义一层卷积层\n",
    "        self.conv1 = Conv2D(in_channels=3, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义批归一化层\n",
    "        self.bn1 = BatchNorm2D(32)\n",
    "        # 定义最大池化\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义第二层卷积层\n",
    "        self.conv2 = Conv2D(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        # 第二个批归一化层\n",
    "        self.bn2 = BatchNorm2D(64)\n",
    "        # 定义最大池化\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 再增加一层卷积\n",
    "        self.conv3 = Conv2D(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = BatchNorm2D(64)\n",
    "\n",
    "        # 这样输出的就是20*16*16,定义一个全连接\n",
    "        self.fc1 = Linear(in_features=4096, out_features=10)\n",
    "        self.fc3 = Linear(in_features=1024, out_features=10)\n",
    "\n",
    "    def forward(self, inputs, label, mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], -1])\n",
    "        x = paddle.to_tensor(x)\n",
    "        x = self.fc1(x)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label = label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "from paddle.optimizer.lr import CosineAnnealingDecay\n",
    "from paddle.nn import BatchNorm2D\n",
    "logwriter = LogWriter(logdir=\"./run/cifar_log/two_conv_new_lenet7\")\n",
    "def train_Cifar_TWO2_lenet(model):\n",
    "    # 定义优化器\n",
    "    EPOCH_NUM = 15\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.005, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            # 数据准备\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels, (-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predict, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predict, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "\n",
    "            # 可以记录\n",
    "            if batch_id%200 ==  0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                logwriter.add_scalar(tag = 'train/acc', step = iter, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag = 'train/loss' , step = iter, value=avg_loss.numpy())\n",
    "                iter+=200\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.reshape(labels,(-1,1))\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss= paddle.mean(loss)\n",
    "\n",
    "            if batch_id % 200 == 0:\n",
    "                logwriter.add_scalar(tag='test/acc', step=iter2, value=acc.numpy())\n",
    "                logwriter.add_scalar(tag='test/loss', step=iter2, value=avg_loss.numpy())\n",
    "                iter2+=200\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [2.5494406], acc is [0.1328125]\n",
      "epoch: 0, batch: 200, loss is: [1.546255], acc is [0.4609375]\n",
      "epoch: 1, batch: 0, loss is: [1.4955535], acc is [0.5]\n",
      "epoch: 1, batch: 200, loss is: [1.3238297], acc is [0.6015625]\n",
      "epoch: 2, batch: 0, loss is: [1.2607361], acc is [0.59375]\n",
      "epoch: 2, batch: 200, loss is: [1.2540855], acc is [0.6328125]\n",
      "epoch: 3, batch: 0, loss is: [1.102382], acc is [0.640625]\n",
      "epoch: 3, batch: 200, loss is: [1.1954417], acc is [0.6640625]\n",
      "epoch: 4, batch: 0, loss is: [1.124525], acc is [0.640625]\n",
      "epoch: 4, batch: 200, loss is: [1.3655242], acc is [0.578125]\n",
      "epoch: 5, batch: 0, loss is: [1.1353847], acc is [0.625]\n",
      "epoch: 5, batch: 200, loss is: [1.190679], acc is [0.625]\n",
      "epoch: 6, batch: 0, loss is: [1.0680808], acc is [0.609375]\n",
      "epoch: 6, batch: 200, loss is: [1.2858982], acc is [0.6328125]\n",
      "epoch: 7, batch: 0, loss is: [1.3763033], acc is [0.578125]\n",
      "epoch: 7, batch: 200, loss is: [1.1359574], acc is [0.640625]\n",
      "epoch: 8, batch: 0, loss is: [1.1208253], acc is [0.6796875]\n",
      "epoch: 8, batch: 200, loss is: [1.131259], acc is [0.6328125]\n",
      "epoch: 9, batch: 0, loss is: [1.0670694], acc is [0.6328125]\n",
      "epoch: 9, batch: 200, loss is: [1.5183477], acc is [0.6015625]\n",
      "epoch: 10, batch: 0, loss is: [1.2377145], acc is [0.625]\n",
      "epoch: 10, batch: 200, loss is: [1.1134982], acc is [0.625]\n",
      "epoch: 11, batch: 0, loss is: [1.2755997], acc is [0.640625]\n",
      "epoch: 11, batch: 200, loss is: [1.081486], acc is [0.59375]\n",
      "epoch: 12, batch: 0, loss is: [1.1003363], acc is [0.65625]\n",
      "epoch: 12, batch: 200, loss is: [1.0382322], acc is [0.6953125]\n",
      "epoch: 13, batch: 0, loss is: [0.87438166], acc is [0.7421875]\n",
      "epoch: 13, batch: 200, loss is: [1.3153304], acc is [0.65625]\n",
      "epoch: 14, batch: 0, loss is: [1.0955257], acc is [0.6640625]\n",
      "epoch: 14, batch: 200, loss is: [1.2748395], acc is [0.609375]\n"
     ]
    }
   ],
   "source": [
    "model = Cifar_TWO_CONV2_lenet_3()\n",
    "train_Cifar_TWO2_lenet(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
