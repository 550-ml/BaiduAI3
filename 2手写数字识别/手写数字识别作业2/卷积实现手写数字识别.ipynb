{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.vision.transforms import Normalize\n",
    "\n",
    "def get_MNIST_dataloader():\n",
    "    # 定义图像归一化处理方法，这里的CHW指图像格式需为 [C通道数，H图像高度，W图像宽度]\n",
    "    transform = Normalize(mean=[127.5], std=[127.5], data_format='chw')\n",
    "    # 下载数据集并初始化 dataset\n",
    "    train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\n",
    "    test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)\n",
    "\n",
    "    # 定义并初始化数据读取器\n",
    "    train_loader = paddle.io.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=1, drop_last=True)\n",
    "    test_loader = paddle.io.DataLoader(test_dataset, batch_size=129, shuffle=False, num_workers=1, drop_last=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_MNIST_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20556"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualdl\n",
    "import visualdl.server.app \n",
    "visualdl.server.app.run('./log',\n",
    "                        host=\"127.0.0.1\",\n",
    "                        port=8080,\n",
    "                        cache_timeout=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "# 多层卷积神经网络实现\n",
    "class MNIST_CONV_INIT(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CONV_INIT, self).__init__()\n",
    "\n",
    "        # 定义卷积层，输出特征通道20，卷积核大小5\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小为2，步长为2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义卷积层，输出特征20维，卷积核大小为5\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义一层全连接层，输入维度是10\n",
    "        self.fc = Linear(in_features=980, out_features=10)\n",
    "\n",
    "    # 定义前向，卷积后使用池化，然后全连接输出\n",
    "    # 激活函数使用ReLU，全连接层使用softmax\n",
    "    def forward(self, inputs, label):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], 980])\n",
    "        x = self.fc(x)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label=label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "def train_MNIST_INIT(model):\n",
    "\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.003, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 5\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            images, labels = data \n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predicts, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "\n",
    "            # 记录损失\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc_of_train', step = iter, value=acc.numpy() )\n",
    "                log_writer.add_scalar(tag = 'loss_of_train', step = iter, value=avg_loss.numpy())\n",
    "                iter += 100\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            if batch_id % 100 == 0:\n",
    "                log_writer.add_scalar(tag = 'acc_of_test', step=iter2, value=acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss_of_test', step=iter2, value=avg_loss.numpy())\n",
    "                iter2 += 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNIST_CONV_INIT()\n",
    "train_MNIST_INIT(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\fluid\\variable_index.py:591: UserWarning: Warning: In Tensor '__getitem__', if the number of scalar elements in the index is equal to the rank of the Tensor, the output should be 0-D. In order to be consistent with the behavior of previous versions, it will be processed to 1-D. But it is not correct and will be removed in release 2.6. If 1-D is still wanted, please modify the index element from scalar to slice (e.g. 'x[i]' => 'x[i:i+1]').\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "paddle.jit.save(model, '.model', [paddle.static.InputSpec([-1,1,28,28]),None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "def train_MNIST_L2(model):\n",
    "\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001, weight_decay=paddle.regularizer.L2Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 5\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            images, labels = data \n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predicts, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "\n",
    "            # 记录损失\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc_of_train_L2', step = iter, value=acc.numpy() )\n",
    "                log_writer.add_scalar(tag = 'loss_of_train_L2', step = iter, value=avg_loss.numpy())\n",
    "                iter += 100\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            if batch_id % 100 == 0:\n",
    "                log_writer.add_scalar(tag = 'acc_of_test_L2', step=iter2, value=acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss_of_test_L2', step=iter2, value=avg_loss.numpy())\n",
    "                iter2 += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.888531], acc is [0.125]\n",
      "epoch: 0, batch: 100, loss is: [0.21369022], acc is [0.90625]\n",
      "epoch: 0, batch: 200, loss is: [0.25103122], acc is [0.921875]\n",
      "epoch: 0, batch: 300, loss is: [0.10832155], acc is [0.953125]\n",
      "epoch: 0, batch: 400, loss is: [0.03853818], acc is [1.]\n",
      "epoch: 0, batch: 500, loss is: [0.06931215], acc is [0.984375]\n",
      "epoch: 0, batch: 600, loss is: [0.16624391], acc is [0.953125]\n",
      "epoch: 0, batch: 700, loss is: [0.23210882], acc is [0.9375]\n",
      "epoch: 0, batch: 800, loss is: [0.20683259], acc is [0.9375]\n",
      "epoch: 0, batch: 900, loss is: [0.14033127], acc is [0.96875]\n",
      "epoch: 1, batch: 0, loss is: [0.02932457], acc is [1.]\n",
      "epoch: 1, batch: 100, loss is: [0.0996362], acc is [0.984375]\n",
      "epoch: 1, batch: 200, loss is: [0.1458218], acc is [0.953125]\n",
      "epoch: 1, batch: 300, loss is: [0.05703544], acc is [0.96875]\n",
      "epoch: 1, batch: 400, loss is: [0.0476568], acc is [0.984375]\n",
      "epoch: 1, batch: 500, loss is: [0.02081344], acc is [1.]\n",
      "epoch: 1, batch: 600, loss is: [0.08612189], acc is [0.984375]\n",
      "epoch: 1, batch: 700, loss is: [0.05592047], acc is [0.984375]\n",
      "epoch: 1, batch: 800, loss is: [0.17104198], acc is [0.96875]\n",
      "epoch: 1, batch: 900, loss is: [0.12405417], acc is [0.96875]\n",
      "epoch: 2, batch: 0, loss is: [0.09145986], acc is [0.984375]\n",
      "epoch: 2, batch: 100, loss is: [0.02936876], acc is [0.984375]\n",
      "epoch: 2, batch: 200, loss is: [0.02366533], acc is [1.]\n",
      "epoch: 2, batch: 300, loss is: [0.02105165], acc is [1.]\n",
      "epoch: 2, batch: 400, loss is: [0.16597718], acc is [0.96875]\n",
      "epoch: 2, batch: 500, loss is: [0.06477633], acc is [0.96875]\n",
      "epoch: 2, batch: 600, loss is: [0.04478589], acc is [0.984375]\n",
      "epoch: 2, batch: 700, loss is: [0.00575728], acc is [1.]\n",
      "epoch: 2, batch: 800, loss is: [0.05320016], acc is [0.96875]\n",
      "epoch: 2, batch: 900, loss is: [0.01626693], acc is [1.]\n",
      "epoch: 3, batch: 0, loss is: [0.01308882], acc is [1.]\n",
      "epoch: 3, batch: 100, loss is: [0.04626837], acc is [0.96875]\n",
      "epoch: 3, batch: 200, loss is: [0.00607007], acc is [1.]\n",
      "epoch: 3, batch: 300, loss is: [0.0077588], acc is [1.]\n",
      "epoch: 3, batch: 400, loss is: [0.01246815], acc is [1.]\n",
      "epoch: 3, batch: 500, loss is: [0.0130862], acc is [1.]\n",
      "epoch: 3, batch: 600, loss is: [0.01365679], acc is [1.]\n",
      "epoch: 3, batch: 700, loss is: [0.03363513], acc is [0.984375]\n",
      "epoch: 3, batch: 800, loss is: [0.01511409], acc is [1.]\n",
      "epoch: 3, batch: 900, loss is: [0.07762549], acc is [0.96875]\n",
      "epoch: 4, batch: 0, loss is: [0.04688591], acc is [0.96875]\n",
      "epoch: 4, batch: 100, loss is: [0.0069871], acc is [1.]\n",
      "epoch: 4, batch: 200, loss is: [0.06367669], acc is [0.96875]\n",
      "epoch: 4, batch: 300, loss is: [0.0235067], acc is [0.984375]\n",
      "epoch: 4, batch: 400, loss is: [0.02423317], acc is [0.984375]\n",
      "epoch: 4, batch: 500, loss is: [0.02402187], acc is [0.984375]\n",
      "epoch: 4, batch: 600, loss is: [0.0138673], acc is [1.]\n",
      "epoch: 4, batch: 700, loss is: [0.09915644], acc is [0.96875]\n",
      "epoch: 4, batch: 800, loss is: [0.05140034], acc is [0.984375]\n",
      "epoch: 4, batch: 900, loss is: [0.00606005], acc is [1.]\n"
     ]
    }
   ],
   "source": [
    "model = MNIST_CONV_INIT()\n",
    "train_MNIST_L2(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程, 使用L1正则\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "def train_MNIST_L1(model):\n",
    "\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001, weight_decay=paddle.regularizer.L1Decay(coeff=1e-5),parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 5\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            images, labels = data \n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predicts, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "\n",
    "            # 记录损失\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc_of_train_L1', step = iter, value=acc.numpy() )\n",
    "                log_writer.add_scalar(tag = 'loss_of_train_L1', step = iter, value=avg_loss.numpy())\n",
    "                iter += 100\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels,model=)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            if batch_id % 100 == 0:\n",
    "                log_writer.add_scalar(tag = 'acc_of_test_L1', step=iter2, value=acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss_of_test_L1', step=iter2, value=avg_loss.numpy())\n",
    "                iter2 += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_L1 = MNIST_CONV_INIT()\n",
    "train_MNIST_L1(model_L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Place(gpu:0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义模型结构,dropout\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "# 多层卷积神经网络实现\n",
    "class MNIST_CONV_DROPOUT(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CONV_DROPOUT, self).__init__()\n",
    "\n",
    "        # 定义卷积层，输出特征通道20，卷积核大小5\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小为2，步长为2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义卷积层，输出特征20维，卷积核大小为5\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义一层全连接层，输入维度是10\n",
    "        self.fc = Linear(in_features=980, out_features=10)\n",
    "\n",
    "    # 定义前向，卷积后使用池化，然后全连接输出\n",
    "    # 激活函数使用ReLU，全连接层使用softmax\n",
    "    def forward(self, inputs, label,mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], 980])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # q全连接层加入一个dropout\n",
    "        x = F.dropout(x, p = 0.01,mode=mode)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label=label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "def train_MNIST_INIT(model):\n",
    "\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.003, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 5\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            images, labels = data \n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predicts, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "\n",
    "            # 记录损失\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc_of_train_dr1', step = iter, value=acc.numpy() )\n",
    "                log_writer.add_scalar(tag = 'loss_of_train_dr1', step = iter, value=avg_loss.numpy())\n",
    "                iter += 100\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels,mode='downscale_in_infer')\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            if batch_id % 100 == 0:\n",
    "                log_writer.add_scalar(tag = 'acc_of_test_dr1', step=iter2, value=acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss_of_test_dr1', step=iter2, value=avg_loss.numpy())\n",
    "                iter2 += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.805146], acc is [0.125]\n",
      "epoch: 0, batch: 100, loss is: [0.22334327], acc is [0.90625]\n",
      "epoch: 0, batch: 200, loss is: [0.15081255], acc is [0.984375]\n",
      "epoch: 0, batch: 300, loss is: [0.13811032], acc is [0.96875]\n",
      "epoch: 0, batch: 400, loss is: [0.15092635], acc is [0.9375]\n",
      "epoch: 0, batch: 500, loss is: [0.09098322], acc is [0.953125]\n",
      "epoch: 0, batch: 600, loss is: [0.03913482], acc is [0.984375]\n",
      "epoch: 0, batch: 700, loss is: [0.05470811], acc is [0.984375]\n",
      "epoch: 0, batch: 800, loss is: [0.05528992], acc is [0.984375]\n",
      "epoch: 0, batch: 900, loss is: [0.2042924], acc is [0.953125]\n",
      "epoch: 1, batch: 0, loss is: [0.21055363], acc is [0.96875]\n",
      "epoch: 1, batch: 100, loss is: [0.06998789], acc is [0.96875]\n",
      "epoch: 1, batch: 200, loss is: [0.05319173], acc is [0.96875]\n",
      "epoch: 1, batch: 300, loss is: [0.01125339], acc is [1.]\n",
      "epoch: 1, batch: 400, loss is: [0.01823329], acc is [1.]\n",
      "epoch: 1, batch: 500, loss is: [0.00940316], acc is [1.]\n",
      "epoch: 1, batch: 600, loss is: [0.02014703], acc is [1.]\n",
      "epoch: 1, batch: 700, loss is: [0.04214926], acc is [0.984375]\n",
      "epoch: 1, batch: 800, loss is: [0.00441248], acc is [1.]\n",
      "epoch: 1, batch: 900, loss is: [0.03861205], acc is [0.96875]\n",
      "epoch: 2, batch: 0, loss is: [0.00620005], acc is [1.]\n",
      "epoch: 2, batch: 100, loss is: [0.02032428], acc is [1.]\n",
      "epoch: 2, batch: 200, loss is: [0.00749056], acc is [1.]\n",
      "epoch: 2, batch: 300, loss is: [0.03488442], acc is [0.984375]\n",
      "epoch: 2, batch: 400, loss is: [0.0158557], acc is [1.]\n",
      "epoch: 2, batch: 500, loss is: [0.01381508], acc is [1.]\n",
      "epoch: 2, batch: 600, loss is: [0.00459255], acc is [1.]\n",
      "epoch: 2, batch: 700, loss is: [0.09056036], acc is [0.984375]\n",
      "epoch: 2, batch: 800, loss is: [0.00553731], acc is [1.]\n",
      "epoch: 2, batch: 900, loss is: [0.01495791], acc is [0.984375]\n",
      "epoch: 3, batch: 0, loss is: [0.00656867], acc is [1.]\n",
      "epoch: 3, batch: 100, loss is: [0.03645948], acc is [0.984375]\n",
      "epoch: 3, batch: 200, loss is: [0.05064623], acc is [0.96875]\n",
      "epoch: 3, batch: 300, loss is: [0.01374305], acc is [1.]\n",
      "epoch: 3, batch: 400, loss is: [0.02605268], acc is [0.984375]\n",
      "epoch: 3, batch: 500, loss is: [0.01348981], acc is [0.984375]\n",
      "epoch: 3, batch: 600, loss is: [0.00166255], acc is [1.]\n",
      "epoch: 3, batch: 700, loss is: [0.06533846], acc is [0.984375]\n",
      "epoch: 3, batch: 800, loss is: [0.03093417], acc is [0.984375]\n",
      "epoch: 3, batch: 900, loss is: [0.06877689], acc is [0.96875]\n",
      "epoch: 4, batch: 0, loss is: [0.01077393], acc is [1.]\n",
      "epoch: 4, batch: 100, loss is: [0.04602965], acc is [0.984375]\n",
      "epoch: 4, batch: 200, loss is: [0.00768775], acc is [1.]\n",
      "epoch: 4, batch: 300, loss is: [0.02064528], acc is [0.984375]\n",
      "epoch: 4, batch: 400, loss is: [0.00977738], acc is [1.]\n",
      "epoch: 4, batch: 500, loss is: [0.00976432], acc is [1.]\n",
      "epoch: 4, batch: 600, loss is: [0.09693098], acc is [0.984375]\n",
      "epoch: 4, batch: 700, loss is: [0.07043047], acc is [0.984375]\n",
      "epoch: 4, batch: 800, loss is: [0.0914141], acc is [0.984375]\n",
      "epoch: 4, batch: 900, loss is: [0.11484648], acc is [0.96875]\n"
     ]
    }
   ],
   "source": [
    "model_dropout = MNIST_CONV_DROPOUT()\n",
    "train_MNIST_INIT(model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现K折交叉验证，先定义两层卷积和一层全连接，增加一个dropout\n",
    "# 定义模型结构,dropout\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import Conv2D, MaxPool2D, Linear\n",
    "\n",
    "# 多层卷积神经网络实现\n",
    "class MNIST_CONV_DROPOUT(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MNIST_CONV_DROPOUT, self).__init__()\n",
    "\n",
    "        # 定义卷积层，输出特征通道20，卷积核大小5\n",
    "        self.conv1 = Conv2D(in_channels=1, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层，池化核的大小为2，步长为2\n",
    "        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义卷积层，输出特征20维，卷积核大小为5\n",
    "        self.conv2 = Conv2D(in_channels=20, out_channels=20, kernel_size=5, stride=1, padding=2)\n",
    "        # 定义池化层\n",
    "        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)\n",
    "        # 定义一层全连接层，输入维度是10\n",
    "        self.fc = Linear(in_features=980, out_features=10)\n",
    "\n",
    "    # 定义前向，卷积后使用池化，然后全连接输出\n",
    "    # 激活函数使用ReLU，全连接层使用softmax\n",
    "    def forward(self, inputs, label,mode='upscale_in_train'):\n",
    "        x = self.conv1(inputs)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = paddle.reshape(x, [x.shape[0], 980])\n",
    "        x = self.fc(x)\n",
    "\n",
    "        # q全连接层加入一个dropout\n",
    "        x = F.dropout(x, p = 0.01,mode=mode)\n",
    "        if label is not None:\n",
    "            acc = paddle.metric.accuracy(input=x, label=label)\n",
    "            return x, acc\n",
    "        else:\n",
    "            return x\n",
    "use_gpu = True\n",
    "paddle.set_device('gpu:0') if use_gpu else paddle.set_device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.vision.transforms import Normalize\n",
    "transform = Normalize(mean=[127.5], std=[127.5], data_format='chw')\n",
    "# 下载数据集并初始化 dataset\n",
    "train_dataset = paddle.vision.datasets.MNIST(mode='train', transform=transform)\n",
    "test_dataset = paddle.vision.datasets.MNIST(mode='test', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluation(model, datasets, mode='downscale_in_infer'):\n",
    "    model.eval()\n",
    "    acc_set = list()\n",
    "    losses = []\n",
    "    for batch_id, data in enumerate(datasets()):\n",
    "        images, labels = data\n",
    "        images = paddle.to_tensor(images)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        pred, ac = model(images,labels, mode=mode)   # 获取预测值\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        acc = paddle.metric.accuracy(input=pred, label=labels)\n",
    "        acc_set.extend(acc.numpy())\n",
    "        losses.append(avg_loss)\n",
    "    \n",
    "    # #计算多个batch的准确率\n",
    "    acc_val_mean = np.array(acc_set).mean()\n",
    "    loss_val_mean = np.array(losses).mean()\n",
    "    return acc_val_mean, loss_val_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Fold 1\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.09375], loss[3.7911503]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.9375], loss[0.1856969]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.9375], loss[0.17303059]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.875], loss[0.19910502]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.96875], loss[0.11269292]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.9375], loss[0.21490234]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.96875], loss[0.07525603]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.9375], loss[0.14679341]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.03188223]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.03552128]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.0676665]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.00960562]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.11558908]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.02667837]\n",
      "Training on Fold 2\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.15625], loss[4.144415]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.96875], loss[0.09646706]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[1.], loss[0.05808906]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[1.], loss[0.02000888]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.9375], loss[0.10921885]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.96875], loss[0.05819018]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[1.], loss[0.02246124]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.01297854]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.08291635]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.01394157]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.0031754]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.04572765]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.13903962]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.0734485]\n",
      "Training on Fold 3\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.15625], loss[2.4516332]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.90625], loss[0.23802575]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.8125], loss[0.42381555]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.875], loss[0.23182479]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[0.90625], loss[0.2566236]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[1.], loss[0.02252704]\n",
      "lr:0.001, bitch_size:32, epoch:0, acc[1.], loss[0.02725879]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.0346448]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.9375], loss[0.23996715]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.0164551]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.08637176]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.00410406]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[0.96875], loss[0.08897164]\n",
      "lr:0.001, bitch_size:32, epoch:1, acc[1.], loss[0.01452207]\n",
      "Training on Fold 1\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.109375], loss[3.287917]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.921875], loss[0.2165412]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.96875], loss[0.1158853]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.953125], loss[0.1327185]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.96875], loss[0.12835877]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.96875], loss[0.08716419]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[1.], loss[0.01431894]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[1.], loss[0.01182641]\n",
      "Training on Fold 2\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.078125], loss[5.3238287]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.953125], loss[0.15699732]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.96875], loss[0.15598847]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.96875], loss[0.05267037]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.984375], loss[0.05168742]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.96875], loss[0.07765286]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.984375], loss[0.04202588]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.96875], loss[0.0600796]\n",
      "Training on Fold 3\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.046875], loss[4.0151944]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.9375], loss[0.23196349]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.96875], loss[0.12743059]\n",
      "lr:0.001, bitch_size:64, epoch:0, acc[0.984375], loss[0.15874177]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.9375], loss[0.16872486]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.984375], loss[0.03846832]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.953125], loss[0.0762329]\n",
      "lr:0.001, bitch_size:64, epoch:1, acc[0.984375], loss[0.04102433]\n",
      "Training on Fold 1\n",
      "lr:0.001, bitch_size:128, epoch:0, acc[0.1484375], loss[4.9888124]\n",
      "lr:0.001, bitch_size:128, epoch:0, acc[0.9375], loss[0.23021449]\n",
      "lr:0.001, bitch_size:128, epoch:1, acc[0.96875], loss[0.11226056]\n",
      "lr:0.001, bitch_size:128, epoch:1, acc[0.9765625], loss[0.07976095]\n",
      "Training on Fold 2\n",
      "lr:0.001, bitch_size:128, epoch:0, acc[0.0703125], loss[3.2999077]\n",
      "lr:0.001, bitch_size:128, epoch:0, acc[0.96875], loss[0.17767596]\n",
      "lr:0.001, bitch_size:128, epoch:1, acc[0.9765625], loss[0.04837899]\n",
      "lr:0.001, bitch_size:128, epoch:1, acc[0.953125], loss[0.14439124]\n",
      "Training on Fold 3\n",
      "lr:0.001, bitch_size:128, epoch:0, acc[0.078125], loss[2.9122632]\n",
      "lr:0.001, bitch_size:128, epoch:0, acc[0.8984375], loss[0.2764189]\n",
      "lr:0.001, bitch_size:128, epoch:1, acc[0.9609375], loss[0.10638726]\n",
      "lr:0.001, bitch_size:128, epoch:1, acc[0.984375], loss[0.0343812]\n",
      "Training on Fold 1\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.125], loss[3.9877162]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.90625], loss[0.28058106]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.19610688]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.84375], loss[0.25592694]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.0395894]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.1748566]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[1.], loss[0.03129618]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.20944105]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[1.], loss[0.02899444]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.2406868]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.39174825]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.09613036]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[1.], loss[0.03629274]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.96875], loss[0.10051411]\n",
      "Training on Fold 2\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.09375], loss[4.6969914]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.9375], loss[0.1421602]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.1238651]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.9375], loss[0.12148949]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.06104121]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.05671687]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[1.], loss[0.0083442]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.875], loss[0.38839376]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.90625], loss[0.1843583]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.96875], loss[0.18834542]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[1.], loss[0.01501317]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[1.], loss[0.03203629]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.20479889]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.96875], loss[0.03663971]\n",
      "Training on Fold 3\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.09375], loss[3.933871]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[1.], loss[0.06475574]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[1.], loss[0.0495291]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.1366521]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[1.], loss[0.03854729]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.04726939]\n",
      "lr:0.005, bitch_size:32, epoch:0, acc[0.96875], loss[0.07188705]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[1.], loss[0.00645128]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.21509057]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.9375], loss[0.22251767]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[1.], loss[0.00331021]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.96875], loss[0.06392235]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.96875], loss[0.04866969]\n",
      "lr:0.005, bitch_size:32, epoch:1, acc[0.96875], loss[0.09150115]\n",
      "Training on Fold 1\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.125], loss[3.1389003]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.921875], loss[0.10545295]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.96875], loss[0.07710783]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.9375], loss[0.1259982]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[1.], loss[0.00562193]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.96875], loss[0.08232815]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.984375], loss[0.12885265]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.96875], loss[0.1222252]\n",
      "Training on Fold 2\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.171875], loss[2.6910696]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.9375], loss[0.12071186]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.953125], loss[0.1349298]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.953125], loss[0.05485003]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.96875], loss[0.06753891]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[1.], loss[0.00731546]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.96875], loss[0.07308713]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.984375], loss[0.03827632]\n",
      "Training on Fold 3\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.078125], loss[3.6980145]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.96875], loss[0.08831257]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.9375], loss[0.13551232]\n",
      "lr:0.005, bitch_size:64, epoch:0, acc[0.984375], loss[0.05269286]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.984375], loss[0.0299473]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.921875], loss[0.22727087]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.96875], loss[0.15172294]\n",
      "lr:0.005, bitch_size:64, epoch:1, acc[0.984375], loss[0.10616914]\n",
      "Training on Fold 1\n",
      "lr:0.005, bitch_size:128, epoch:0, acc[0.109375], loss[3.0087762]\n",
      "lr:0.005, bitch_size:128, epoch:0, acc[0.9140625], loss[0.23831318]\n",
      "lr:0.005, bitch_size:128, epoch:1, acc[0.984375], loss[0.04776246]\n",
      "lr:0.005, bitch_size:128, epoch:1, acc[0.9765625], loss[0.07739695]\n",
      "Training on Fold 2\n",
      "lr:0.005, bitch_size:128, epoch:0, acc[0.09375], loss[3.2713828]\n",
      "lr:0.005, bitch_size:128, epoch:0, acc[0.984375], loss[0.07629784]\n",
      "lr:0.005, bitch_size:128, epoch:1, acc[0.984375], loss[0.05128175]\n",
      "lr:0.005, bitch_size:128, epoch:1, acc[0.9765625], loss[0.08589598]\n",
      "Training on Fold 3\n",
      "lr:0.005, bitch_size:128, epoch:0, acc[0.1015625], loss[3.4097068]\n",
      "lr:0.005, bitch_size:128, epoch:0, acc[0.9140625], loss[0.24875943]\n",
      "lr:0.005, bitch_size:128, epoch:1, acc[0.953125], loss[0.1025569]\n",
      "lr:0.005, bitch_size:128, epoch:1, acc[0.984375], loss[0.08197661]\n",
      "Training on Fold 1\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.0625], loss[3.9396355]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.875], loss[0.45896265]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.04678633]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.43744835]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.9375], loss[0.24370562]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.90625], loss[0.38342416]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.07064622]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.9375], loss[0.16619612]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.9375], loss[0.11996725]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.90625], loss[0.2519084]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.9375], loss[0.10515106]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.06034405]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.875], loss[0.4157706]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.08634951]\n",
      "Training on Fold 2\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.], loss[4.7291574]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.8125], loss[0.6766608]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.17600474]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.18364844]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.05209562]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[1.], loss[0.03424204]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.22129875]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.13245776]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[1.], loss[0.03993276]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.06622328]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[1.], loss[0.00793047]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[1.], loss[0.00698083]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[1.], loss[0.02602807]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.16019224]\n",
      "Training on Fold 3\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.09375], loss[2.6168883]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.84375], loss[0.3018577]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.90625], loss[0.2631352]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.90625], loss[0.14686362]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.875], loss[0.26925373]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.9375], loss[0.3304063]\n",
      "lr:0.009, bitch_size:32, epoch:0, acc[0.96875], loss[0.23072249]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.90625], loss[0.30739266]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[1.], loss[0.0169508]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.13120703]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.875], loss[0.5788262]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.0799529]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.96875], loss[0.09045568]\n",
      "lr:0.009, bitch_size:32, epoch:1, acc[0.9375], loss[0.16810921]\n",
      "Training on Fold 1\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.140625], loss[3.4741073]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.984375], loss[0.06559976]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.96875], loss[0.09475672]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.953125], loss[0.08843976]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.921875], loss[0.2627757]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.96875], loss[0.12382283]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.984375], loss[0.05329992]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.96875], loss[0.14760643]\n",
      "Training on Fold 2\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.078125], loss[3.0726871]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.96875], loss[0.10162184]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.984375], loss[0.03836954]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.9375], loss[0.0956965]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[1.], loss[0.02031917]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.96875], loss[0.11280469]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.953125], loss[0.08427899]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.96875], loss[0.05704658]\n",
      "Training on Fold 3\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.09375], loss[3.9468465]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.875], loss[0.34450477]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.90625], loss[0.2298887]\n",
      "lr:0.009, bitch_size:64, epoch:0, acc[0.921875], loss[0.17063984]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.875], loss[0.3508625]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.90625], loss[0.24152595]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.953125], loss[0.09998213]\n",
      "lr:0.009, bitch_size:64, epoch:1, acc[0.953125], loss[0.11847135]\n",
      "Training on Fold 1\n",
      "lr:0.009, bitch_size:128, epoch:0, acc[0.09375], loss[3.4047458]\n",
      "lr:0.009, bitch_size:128, epoch:0, acc[0.9921875], loss[0.05317694]\n",
      "lr:0.009, bitch_size:128, epoch:1, acc[0.9765625], loss[0.05801705]\n",
      "lr:0.009, bitch_size:128, epoch:1, acc[0.9609375], loss[0.16580634]\n",
      "Training on Fold 2\n",
      "lr:0.009, bitch_size:128, epoch:0, acc[0.0703125], loss[3.0521736]\n",
      "lr:0.009, bitch_size:128, epoch:0, acc[1.], loss[0.029027]\n",
      "lr:0.009, bitch_size:128, epoch:1, acc[0.96875], loss[0.07677697]\n",
      "lr:0.009, bitch_size:128, epoch:1, acc[1.], loss[0.01684617]\n",
      "Training on Fold 3\n",
      "lr:0.009, bitch_size:128, epoch:0, acc[0.15625], loss[3.6805859]\n",
      "lr:0.009, bitch_size:128, epoch:0, acc[0.9453125], loss[0.161291]\n",
      "lr:0.009, bitch_size:128, epoch:1, acc[0.9609375], loss[0.12649697]\n",
      "lr:0.009, bitch_size:128, epoch:1, acc[0.9765625], loss[0.09233037]\n",
      "Training on Fold 1\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.125], loss[3.5580676]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.90625], loss[0.5963383]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[1.], loss[0.07507364]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.9375], loss[0.29423052]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.96875], loss[0.07746968]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.90625], loss[0.31846434]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.96875], loss[0.11816013]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.90625], loss[0.5816526]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.96875], loss[0.15071636]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[1.], loss[0.00527957]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[1.], loss[0.03418407]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.96875], loss[0.05613206]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.9375], loss[0.24811818]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.9375], loss[0.17002258]\n",
      "Training on Fold 2\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.125], loss[3.5974946]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.90625], loss[0.9677608]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.90625], loss[0.35523948]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.9375], loss[0.06519218]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[1.], loss[0.02265842]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.96875], loss[0.18964152]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.9375], loss[0.17962866]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.96875], loss[0.04825538]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.9375], loss[0.17717803]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.96875], loss[0.17598797]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.9375], loss[0.16144401]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.9375], loss[0.17003387]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[1.], loss[0.01338044]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.9375], loss[0.0928202]\n",
      "Training on Fold 3\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.15625], loss[3.8097887]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.03125], loss[2.3255944]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.09375], loss[2.3032732]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.03125], loss[2.3118007]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.03125], loss[2.2914307]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.09375], loss[2.3114178]\n",
      "lr:0.012, bitch_size:32, epoch:0, acc[0.28125], loss[2.2784228]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.15625], loss[2.3014684]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.0625], loss[2.321056]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.125], loss[2.3096662]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.09375], loss[2.2925138]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.09375], loss[2.3054087]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.03125], loss[2.3221278]\n",
      "lr:0.012, bitch_size:32, epoch:1, acc[0.1875], loss[2.2859476]\n",
      "Training on Fold 1\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.03125], loss[3.9009063]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.15625], loss[2.2972744]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.125], loss[2.299913]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.21875], loss[2.2871737]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.1875], loss[2.2910662]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.0625], loss[2.3005586]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.125], loss[2.2882662]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.140625], loss[2.2941842]\n",
      "Training on Fold 2\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.125], loss[3.6438816]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.875], loss[0.43738022]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.9375], loss[0.26294288]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.984375], loss[0.12985654]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[1.], loss[0.03303839]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.921875], loss[0.19550672]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.953125], loss[0.13419972]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.96875], loss[0.07568499]\n",
      "Training on Fold 3\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.046875], loss[3.1574602]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.15625], loss[2.3046343]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.15625], loss[2.2936711]\n",
      "lr:0.012, bitch_size:64, epoch:0, acc[0.09375], loss[2.292342]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.078125], loss[2.2996173]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.046875], loss[2.3204558]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.140625], loss[2.305522]\n",
      "lr:0.012, bitch_size:64, epoch:1, acc[0.15625], loss[2.2961]\n",
      "Training on Fold 1\n",
      "lr:0.012, bitch_size:128, epoch:0, acc[0.125], loss[3.0300467]\n",
      "lr:0.012, bitch_size:128, epoch:0, acc[0.9765625], loss[0.07923617]\n",
      "lr:0.012, bitch_size:128, epoch:1, acc[0.9609375], loss[0.13280985]\n",
      "lr:0.012, bitch_size:128, epoch:1, acc[0.96875], loss[0.14981572]\n",
      "Training on Fold 2\n",
      "lr:0.012, bitch_size:128, epoch:0, acc[0.09375], loss[5.3462315]\n",
      "lr:0.012, bitch_size:128, epoch:0, acc[0.9453125], loss[0.1799517]\n",
      "lr:0.012, bitch_size:128, epoch:1, acc[0.9609375], loss[0.09059927]\n",
      "lr:0.012, bitch_size:128, epoch:1, acc[0.984375], loss[0.08417316]\n",
      "Training on Fold 3\n",
      "lr:0.012, bitch_size:128, epoch:0, acc[0.0859375], loss[2.890294]\n",
      "lr:0.012, bitch_size:128, epoch:0, acc[0.9453125], loss[0.19674864]\n",
      "lr:0.012, bitch_size:128, epoch:1, acc[0.9375], loss[0.15775418]\n",
      "lr:0.012, bitch_size:128, epoch:1, acc[0.9609375], loss[0.16582532]\n"
     ]
    }
   ],
   "source": [
    "from paddle.io import Subset\n",
    "from visualdl import LogWriter\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 定义要调整的超参数范围,我这里要搜索优化算法，batch大小\n",
    "lr0 = 0\n",
    "learning_rates = [0.001, 0.005, 0.009, 0.012]\n",
    "batch_size = [32, 64, 128]\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# 存储每个超参数组合的性能结果\n",
    "results = []\n",
    "\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for b_s in batch_size:\n",
    "        for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset), 1):\n",
    "            print(f\"Training on Fold {fold}\")\n",
    "            train_data = Subset(train_dataset, train_indices)\n",
    "            val_data = Subset(train_dataset, val_indices)\n",
    "\n",
    "            model = MNIST_CONV_DROPOUT()  # 创建模型\n",
    "            opt = paddle.optimizer.Adam(learning_rate=lr, parameters=model.parameters())\n",
    "            \n",
    "            train_loader = paddle.io.DataLoader(train_data, batch_size=b_s, shuffle=True, num_workers=1, drop_last=True)\n",
    "            EPOCH_NUM = 2\n",
    "            for epoch in range(EPOCH_NUM):\n",
    "                for batch_id, data in enumerate(train_loader()):\n",
    "                    # 训练模型\n",
    "                    images, labels = data \n",
    "                    images = paddle.to_tensor(images)\n",
    "                    labels = paddle.to_tensor(labels)\n",
    "                    \n",
    "                    # 前向传播\n",
    "                    predicts, ac = model(images, labels)\n",
    "                    loss = F.cross_entropy(predicts, labels)\n",
    "                    avg_loss = paddle.mean(loss)\n",
    "                    \n",
    "                    if batch_id % 200 ==0:\n",
    "                        print(f'lr:{lr}, bitch_size:{b_s}, epoch:{epoch}, acc{ac.numpy()}, loss{avg_loss.numpy()}')\n",
    "                    # 反向传播\n",
    "                    avg_loss.backward()\n",
    "                    opt.step()\n",
    "                    opt.clear_grad()\n",
    "\n",
    "                # 在验证集上评估性能\n",
    "                val_loader = paddle.io.DataLoader(val_data, batch_size=b_s, shuffle=False, num_workers=1, drop_last=True)\n",
    "                val_acc, val_loss = evaluation(model, val_loader)  # 自定义evaluate函数\n",
    "                log_writer.add_scalar(tag='acc_of_val_1', step=(fold - 1) * EPOCH_NUM + epoch, value=val_acc)\n",
    "                log_writer.add_scalar(tag='loss_of_val_1', step=(fold - 1) * EPOCH_NUM + epoch, value=val_loss)\n",
    "            \n",
    "            results.append((lr, b_s, val_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Learning Rate: 0.005\n",
      "Best batch size128\n",
      "Best Validation Accuracy: 0.9772469202677408\n"
     ]
    }
   ],
   "source": [
    "# 计算每个超参数组合的平均验证准确度\n",
    "average_results = {}\n",
    "for lr, b_s, _ in results:\n",
    "    avg_val_acc = sum(val_acc for lr_, b_s_, val_acc in results if lr_ == lr and b_s_ == b_s) / 3\n",
    "    average_results[(lr, b_s)] = avg_val_acc\n",
    "\n",
    "# 选择具有最佳平均性能的超参数\n",
    "best_lr, best_b_s = max(average_results, key=average_results.get)\n",
    "best_val_acc = average_results[(best_lr, best_b_s)]\n",
    "\n",
    "print(f\"Best Learning Rate: {best_lr}\")\n",
    "print(f'Best batch size{best_b_s}')\n",
    "print(f\"Best Validation Accuracy: {best_val_acc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最佳超参数进行最终训练\n",
    "# 定义训练过程\n",
    "from visualdl import LogWriter\n",
    "log_writer = LogWriter(logdir=\"./log\")\n",
    "def train_MNIST_INIT_k(model):\n",
    "\n",
    "    # 定义优化器\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.005, parameters=model.parameters())\n",
    "\n",
    "    # 训练参数\n",
    "    EPOCH_NUM = 5\n",
    "    iter = 0\n",
    "    iter2 = 0\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            model.train()\n",
    "            images, labels = data \n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 前向\n",
    "            predicts, acc = model(images, labels)\n",
    "\n",
    "            # 损失\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            \n",
    "\n",
    "            # 记录损失\n",
    "            if batch_id % 100 == 0:\n",
    "                print(\"epoch: {}, batch: {}, loss is: {}, acc is {}\".format(epoch_id, batch_id, avg_loss.numpy(), acc.numpy()))\n",
    "                log_writer.add_scalar(tag = 'acc_of_train_k', step = iter, value=acc.numpy() )\n",
    "                log_writer.add_scalar(tag = 'loss_of_train_k', step = iter, value=avg_loss.numpy())\n",
    "                iter += 100\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader()):\n",
    "            model.eval()\n",
    "            images, labels = data\n",
    "            images = paddle.to_tensor(images)\n",
    "            labels = paddle.to_tensor(labels)\n",
    "\n",
    "            # 预测\n",
    "            predicts, acc = model(images, labels)\n",
    "            loss = F.cross_entropy(predicts, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            if batch_id % 100 == 0:\n",
    "                log_writer.add_scalar(tag = 'acc_of_test_k', step=iter2, value=acc.numpy())\n",
    "                log_writer.add_scalar(tag = 'loss_of_test_k', step=iter2, value=avg_loss.numpy())\n",
    "                iter2 += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch: 0, loss is: [3.07781], acc is [0.1484375]\n",
      "epoch: 0, batch: 100, loss is: [0.36147293], acc is [0.890625]\n",
      "epoch: 0, batch: 200, loss is: [0.24685222], acc is [0.9296875]\n",
      "epoch: 0, batch: 300, loss is: [0.31525332], acc is [0.9140625]\n",
      "epoch: 0, batch: 400, loss is: [0.12449379], acc is [0.9453125]\n",
      "epoch: 1, batch: 0, loss is: [0.1812364], acc is [0.9296875]\n",
      "epoch: 1, batch: 100, loss is: [0.21236974], acc is [0.9140625]\n",
      "epoch: 1, batch: 200, loss is: [0.11446952], acc is [0.96875]\n",
      "epoch: 1, batch: 300, loss is: [0.13346854], acc is [0.9609375]\n",
      "epoch: 1, batch: 400, loss is: [0.09755027], acc is [0.9765625]\n",
      "epoch: 2, batch: 0, loss is: [0.15984453], acc is [0.953125]\n",
      "epoch: 2, batch: 100, loss is: [0.13029255], acc is [0.9609375]\n",
      "epoch: 2, batch: 200, loss is: [0.09265825], acc is [0.9765625]\n",
      "epoch: 2, batch: 300, loss is: [0.04480406], acc is [0.984375]\n",
      "epoch: 2, batch: 400, loss is: [0.13464305], acc is [0.96875]\n",
      "epoch: 3, batch: 0, loss is: [0.03072544], acc is [1.]\n",
      "epoch: 3, batch: 100, loss is: [0.11173224], acc is [0.9609375]\n",
      "epoch: 3, batch: 200, loss is: [0.21028729], acc is [0.9609375]\n",
      "epoch: 3, batch: 300, loss is: [0.02152938], acc is [1.]\n",
      "epoch: 3, batch: 400, loss is: [0.03700732], acc is [0.9765625]\n",
      "epoch: 4, batch: 0, loss is: [0.03136224], acc is [0.984375]\n",
      "epoch: 4, batch: 100, loss is: [0.03961962], acc is [0.9921875]\n",
      "epoch: 4, batch: 200, loss is: [0.05634446], acc is [0.96875]\n",
      "epoch: 4, batch: 300, loss is: [0.19478501], acc is [0.9609375]\n",
      "epoch: 4, batch: 400, loss is: [0.05708655], acc is [0.96875]\n"
     ]
    }
   ],
   "source": [
    "model_k = MNIST_CONV_DROPOUT()\n",
    "train_MNIST_INIT_k(model_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
