{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import paddle\n",
    "from paddle import nn\n",
    "from paddle.nn import functional as F\n",
    "import d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': 1, 'y': 2, 'N': 3, 'V': 4, 'p': 5, 'C': 6, 'z': 7, 'T': 8, 'm': 9, 'A': 10, 'd': 11, 'f': 12, '$': 13, 'k': 14, 'P': 15, 'b': 16, 'i': 17, 'O': 18, 'U': 19, 's': 20, 'u': 21, 'o': 22, '^': 23, 'F': 24, 'D': 25, 'R': 26, 'x': 27, 'c': 28, 'e': 29, 'g': 30, 'H': 31, 'Y': 32, 'Z': 33, 'B': 34, 'J': 35, 'a': 36, 'M': 37, 'I': 38, ' ': 39, 'j': 40, 'E': 41, 'W': 42, 'G': 43, 'v': 44, 't': 45, 'r': 46, '-': 47, 'K': 48, \"'\": 49, 'S': 50, 'L': 51, 'X': 52, 'l': 53, 'Q': 54, 'n': 55, 'h': 56, 'q': 57}\n"
     ]
    }
   ],
   "source": [
    "# 定义起始和结束标识符\n",
    "START_TOKEN = '^'\n",
    "END_TOKEN = '$'\n",
    "\n",
    "# 读取原始文件\n",
    "file_path = 'male.txt'  # 替换为你的文件路径\n",
    "with open(file_path, 'r') as file:\n",
    "    names = file.readlines()\n",
    "\n",
    "# 清理每个名字，并添加起始和结束标识符\n",
    "processed_names = [f\"{START_TOKEN}{name.strip()}{END_TOKEN}\" for name in names]\n",
    "\n",
    "# 获取最长的名字长度\n",
    "max_length = max(len(name) for name in processed_names)\n",
    "\n",
    "# 创建字符到索引的映射\n",
    "all_chars = set(''.join(processed_names))\n",
    "char_to_index = {char: idx + 1 for idx, char in enumerate(all_chars)}\n",
    "index_to_char = {idx: char for char, idx in char_to_index.items()}\n",
    "print(char_to_index)\n",
    "# 将字符编码并保存为新的.txt文件，用0进行填充以保持相同长度\n",
    "processed_file_path = 'processed_names.txt'  # 保存处理后的文件路径\n",
    "with open(processed_file_path, 'w') as processed_file:\n",
    "    for name in processed_names:\n",
    "        encoded_name = [char_to_index[char] for char in name]\n",
    "        # 使用0填充使每个编码后的名字长度相同\n",
    "        encoded_name += [0] * (max_length - len(encoded_name))\n",
    "        encoded_name = ' '.join(map(str, encoded_name))  # 用空格分隔编码以便保存\n",
    "        processed_file.write(encoded_name + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23 10 36 ...  0  0  0]\n",
      " [23 10 36 ...  0  0  0]\n",
      " [23 10 16 ...  0  0  0]\n",
      " ...\n",
      " [23 48 36 ...  0  0  0]\n",
      " [23 43 17 ...  0  0  0]\n",
      " [23 43 29 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "file_path = 'processed_names.txt'  # 修改为你的文件路径\n",
    "with open(file_path, 'r') as file:\n",
    "    encoded_names = file.readlines()\n",
    "\n",
    "# 转换为列表，每个元素是一个编码后的名字的列表\n",
    "encoded_names = [\n",
    "    list(map(int, name.strip().split())) for name in encoded_names\n",
    "]\n",
    "encoded_names = np.array(encoded_names)\n",
    "print(encoded_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[29696], dtype=float32, place=Place(gpu:0), stop_gradient=True,\n",
      "       [0., 0., 0., ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "import paddle\n",
    "from paddle.io import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 这里是你读取已编码名字的代码，确保已正确读取到 encoded_names\n",
    "\n",
    "# 划分数据集：80% 训练集，10% 验证集，10% 测试集\n",
    "train_names, val_test_names = train_test_split(encoded_names, test_size=0.2, random_state=42)\n",
    "val_names, test_names = train_test_split(val_test_names, test_size=0.5, random_state=42)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 32  # 设置 mini-batch 大小\n",
    "\n",
    "\n",
    "# 创建 DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_names,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    "\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_names,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    "\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_names,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "# 打印输出一个mini-batch的数据\n",
    "for idx, item in enumerate(train_loader()):\n",
    "    X = item[:,:-1]\n",
    "    Y = item[:,1:]\n",
    "    Y = F.one_hot(Y,len(char_to_index)+1)\n",
    "    Y = paddle.reshape(Y, shape=[-1])\n",
    "    print(Y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "from paddle.nn import Linear\n",
    "\n",
    "\n",
    "class RNN_paddle(paddle.nn.Layer): \n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens):\n",
    "        super(RNN_paddle, self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.vocab_size = vocab_size  # 修改变量名为vocab_size\n",
    "        self.i2h = paddle.nn.Linear(vocab_size, num_hiddens)\n",
    "        self.h2h = paddle.nn.Linear(num_hiddens, num_hiddens)\n",
    "        self.h2o = paddle.nn.Linear(num_hiddens, vocab_size)\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # 先进行one-hot\n",
    "        X = paddle.nn.functional.one_hot(inputs.T, self.vocab_size)  # 将X修改为inputs\n",
    "        H, = state\n",
    "        outputs = []\n",
    "        for X in X:  # 修改变量名X为inputs\n",
    "            H = paddle.tanh(self.i2h(X) + self.h2h(H))\n",
    "            Y = self.h2o(H)\n",
    "            outputs.append(Y)\n",
    "        return paddle.concat(x=outputs, axis=0), (H,)\n",
    "\n",
    "    def begin_state(self, batch_size):\n",
    "        return (paddle.zeros(shape=[batch_size, self.num_hiddens]), )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(shape=[2, 5], dtype=int64, place=Place(gpu:0), stop_gradient=True,\n",
      "       [[0, 1, 2, 3, 4],\n",
      "        [5, 6, 7, 8, 9]])\n",
      "Tensor(shape=[10, 58], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.05397610,  0.01679978, -0.01953557, -0.02176125,  0.02758280,\n",
      "          0.02053785, -0.04334676, -0.05973249,  0.02747209,  0.03925230,\n",
      "          0.12498002, -0.00884516, -0.05159238, -0.11097369,  0.08773311,\n",
      "         -0.14795548, -0.16523133, -0.07006428, -0.00715754, -0.07530754,\n",
      "          0.06586301,  0.14571318, -0.05036495, -0.14330414, -0.09676053,\n",
      "          0.04767501,  0.01844257, -0.11115052,  0.17007712, -0.00721151,\n",
      "         -0.02323144, -0.05224719, -0.16599725,  0.02449747, -0.10089554,\n",
      "         -0.18132968,  0.15819982, -0.03775397,  0.12820339, -0.10379504,\n",
      "         -0.02361858, -0.08919550,  0.16424690, -0.02595349, -0.10125621,\n",
      "         -0.08449287,  0.02723853, -0.03749647, -0.01434819, -0.23135513,\n",
      "         -0.04760276, -0.04856937, -0.16341479, -0.04339372,  0.02708528,\n",
      "         -0.01137551, -0.06248784, -0.02114241],\n",
      "        [ 0.02004546,  0.05345334,  0.04036532, -0.00671731,  0.17574807,\n",
      "          0.00325338,  0.11740179,  0.11464169, -0.03651652, -0.03077777,\n",
      "          0.01561288, -0.17898810, -0.16754331,  0.15887706, -0.22000362,\n",
      "          0.08795999,  0.04652147,  0.14954782,  0.32556266, -0.02484883,\n",
      "         -0.15533419, -0.18945353,  0.02918599,  0.12971878, -0.18039288,\n",
      "          0.06915884,  0.10387434, -0.05054969,  0.22692966, -0.07476549,\n",
      "         -0.00828633, -0.09719489, -0.09501776,  0.01828277, -0.03031575,\n",
      "          0.00001585,  0.13124591, -0.12883270,  0.10019688,  0.05926681,\n",
      "          0.00309519,  0.07559086, -0.00509826, -0.10484949, -0.19984958,\n",
      "         -0.00775476,  0.29956517, -0.23229638,  0.28184009, -0.01779974,\n",
      "          0.25849250, -0.05152458, -0.26097497,  0.21176821,  0.09968255,\n",
      "          0.03976011, -0.16982216,  0.13289171],\n",
      "        [-0.10809274, -0.17208257,  0.00247537, -0.03843910, -0.22760563,\n",
      "          0.02194843,  0.08428388, -0.13396889, -0.13269809, -0.00968547,\n",
      "         -0.27330038, -0.03583077, -0.06963064, -0.11972796, -0.00010367,\n",
      "          0.44109407,  0.12329488, -0.11324912, -0.40312091,  0.12657708,\n",
      "         -0.28188807, -0.02239972, -0.03169247, -0.17808917,  0.43467984,\n",
      "         -0.14493506,  0.12813130,  0.12417948,  0.12301702, -0.14140984,\n",
      "          0.44367552, -0.05338088,  0.35605994,  0.03833275,  0.06134173,\n",
      "         -0.11524293, -0.17166227,  0.04385631, -0.13785458, -0.15885136,\n",
      "          0.12832047,  0.22603692, -0.14547656, -0.44889629,  0.19147795,\n",
      "          0.10235606,  0.08654098,  0.28319901, -0.16871114,  0.16352838,\n",
      "         -0.06465928,  0.11822726,  0.19992852,  0.05582442,  0.13530496,\n",
      "         -0.07415311,  0.12736359,  0.07824221],\n",
      "        [-0.14931212,  0.20392308, -0.00949084, -0.00808470, -0.21791562,\n",
      "          0.01790988, -0.07075348, -0.23561667,  0.01028296,  0.19824883,\n",
      "         -0.17703307,  0.08344336, -0.04902837,  0.11022089, -0.11015709,\n",
      "          0.11209098, -0.10281032,  0.00905051,  0.03400525,  0.08660787,\n",
      "          0.07348143,  0.16356787, -0.10928091, -0.00146829, -0.01556032,\n",
      "          0.06535038,  0.21111828, -0.06922138,  0.14738367,  0.20464611,\n",
      "          0.08149460, -0.07467467,  0.08772157, -0.10386527,  0.07618549,\n",
      "         -0.01727818, -0.09584355,  0.21439604,  0.10132071, -0.04806940,\n",
      "         -0.05914422, -0.00476363, -0.01811503, -0.22210842, -0.11180481,\n",
      "          0.05740298,  0.01173751,  0.24024488,  0.11561794, -0.15992206,\n",
      "          0.45490295, -0.13511683,  0.11835209,  0.04879314,  0.04035458,\n",
      "         -0.10129310, -0.05000202, -0.06903522],\n",
      "        [-0.00283043, -0.06336117,  0.00160978, -0.02433442,  0.00254956,\n",
      "         -0.21607377, -0.44655383, -0.01176202, -0.01248627,  0.20217644,\n",
      "         -0.02670596,  0.03273805, -0.15063792, -0.12972772, -0.09389116,\n",
      "          0.03035456,  0.28357840, -0.03578245,  0.20306216,  0.01631162,\n",
      "         -0.09648938,  0.00012571, -0.09364901, -0.08049855,  0.30074644,\n",
      "          0.09457120, -0.11963955,  0.32001278, -0.07268823, -0.27033013,\n",
      "         -0.14740750, -0.24434388,  0.33715242, -0.17850019, -0.02795089,\n",
      "          0.02569995,  0.17381527, -0.39695534, -0.14903130,  0.09055240,\n",
      "         -0.12233443,  0.04711019,  0.06285668,  0.13416666, -0.04600657,\n",
      "          0.12483410,  0.21484277, -0.11660392,  0.17036597,  0.27410820,\n",
      "         -0.01180393,  0.18277052,  0.05321476,  0.15532285,  0.28349972,\n",
      "         -0.06790998,  0.11549105,  0.09644578],\n",
      "        [ 0.09848370, -0.22138762,  0.12280958,  0.06391413, -0.10151287,\n",
      "         -0.17735568, -0.21317023, -0.05394203, -0.21811730, -0.16433683,\n",
      "          0.13264874, -0.15872502, -0.33835462, -0.11885870, -0.04968595,\n",
      "          0.27728450,  0.28376266,  0.07780384,  0.38892314,  0.01359332,\n",
      "         -0.11116846,  0.11198898,  0.34348634,  0.21659026,  0.26275569,\n",
      "         -0.07543592,  0.06340133,  0.06006791,  0.16311952, -0.02575700,\n",
      "         -0.37629101, -0.16122435,  0.07499879,  0.00229588, -0.03442204,\n",
      "          0.06387779, -0.11776499,  0.04330237, -0.02726838, -0.08677321,\n",
      "          0.13618371,  0.01151091, -0.17315036, -0.15115461, -0.07808302,\n",
      "          0.14222707, -0.15772310, -0.05029814,  0.19500041,  0.37988433,\n",
      "         -0.02344450,  0.02498615, -0.15906203, -0.13351531,  0.29252076,\n",
      "         -0.20783687, -0.02920349, -0.17117208],\n",
      "        [-0.07513461, -0.09604957, -0.19003357,  0.07110831, -0.43162772,\n",
      "         -0.21122365,  0.43891409,  0.34625822, -0.27221474,  0.28457630,\n",
      "         -0.24313247, -0.05823950, -0.06429431,  0.30327839,  0.22155769,\n",
      "          0.23169656, -0.25213066, -0.37144375, -0.24505152,  0.06895304,\n",
      "         -0.04473242,  0.13754906, -0.33593524,  0.24414010,  0.14171246,\n",
      "         -0.00907750, -0.07285063, -0.00261159, -0.13747911, -0.08343538,\n",
      "          0.07570130,  0.13455811,  0.01849212, -0.19723380, -0.04344609,\n",
      "         -0.15851927, -0.39577460,  0.52754450, -0.22363333, -0.16370147,\n",
      "         -0.21469182, -0.09909181, -0.42612949, -0.22124048,  0.21437314,\n",
      "         -0.11926240,  0.23691089, -0.07175464,  0.04934144, -0.06372608,\n",
      "         -0.23854029,  0.44143280,  0.30924776, -0.20347947, -0.26002690,\n",
      "         -0.09236696,  0.44411969, -0.31882548],\n",
      "        [ 0.09607889,  0.17741671, -0.04574570, -0.18416879, -0.01269867,\n",
      "         -0.24191898, -0.06901045,  0.01200041,  0.03072003, -0.07903454,\n",
      "         -0.10449867, -0.03812914,  0.07812503,  0.34246579, -0.06599489,\n",
      "         -0.07054153,  0.35421222, -0.22705874,  0.14939192, -0.01807650,\n",
      "         -0.05800538,  0.09642633, -0.14395389, -0.09630294,  0.14538950,\n",
      "         -0.04219627,  0.00227098,  0.22121459, -0.20172608,  0.04642368,\n",
      "          0.78628951, -0.20517753,  0.45415148, -0.04719108, -0.08523928,\n",
      "         -0.22806501,  0.01591285, -0.14118044, -0.14927723,  0.14477456,\n",
      "         -0.19690369,  0.42067039, -0.08712606,  0.14497675,  0.29824790,\n",
      "         -0.10660459,  0.24851489,  0.12795949, -0.20899364,  0.02209766,\n",
      "          0.05448630,  0.34078094,  0.05293443, -0.18213883,  0.09203795,\n",
      "          0.13706350,  0.04884297, -0.10395227],\n",
      "        [ 0.19755772, -0.10305576,  0.13972500, -0.16618863,  0.08374009,\n",
      "         -0.30198854, -0.23210727,  0.00944243, -0.12928213, -0.02503750,\n",
      "         -0.23093699,  0.12382804, -0.27917933, -0.24631125, -0.07102811,\n",
      "         -0.01527685, -0.51093280,  0.14198445,  0.05916951,  0.13488480,\n",
      "          0.21195331, -0.13061172,  0.17422709, -0.08871048, -0.01868495,\n",
      "          0.25534749,  0.01534659,  0.11205456, -0.10810605, -0.17302319,\n",
      "          0.02133359, -0.14213504, -0.22981402, -0.42231700, -0.14215016,\n",
      "          0.04828001, -0.15544014, -0.01799679,  0.35665360, -0.36690399,\n",
      "          0.12856749, -0.00814362,  0.21762677,  0.24148703, -0.08855698,\n",
      "         -0.19084907,  0.24856958, -0.25445163, -0.13415825, -0.12801118,\n",
      "         -0.40460032, -0.47345549,  0.11541037,  0.61145890,  0.10828710,\n",
      "          0.08274733,  0.05713370,  0.02633692],\n",
      "        [ 0.41788012, -0.08275279,  0.14280412,  0.27325425,  0.20440149,\n",
      "         -0.16651648, -0.30521137,  0.27287516, -0.16515750,  0.08230786,\n",
      "         -0.13111323,  0.10851090, -0.37470824, -0.11785967,  0.14800425,\n",
      "          0.17878133, -0.08148210, -0.03560171,  0.18624744, -0.16103047,\n",
      "          0.17886408, -0.07840552,  0.26968232, -0.08304735, -0.08541118,\n",
      "          0.17932189, -0.13120405, -0.07478014,  0.33693311, -0.50181383,\n",
      "         -0.26048943, -0.00612644, -0.42425600, -0.09187708, -0.25945520,\n",
      "          0.12086420,  0.07283942, -0.23820958,  0.28434184, -0.33349475,\n",
      "          0.31365645,  0.01035341, -0.09572551, -0.25971240, -0.12249255,\n",
      "          0.24705592,  0.23905313, -0.40358481,  0.45332310, -0.05545292,\n",
      "         -0.13327850, -0.06674640, -0.31452227, -0.00478790,  0.20358670,\n",
      "          0.07786708, -0.00772747, -0.38042337]])\n",
      "Tensor(shape=[10, 58], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[0.01859678, 0.01791811, 0.01727874, 0.01724033, 0.01811237, 0.01798522,\n",
      "         0.01687217, 0.01659796, 0.01811037, 0.01832497, 0.01996524, 0.01746445,\n",
      "         0.01673362, 0.01576889, 0.01923527, 0.01519638, 0.01493610, 0.01642736,\n",
      "         0.01749395, 0.01634145, 0.01881916, 0.02038350, 0.01675417, 0.01526722,\n",
      "         0.01599461, 0.01847997, 0.01794758, 0.01576610, 0.02088622, 0.01749300,\n",
      "         0.01721500, 0.01672267, 0.01492466, 0.01805658, 0.01592861, 0.01469758,\n",
      "         0.02063961, 0.01696680, 0.02002969, 0.01588249, 0.01720833, 0.01611607,\n",
      "         0.02076480, 0.01716820, 0.01592287, 0.01619204, 0.01810614, 0.01697117,\n",
      "         0.01736860, 0.01398041, 0.01680052, 0.01678429, 0.01496326, 0.01687138,\n",
      "         0.01810336, 0.01742031, 0.01655229, 0.01725100],\n",
      "        [0.01716935, 0.01775263, 0.01752180, 0.01671594, 0.02006202, 0.01688345,\n",
      "         0.01892497, 0.01887280, 0.01622517, 0.01631855, 0.01709341, 0.01407066,\n",
      "         0.01423263, 0.01972639, 0.01350523, 0.01837590, 0.01762999, 0.01954321,\n",
      "         0.02330442, 0.01641559, 0.01440746, 0.01392418, 0.01732700, 0.01915951,\n",
      "         0.01405091, 0.01803364, 0.01867068, 0.01599907, 0.02111555, 0.01561629,\n",
      "         0.01668974, 0.01526993, 0.01530321, 0.01713911, 0.01632609, 0.01682887,\n",
      "         0.01918879, 0.01479438, 0.01860215, 0.01785613, 0.01688077, 0.01815001,\n",
      "         0.01674303, 0.01515349, 0.01378017, 0.01669861, 0.02270637, 0.01334022,\n",
      "         0.02230744, 0.01653171, 0.02179265, 0.01598348, 0.01296308, 0.02079782,\n",
      "         0.01859258, 0.01751119, 0.01420023, 0.01922040],\n",
      "        [0.01513110, 0.01419319, 0.01690011, 0.01622261, 0.01342662, 0.01723243,\n",
      "         0.01834081, 0.01474459, 0.01476333, 0.01669584, 0.01282690, 0.01626498,\n",
      "         0.01572441, 0.01495606, 0.01685658, 0.02620470, 0.01907044, 0.01505328,\n",
      "         0.01126526, 0.01913314, 0.01271722, 0.01648491, 0.01633242, 0.01410819,\n",
      "         0.02603716, 0.01458378, 0.01916290, 0.01908732, 0.01906515, 0.01463528,\n",
      "         0.02627244, 0.01598201, 0.02406852, 0.01751710, 0.01792482, 0.01502329,\n",
      "         0.01419916, 0.01761413, 0.01468740, 0.01438223, 0.01916653, 0.02113397,\n",
      "         0.01457588, 0.01076122, 0.02041608, 0.01867528, 0.01838225, 0.02237723,\n",
      "         0.01424112, 0.01985336, 0.01580277, 0.01897405, 0.02058934, 0.01782620,\n",
      "         0.01930087, 0.01565346, 0.01914820, 0.01823034],\n",
      "        [0.01452318, 0.02067616, 0.01670265, 0.01672615, 0.01356025, 0.01716664,\n",
      "         0.01571011, 0.01332233, 0.01703621, 0.02055917, 0.01412611, 0.01832931,\n",
      "         0.01605515, 0.01882676, 0.01510311, 0.01886200, 0.01521448, 0.01701523,\n",
      "         0.01744518, 0.01838741, 0.01814762, 0.01985838, 0.01511636, 0.01683719,\n",
      "         0.01660158, 0.01800066, 0.02082546, 0.01573420, 0.01953957, 0.02069111,\n",
      "         0.01829363, 0.01564863, 0.01840790, 0.01519844, 0.01819676, 0.01657308,\n",
      "         0.01532085, 0.02089383, 0.01865994, 0.01607055, 0.01589356, 0.01678179,\n",
      "         0.01655922, 0.01350351, 0.01507825, 0.01785817, 0.01706101, 0.02144096,\n",
      "         0.01892864, 0.01436990, 0.02657474, 0.01473081, 0.01898046, 0.01770507,\n",
      "         0.01755630, 0.01523758, 0.01603953, 0.01573713],\n",
      "        [0.01679045, 0.01580426, 0.01686517, 0.01643324, 0.01688103, 0.01356601,\n",
      "         0.01077347, 0.01664115, 0.01662911, 0.02061084, 0.01639432, 0.01739841,\n",
      "         0.01448340, 0.01478943, 0.01532905, 0.01735699, 0.02235878, 0.01624619,\n",
      "         0.02062911, 0.01711495, 0.01528927, 0.01684016, 0.01533276, 0.01553573,\n",
      "         0.02274595, 0.01850817, 0.01493939, 0.02318843, 0.01565754, 0.01284957,\n",
      "         0.01453026, 0.01318786, 0.02358930, 0.01408543, 0.01637392, 0.01727639,\n",
      "         0.02003451, 0.01132130, 0.01450668, 0.01843393, 0.01489918, 0.01765027,\n",
      "         0.01793040, 0.01925571, 0.01608093, 0.01907684, 0.02087357, 0.01498481,\n",
      "         0.01996552, 0.02214804, 0.01664046, 0.02021473, 0.01775835, 0.01966742,\n",
      "         0.02235702, 0.01573253, 0.01889943, 0.01854289],\n",
      "        [0.01877492, 0.01363514, 0.01923723, 0.01813697, 0.01537165, 0.01424894,\n",
      "         0.01374765, 0.01612057, 0.01367981, 0.01443566, 0.01942744, 0.01451689,\n",
      "         0.01213002, 0.01510732, 0.01618933, 0.02245072, 0.02259663, 0.01839064,\n",
      "         0.02510234, 0.01724688, 0.01522394, 0.01903019, 0.02398730, 0.02112861,\n",
      "         0.02212689, 0.01577777, 0.01812767, 0.01806734, 0.02002853, 0.01658139,\n",
      "         0.01167847, 0.01448066, 0.01833913, 0.01705313, 0.01643833, 0.01813631,\n",
      "         0.01512385, 0.01776696, 0.01655635, 0.01559990, 0.01949624, 0.01721100,\n",
      "         0.01430899, 0.01462721, 0.01573606, 0.01961442, 0.01453145, 0.01617942,\n",
      "         0.02067734, 0.02487647, 0.01661978, 0.01744450, 0.01451200, 0.01488751,\n",
      "         0.02279540, 0.01382116, 0.01652434, 0.01433732],\n",
      "        [0.01601615, 0.01568465, 0.01427770, 0.01853833, 0.01121334, 0.01397834,\n",
      "         0.02677976, 0.02440994, 0.01315126, 0.02294978, 0.01353934, 0.01628904,\n",
      "         0.01619072, 0.02338303, 0.02154815, 0.02176774, 0.01341806, 0.01190893,\n",
      "         0.01351339, 0.01849842, 0.01651056, 0.01981187, 0.01233940, 0.02204030,\n",
      "         0.01989453, 0.01710986, 0.01605277, 0.01722085, 0.01504812, 0.01588375,\n",
      "         0.01862367, 0.01975271, 0.01758813, 0.01417527, 0.01653181, 0.01473481,\n",
      "         0.01162267, 0.02926162, 0.01380594, 0.01465865, 0.01392994, 0.01563701,\n",
      "         0.01127517, 0.01383901, 0.02139389, 0.01532476, 0.02188154, 0.01607038,\n",
      "         0.01813917, 0.01619992, 0.01360166, 0.02684730, 0.02352303, 0.01408701,\n",
      "         0.01331253, 0.01574252, 0.02691953, 0.01255234],\n",
      "        [0.01804528, 0.01957439, 0.01565921, 0.01363495, 0.01618535, 0.01286984,\n",
      "         0.01529911, 0.01659009, 0.01690358, 0.01514652, 0.01476569, 0.01577894,\n",
      "         0.01772419, 0.02308703, 0.01534532, 0.01527570, 0.02335982, 0.01306251,\n",
      "         0.01903343, 0.01609854, 0.01546841, 0.01805155, 0.01419446, 0.01488721,\n",
      "         0.01895741, 0.01571489, 0.01642946, 0.02045075, 0.01339765, 0.01717112,\n",
      "         0.03598473, 0.01335149, 0.02581503, 0.01563660, 0.01505283, 0.01304938,\n",
      "         0.01665513, 0.01423388, 0.01411909, 0.01894575, 0.01346241, 0.02496502,\n",
      "         0.01502445, 0.01894958, 0.02208841, 0.01473463, 0.02101676, 0.01862984,\n",
      "         0.01330063, 0.01675845, 0.01731012, 0.02304817, 0.01728328, 0.01366266,\n",
      "         0.01797251, 0.01880022, 0.01721271, 0.01477377],\n",
      "        [0.02127932, 0.01575444, 0.02008358, 0.01479056, 0.01899010, 0.01291241,\n",
      "         0.01384703, 0.01763032, 0.01534663, 0.01703279, 0.01386324, 0.01976684,\n",
      "         0.01321032, 0.01365173, 0.01626718, 0.01719985, 0.01047766, 0.02012902,\n",
      "         0.01852919, 0.01998661, 0.02158786, 0.01532624, 0.02078860, 0.01598207,\n",
      "         0.01714133, 0.02254527, 0.01773472, 0.01953548, 0.01567507, 0.01468982,\n",
      "         0.01784122, 0.01515064, 0.01387882, 0.01144852, 0.01515041, 0.01832851,\n",
      "         0.01495039, 0.01715314, 0.02494894, 0.01210083, 0.01986075, 0.01732298,\n",
      "         0.02171069, 0.02223494, 0.01598452, 0.01443028, 0.02239298, 0.01354106,\n",
      "         0.01527198, 0.01536614, 0.01165316, 0.01087778, 0.01960115, 0.03218938,\n",
      "         0.01946202, 0.01897126, 0.01849151, 0.01793071],\n",
      "        [0.02598508, 0.01575077, 0.01973604, 0.02248608, 0.02098995, 0.01448518,\n",
      "         0.01260925, 0.02247756, 0.01450488, 0.01857748, 0.01500718, 0.01907070,\n",
      "         0.01176271, 0.01520740, 0.01983894, 0.02045901, 0.01577080, 0.01651123,\n",
      "         0.02061233, 0.01456486, 0.02046071, 0.01581939, 0.02240591, 0.01574613,\n",
      "         0.01570896, 0.02047008, 0.01500582, 0.01587685, 0.02396454, 0.01035872,\n",
      "         0.01318596, 0.01700514, 0.01119409, 0.01560771, 0.01319961, 0.01930775,\n",
      "         0.01840241, 0.01348304, 0.02273678, 0.01225762, 0.02341317, 0.01728771,\n",
      "         0.01554776, 0.01319621, 0.01513712, 0.02190463, 0.02173003, 0.01142790,\n",
      "         0.02692258, 0.01618669, 0.01497472, 0.01600492, 0.01249239, 0.01702792,\n",
      "         0.02097286, 0.01849516, 0.01697794, 0.01169567]])\n"
     ]
    }
   ],
   "source": [
    "X = paddle.arange(10).reshape((2, 5))\n",
    "print(X)  # 假如这是两个数字\n",
    "num_hiddens = 64\n",
    "net = RNN_paddle(len(char_to_index)+1, num_hiddens)\n",
    "state = net.begin_state(2)\n",
    "Y, new_state = net(X, state)\n",
    "Y.shape, len(new_state), new_state[0].shape\n",
    "print(Y)\n",
    "print(F.softmax(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "Tensor(shape=[58, 512], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.07504431, -0.08937682,  0.02452077, ..., -0.08960503,\n",
      "         -0.00979052, -0.09207264],\n",
      "        [-0.01492514,  0.06090340, -0.09600491, ..., -0.03521815,\n",
      "          0.04943486,  0.08243959],\n",
      "        [ 0.02515448,  0.02092591,  0.07248142, ...,  0.05058851,\n",
      "         -0.06122007,  0.10113946],\n",
      "        ...,\n",
      "        [-0.02011482, -0.07541491, -0.04090464, ...,  0.01871093,\n",
      "          0.02831748, -0.04397932],\n",
      "        [-0.01754829,  0.09012064, -0.00134355, ..., -0.09615290,\n",
      "          0.10001893,  0.00466319],\n",
      "        [ 0.09934632,  0.04565306,  0.05140664, ..., -0.04117487,\n",
      "          0.08582909,  0.05964456]]), Parameter containing:\n",
      "Tensor(shape=[512], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.]), Parameter containing:\n",
      "Tensor(shape=[512, 512], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.00145903,  0.06405918, -0.05674967, ..., -0.04521538,\n",
      "          0.05488057, -0.04006285],\n",
      "        [-0.05636926, -0.01863615,  0.03149226, ..., -0.01829127,\n",
      "         -0.06690881, -0.01380429],\n",
      "        [-0.06619145, -0.04276128, -0.03781599, ...,  0.01439863,\n",
      "          0.04550711,  0.04040756],\n",
      "        ...,\n",
      "        [-0.01116558, -0.00286317, -0.00062803, ...,  0.05267933,\n",
      "         -0.01644061, -0.02697310],\n",
      "        [-0.03937883, -0.05161893,  0.01274207, ..., -0.05148725,\n",
      "          0.02209995,  0.05121826],\n",
      "        [-0.01617362, -0.00282018,  0.04149292, ...,  0.07116260,\n",
      "         -0.07512908,  0.03523191]]), Parameter containing:\n",
      "Tensor(shape=[512], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.]), Parameter containing:\n",
      "Tensor(shape=[512, 58], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [[ 0.04082612,  0.04719847, -0.01176364, ..., -0.05704479,\n",
      "         -0.03807193,  0.04123021],\n",
      "        [-0.06737096,  0.04669731, -0.01639562, ...,  0.02394856,\n",
      "          0.06427928, -0.10033537],\n",
      "        [ 0.04231593,  0.03231674, -0.08642460, ...,  0.05395105,\n",
      "          0.07114691, -0.03318897],\n",
      "        ...,\n",
      "        [ 0.00532402, -0.00564671, -0.03554724, ..., -0.06439037,\n",
      "          0.03324496, -0.00209880],\n",
      "        [ 0.04321342, -0.03462283, -0.04322757, ..., -0.01057925,\n",
      "         -0.04396877,  0.08948454],\n",
      "        [ 0.01878913,  0.04039580,  0.02584062, ...,  0.02442729,\n",
      "          0.02625446, -0.10068215]]), Parameter containing:\n",
      "Tensor(shape=[58], dtype=float32, place=Place(gpu:0), stop_gradient=False,\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "# 定义模型参数\n",
    "num_hiddens = 512\n",
    "lr = 0.01\n",
    "loss = nn.CrossEntropyLoss()\n",
    "num_epochs = 30\n",
    "model = RNN_paddle(len(char_to_index)+1, num_hiddens)\n",
    "\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=lr,  parameters= model.parameters()) \n",
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "steps = []\n",
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Layer):\n",
    "        params = [p for p in net.parameters() if not p.stop_gradient]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = paddle.sqrt(sum(paddle.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        with paddle.no_grad():\n",
    "            for param in params:\n",
    "                param.grad[:] *= theta / norm\n",
    "                param.stop_gradient = False\n",
    "def train(model):\n",
    "    # 开启训练模式\n",
    "    model.train()\n",
    "    global_step =  0\n",
    "    state = None\n",
    "    clip_threshold = 1.0 \n",
    "    for epoch in range(num_epochs):\n",
    "        for step, data in enumerate(train_loader):\n",
    "            X = data[:,:-1]\n",
    "            Y = data[:,1:]\n",
    "            # Y = F.one_hot(Y,len(char_to_index)+1)\n",
    "            Y = paddle.reshape(Y.T, shape=[-1])\n",
    "            if state is None :\n",
    "                # 在第一次迭代或使用随机抽样时初始化`state`\n",
    "                state = model.begin_state(batch_size=X.shape[0])\n",
    "            else:\n",
    "                if isinstance(net, nn.Layer) and not isinstance(state, tuple):\n",
    "                    state.stop_gradient=True\n",
    "                else:\n",
    "                    for s in state:\n",
    "                        s.stop_gradient=True\n",
    "            y_hat, state = model(X, state)\n",
    "            # print(F.softmax(y_hat))\n",
    "            # print(y_hat,Y)\n",
    "            # 计算损失\n",
    "            loss = F.cross_entropy(input=y_hat, label=Y)\n",
    "            loss = paddle.mean(loss)\n",
    "            \n",
    "            optimizer.clear_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            grad_clipping(model, 1)\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            # 更新参数\n",
    "            global_step+=1\n",
    "            if step % 100 == 0:\n",
    "                # 记录当前步骤的loss变化情况\n",
    "                losses.append(loss.numpy()[0])\n",
    "                steps.append(step)\n",
    "                # 打印当前loss数值\n",
    "                print(\"epoch %d, step %d, loss %.3f\" % (epoch,global_step, loss.numpy()[0]))\n",
    "            \n",
    "            \n",
    "    # predict = lambda prefix: predict_ch8(prefix, 7, net,char_to_index,index_to_char, device)\n",
    "    # # 训练和预测\n",
    "    # for epoch in range(num_epochs):\n",
    "    #     train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n",
    "    #     if (epoch + 1) % 10 == 0:\n",
    "    #         print(predict('time traveller'))\n",
    "    # print(predict('Asto'))\n",
    "    # print(predict('Lenb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, step 1, loss 1.670\n",
      "epoch 0, step 101, loss 1.392\n",
      "epoch 1, step 199, loss 1.374\n",
      "epoch 1, step 299, loss 1.951\n",
      "epoch 2, step 397, loss 1.723\n",
      "epoch 2, step 497, loss 1.986\n",
      "epoch 3, step 595, loss 1.740\n",
      "epoch 3, step 695, loss 1.619\n",
      "epoch 4, step 793, loss 1.466\n",
      "epoch 4, step 893, loss 1.654\n",
      "epoch 5, step 991, loss 1.874\n",
      "epoch 5, step 1091, loss 1.472\n",
      "epoch 6, step 1189, loss 1.548\n",
      "epoch 6, step 1289, loss 1.543\n",
      "epoch 7, step 1387, loss 1.360\n",
      "epoch 7, step 1487, loss 1.710\n",
      "epoch 8, step 1585, loss 1.564\n",
      "epoch 8, step 1685, loss 1.486\n",
      "epoch 9, step 1783, loss 1.624\n",
      "epoch 9, step 1883, loss 1.852\n",
      "epoch 10, step 1981, loss 1.732\n",
      "epoch 10, step 2081, loss 2.039\n",
      "epoch 11, step 2179, loss 1.966\n",
      "epoch 11, step 2279, loss 1.677\n",
      "epoch 12, step 2377, loss 1.574\n",
      "epoch 12, step 2477, loss 1.830\n",
      "epoch 13, step 2575, loss 1.452\n",
      "epoch 13, step 2675, loss 1.629\n",
      "epoch 14, step 2773, loss 1.472\n",
      "epoch 14, step 2873, loss 1.527\n",
      "epoch 15, step 2971, loss 1.413\n",
      "epoch 15, step 3071, loss 1.674\n",
      "epoch 16, step 3169, loss 1.507\n",
      "epoch 16, step 3269, loss 1.454\n",
      "epoch 17, step 3367, loss 1.339\n",
      "epoch 17, step 3467, loss 1.572\n",
      "epoch 18, step 3565, loss 1.269\n",
      "epoch 18, step 3665, loss 1.910\n",
      "epoch 19, step 3763, loss 1.332\n",
      "epoch 19, step 3863, loss 1.218\n",
      "epoch 20, step 3961, loss 1.416\n",
      "epoch 20, step 4061, loss 1.712\n",
      "epoch 21, step 4159, loss 1.766\n",
      "epoch 21, step 4259, loss 1.732\n",
      "epoch 22, step 4357, loss 1.922\n",
      "epoch 22, step 4457, loss 1.983\n",
      "epoch 23, step 4555, loss 1.690\n",
      "epoch 23, step 4655, loss 1.490\n",
      "epoch 24, step 4753, loss 1.694\n",
      "epoch 24, step 4853, loss 1.789\n",
      "epoch 25, step 4951, loss 1.427\n",
      "epoch 25, step 5051, loss 1.678\n",
      "epoch 26, step 5149, loss 1.502\n",
      "epoch 26, step 5249, loss 1.642\n",
      "epoch 27, step 5347, loss 1.483\n",
      "epoch 27, step 5447, loss 1.402\n",
      "epoch 28, step 5545, loss 1.500\n",
      "epoch 28, step 5645, loss 2.369\n",
      "epoch 29, step 5743, loss 1.833\n",
      "epoch 29, step 5843, loss 1.613\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "(NotFound) The kernel with key (GPU, Undefined(AnyLayout), float32) of kernel `one_hot` is not registered and fail to fallback to CPU one. Selected wrong DataType `float32`. Paddle support following DataTypes: int32, int64.\n  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ..\\paddle\\phi\\core\\kernel_factory.cc:259)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Myproject\\VSCode_project\\01BaiduAI\\4.自然语言处理\\RNN起名.ipynb 单元格 13\u001b[0m line \u001b[0;36m<cell line: 25>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     predicted_char \u001b[39m=\u001b[39m index_to_char[pred]  \u001b[39m# 将数字编码转换为对应的字母\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_char\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m predict_name(\u001b[39m'\u001b[39;49m\u001b[39mGav\u001b[39;49m\u001b[39m'\u001b[39;49m,model, char_to_index, index_to_char)\n",
      "\u001b[1;32me:\\Myproject\\VSCode_project\\01BaiduAI\\4.自然语言处理\\RNN起名.ipynb 单元格 13\u001b[0m line \u001b[0;36mpredict_name\u001b[1;34m(input_names, model, char_to_index, index_to_char)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m state:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                 s\u001b[39m.\u001b[39mstop_gradient \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     y_hat, state \u001b[39m=\u001b[39m model(X, state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# 对预测结果进行处理\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m y_prob \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(y_hat, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32me:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\nn\\layer\\layers.py:1254\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1246\u001b[0m     (\u001b[39mnot\u001b[39;00m in_declarative_mode())\n\u001b[0;32m   1247\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1251\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m in_profiler_mode())\n\u001b[0;32m   1252\u001b[0m ):\n\u001b[0;32m   1253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_once(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dygraph_call_func(\u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32me:\\Myproject\\VSCode_project\\01BaiduAI\\4.自然语言处理\\RNN起名.ipynb 单元格 13\u001b[0m line \u001b[0;36mRNN_paddle.forward\u001b[1;34m(self, inputs, state)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, inputs, state):\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# 先进行one-hot\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     X \u001b[39m=\u001b[39m paddle\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(inputs\u001b[39m.\u001b[39;49mT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab_size)  \u001b[39m# 将X修改为inputs\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     H, \u001b[39m=\u001b[39m state\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X25sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     outputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32me:\\SoftWare\\Program\\Anaconda\\envs\\d2l\\lib\\site-packages\\paddle\\nn\\functional\\input.py:89\u001b[0m, in \u001b[0;36mone_hot\u001b[1;34m(x, num_classes, name)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39mThe operator converts each id in the input 'x' to an one-hot vector with a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \n\u001b[0;32m     86\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m in_dynamic_mode():\n\u001b[1;32m---> 89\u001b[0m     \u001b[39mreturn\u001b[39;00m _C_ops\u001b[39m.\u001b[39;49mone_hot(x, num_classes)\n\u001b[0;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     check_variable_and_dtype(x, \u001b[39m'\u001b[39m\u001b[39minput\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39m'\u001b[39m\u001b[39mint32\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mone_hot_v2\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: (NotFound) The kernel with key (GPU, Undefined(AnyLayout), float32) of kernel `one_hot` is not registered and fail to fallback to CPU one. Selected wrong DataType `float32`. Paddle support following DataTypes: int32, int64.\n  [Hint: Expected kernel_iter != iter->second.end(), but received kernel_iter == iter->second.end().] (at ..\\paddle\\phi\\core\\kernel_factory.cc:259)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_name(input_names, model, char_to_index, index_to_char):\n",
    "    state = None\n",
    "    input_names = [char_to_index[ch] for ch in input_names]  # 将字母转换为对应的数字编码\n",
    "    for name in input_names:\n",
    "        X = np.array([[name]], dtype=np.float32)\n",
    "        X = paddle.to_tensor(X)\n",
    "        if state is None:\n",
    "            state = model.begin_state(batch_size=1)\n",
    "        else:\n",
    "            if isinstance(model, paddle.nn.Layer) and not isinstance(state, tuple):\n",
    "                state.stop_gradient = True\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.stop_gradient = True\n",
    "        y_hat, state = model(X, state)\n",
    "    \n",
    "    # 对预测结果进行处理\n",
    "    y_prob = F.softmax(y_hat, axis=-1)\n",
    "    pred = int(paddle.argmax(y_prob, axis=-1).numpy()[0][0])\n",
    "    predicted_char = index_to_char[pred]  # 将数字编码转换为对应的字母\n",
    "    \n",
    "    return predicted_char\n",
    "predict_name('Gav',model, char_to_index, index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, char_to_index, index_to_char,device):  #@save\n",
    "    \"\"\"预测字符\n",
    "\n",
    "    Args:\n",
    "        prefix (_type_): 要输入的字符\n",
    "        num_preds (_type_): 要预测的数量\n",
    "        net (_type_): rnn网络\n",
    "        vocab (_type_): 字典\n",
    "        device (_type_): 设备\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    state = net.begin_state(batch_size=1)  # 生成隐藏层h0\n",
    "    outputs = [char_to_index[prefix[0]]]  # 取第一个字符对应的索引值\n",
    "    get_input = lambda: paddle.reshape(paddle.to_tensor(outputs[-1], place=device), (1, 1))  # 一个函数，取一个字符，返回1*1的向量\n",
    "    for y in prefix[1:]:  # 预热期，每一个字符\n",
    "        _, state = net(get_input(), state)  # 每一个字符作为输入\n",
    "        outputs.append(char_to_index[y])  # 索引表示\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(paddle.reshape(paddle.argmax(y,axis=1),shape=[1])))\n",
    "    return ''.join([index_to_char[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^Adonnnnn'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('^Ado', 5, model, char_to_index,index_to_char, 'gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Layer):\n",
    "        params = [p for p in net.parameters() if not p.stop_gradient]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = paddle.sqrt(sum(paddle.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        with paddle.no_grad():\n",
    "            for param in params:\n",
    "                param.grad[:] *= theta / norm\n",
    "                param.stop_gradient = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单个epoch，里面要进行one-hot编码\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        net (_type_):RNN\n",
    "        train_iter (_type_): _训练迭代器_\n",
    "        loss (_type_): _损失函数_\n",
    "        updater (_type_): _description_\n",
    "        device (_type_): _设备_\n",
    "        use_random_iter (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    for X, Y in train_iter():\n",
    "        if state is None or use_random_iter:\n",
    "            # 在第一次迭代或使用随机抽样时初始化`state`\n",
    "            state = net.begin_state(batch_size=X.shape[0])\n",
    "        else:\n",
    "            if isinstance(net, nn.Layer) and not isinstance(state, tuple):\n",
    "                # `state`对于`nn.GRU`是个张量\n",
    "                state.stop_gradient=True\n",
    "            else:\n",
    "                # `state`对于`nn.LSTM`或对于我们从零开始实现的模型是个张量\n",
    "                for s in state:\n",
    "                    s.stop_gradient=True\n",
    "        y = paddle.reshape(Y.T,shape=[-1])\n",
    "        X = paddle.to_tensor(X, place=device)\n",
    "        y = paddle.to_tensor(y, place=device)\n",
    "        y_hat, state = net(X, state)\n",
    "        l = loss(y_hat, y).mean()\n",
    "        if isinstance(updater, paddle.optimizer.Optimizer):\n",
    "            updater.clear_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # 因为已经调用了`mean`函数\n",
    "            net.params = updater(batch_size=1)\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, char_to_index,index_to_char, lr, num_epochs, device, use_random_iter=False):\n",
    "    \"\"\"训练\n",
    "\n",
    "    Args:\n",
    "        net (_type_): rnn网络\n",
    "        train_iter (_type_): 训练迭代器\n",
    "        vocab (_type_): 字典\n",
    "        lr (_type_): 学习率\n",
    "        num_epochs (_type_): 多少个 epoch\n",
    "        device (_type_): 设备\n",
    "        use_random_iter (bool, optional): _description_. Defaults to False.\n",
    "    \"\"\"\n",
    "    loss = nn.CrossEntropyLoss()  # 定义损失函数\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Layer):\n",
    "        updater = paddle.optimizer.SGD(\n",
    "                learning_rate=lr, parameters=net.parameters())\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 7, net,char_to_index,index_to_char, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "    print(predict('Asto'))\n",
    "    print(predict('Lenb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3928165943.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [140]\u001b[1;36m\u001b[0m\n\u001b[1;33m    def train()\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Myproject\\VSCode_project\\01BaiduAI\\4.自然语言处理\\RNN起名.ipynb 单元格 15\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m num_epochs, lr \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, \u001b[39m0.1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m train_ch8(net, train_loader, char_to_index,index_to_char, lr, num_epochs, \u001b[39m'\u001b[39;49m\u001b[39mgpu\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32me:\\Myproject\\VSCode_project\\01BaiduAI\\4.自然语言处理\\RNN起名.ipynb 单元格 15\u001b[0m line \u001b[0;36mtrain_ch8\u001b[1;34m(net, train_iter, char_to_index, index_to_char, lr, num_epochs, device, use_random_iter)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# 训练和预测\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39mprint\u001b[39m(predict(\u001b[39m'\u001b[39m\u001b[39mtime traveller\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;32me:\\Myproject\\VSCode_project\\01BaiduAI\\4.自然语言处理\\RNN起名.ipynb 单元格 15\u001b[0m line \u001b[0;36mtrain_epoch_ch8\u001b[1;34m(net, train_iter, loss, updater, device, use_random_iter)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_epoch_ch8\u001b[39m(net, train_iter, loss, updater, device, use_random_iter):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"_summary_\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m X, Y \u001b[39min\u001b[39;00m train_iter():\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m use_random_iter:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m             \u001b[39m# 在第一次迭代或使用随机抽样时初始化`state`\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Myproject/VSCode_project/01BaiduAI/4.%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/RNN%E8%B5%B7%E5%90%8D.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m             state \u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mbegin_state(batch_size\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 10, 0.1\n",
    "train_ch8(net, train_loader, char_to_index,index_to_char, lr, num_epochs, 'gpu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
